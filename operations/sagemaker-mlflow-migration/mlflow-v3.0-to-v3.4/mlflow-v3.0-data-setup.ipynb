{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6e526f55-dd2e-43b7-b404-01e05d0d670c",
   "metadata": {},
   "source": [
    "# MLflow v3.0 Data Setup for Migration Testing"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Install dependencies\n",
    "\n",
    "!pip3 install mlflow==3.0\n",
    "!pip3 install sagemaker-mlflow==0.2.0"
   ],
   "id": "b000949611f9ca25"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# import modules\n",
    "\n",
    "import mlflow\n",
    "from mlflow.models import infer_signature\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import mlflow\n",
    "from mlflow import MlflowClient"
   ],
   "id": "629e686dcc1b1ca1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Configuration - Replace with your actual values\n",
    "tracking_server_name = '<your-tracking-server-name>'\n",
    "tracking_server_arn = \"<your-tracking-server-arn>\"\n",
    "registered_model_name = \"<your-model-name>\"\n",
    "mlflow.set_tracking_uri(tracking_server_arn)"
   ],
   "id": "463fd667bbbbe27"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaaf840f-144c-4a16-87ae-ba8d7cd15730",
   "metadata": {},
   "outputs": [],
   "source": [
    "@mlflow.trace\n",
    "def train_and_log_linear_regression_model(experiment_name):\n",
    "\n",
    "    mlflow.set_experiment(experiment_name)\n",
    "    # enable autologging\n",
    "    mlflow.sklearn.autolog(registered_model_name=registered_model_name)\n",
    "    \n",
    "    with mlflow.start_span(name=\"data_preparation\") as span:\n",
    "        # prepare training data\n",
    "        X = np.array([[1, 1], [1, 2], [2, 2], [2, 3]])\n",
    "        y = np.dot(X, np.array([1, 2])) + 3\n",
    "        span.set_inputs({\"X_shape\": X.shape, \"y_shape\": y.shape})\n",
    "        span.set_outputs({\"data_ready\": True})\n",
    "    \n",
    "    with mlflow.start_span(name=\"model_training\") as span:\n",
    "        model = LinearRegression()\n",
    "        with mlflow.start_run() as run:\n",
    "            # Log additional data\n",
    "            mlflow.log_param(\"data_shape\", X.shape)\n",
    "            mlflow.log_param(\"model_type\", \"LinearRegression\")\n",
    "            mlflow.log_metric(\"training_samples\", len(X))\n",
    "            \n",
    "            model.fit(X, y)\n",
    "            span.set_inputs({\"model\": \"LinearRegression\", \"training_data\": X.shape})\n",
    "            span.set_outputs({\"model_trained\": True})\n",
    "            \n",
    "            with mlflow.start_span(name=\"model_evaluation\") as eval_span:\n",
    "                # Log predictions and score\n",
    "                predictions = model.predict(X)\n",
    "                score = model.score(X, y)\n",
    "                mlflow.log_metric(\"r2_score\", score)\n",
    "                mlflow.log_metric(\"mean_prediction\", np.mean(predictions))\n",
    "                \n",
    "                # Log model coefficients\n",
    "                mlflow.log_param(\"coefficients\", model.coef_.tolist())\n",
    "                mlflow.log_param(\"intercept\", model.intercept_)\n",
    "                \n",
    "                eval_span.set_inputs({\"test_data\": X.shape})\n",
    "                eval_span.set_outputs({\"r2_score\": score, \"predictions\": predictions.tolist()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "597911d7-301e-40e0-889f-abac8a4adaa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sample_experiments():\n",
    "    exps = [f\"linear-regression-baseline-{tracking_server_name}\", f\"model-performance-analysis-{tracking_server_name}\", f\"sklearn-migration-test-{tracking_server_name}\", \n",
    "            f\"data-validation-experiment-{tracking_server_name}\", f\"regression-model-comparison-{tracking_server_name}\"]\n",
    "    \n",
    "    for exp in exps:\n",
    "        train_and_log_linear_regression_model(exp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed4fdef8-3de5-4372-83e9-853341320044",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create Sample Experiments for Migration Testing\n",
    "\n",
    "create_sample_experiments()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prompt-section",
   "metadata": {},
   "source": [
    "## MLflow v3.0 Prompt Creation for Migration Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "create-prompt-func",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_prompt(prompt_name, template, tags=None, commit_message=None):\n",
    "    \"\"\"Create a prompt with version compatibility.\n",
    "    \n",
    "    Works with MLflow 2.21+ and 3.0+.\n",
    "    Note: MLflow 2.16-2.20 do NOT support prompts.\n",
    "    \"\"\"\n",
    "    import mlflow\n",
    "    \n",
    "    # Try MLflow 3.0+ API first\n",
    "    try:\n",
    "        import mlflow.genai\n",
    "        prompt = mlflow.genai.register_prompt(\n",
    "            name=prompt_name,\n",
    "            template=template,\n",
    "            tags=tags or {},\n",
    "            commit_message=commit_message or f\"Created {prompt_name}\"\n",
    "        )\n",
    "        print(f\"Created: {prompt_name} v{prompt.version}\")\n",
    "        return prompt\n",
    "    except (ImportError, AttributeError):\n",
    "        # Fallback for MLflow 2.21-2.x clients\n",
    "        try:\n",
    "            import warnings\n",
    "            with warnings.catch_warnings():\n",
    "                warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "                prompt = mlflow.register_prompt(\n",
    "                    name=prompt_name,\n",
    "                    template=template,\n",
    "                    tags=tags or {},\n",
    "                    commit_message=commit_message or f\"Created {prompt_name}\"\n",
    "                )\n",
    "            print(f\"Created: {prompt_name} v{prompt.version}\")\n",
    "            return prompt\n",
    "        except Exception as e:\n",
    "            print(f\"Failed: {prompt_name} - {e}\")\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        print(f\"Failed: {prompt_name} - {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "create-test-prompts",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_test_prompts():\n",
    "    prompts_to_create = [\n",
    "        {\n",
    "            \"name\": f\"summarization-prompt-{tracking_server_name}\",\n",
    "            \"template\": \"Summarize the following text in {{max_words}} words or less:\\n\\n{{text}}\",\n",
    "            \"tags\": {\"task\": \"summarization\", \"version\": \"v1\"},\n",
    "            \"commit_message\": \"Initial summarization prompt\"\n",
    "        },\n",
    "        {\n",
    "            \"name\": f\"qa-prompt-{tracking_server_name}\",\n",
    "            \"template\": \"Answer the following question based on the context:\\n\\nContext: {{context}}\\n\\nQuestion: {{question}}\\n\\nAnswer:\",\n",
    "            \"tags\": {\"task\": \"qa\", \"version\": \"v1\"},\n",
    "            \"commit_message\": \"Initial Q&A prompt\"\n",
    "        },\n",
    "        {\n",
    "            \"name\": f\"translation-prompt-{tracking_server_name}\",\n",
    "            \"template\": \"Translate the following text from {{source_lang}} to {{target_lang}}:\\n\\n{{text}}\",\n",
    "            \"tags\": {\"task\": \"translation\", \"version\": \"v1\"},\n",
    "            \"commit_message\": \"Initial translation prompt\"\n",
    "        },\n",
    "        {\n",
    "            \"name\": f\"classification-prompt-{tracking_server_name}\",\n",
    "            \"template\": \"Classify the following text into one of these categories: {{categories}}\\n\\nText: {{text}}\\n\\nCategory:\",\n",
    "            \"tags\": {\"task\": \"classification\", \"version\": \"v1\"},\n",
    "            \"commit_message\": \"Initial classification prompt\"\n",
    "        },\n",
    "        {\n",
    "            \"name\": f\"code-generation-prompt-{tracking_server_name}\",\n",
    "            \"template\": \"Generate {{language}} code for the following task:\\n\\n{{task_description}}\\n\\nCode:\",\n",
    "            \"tags\": {\"task\": \"code-generation\", \"version\": \"v1\"},\n",
    "            \"commit_message\": \"Initial code generation prompt\"\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    created_prompts = []\n",
    "    for prompt_config in prompts_to_create:\n",
    "        prompt = create_prompt(\n",
    "            prompt_name=prompt_config[\"name\"],\n",
    "            template=prompt_config[\"template\"],\n",
    "            tags=prompt_config[\"tags\"],\n",
    "            commit_message=prompt_config[\"commit_message\"]\n",
    "        )\n",
    "        if prompt:\n",
    "            created_prompts.append(prompt)\n",
    "    \n",
    "    print(f\"\\nCreated {len(created_prompts)} prompts\")\n",
    "    return created_prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "create-prompt-versions",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_prompt_versions():\n",
    "    prompt_name = f\"multi-version-prompt-{tracking_server_name}\"\n",
    "    versions = [\n",
    "        {\"template\": \"Version 1: Simple prompt with {{variable}}\", \"tags\": {\"version\": \"v1\", \"status\": \"deprecated\"}, \"commit_message\": \"Initial version\"},\n",
    "        {\"template\": \"Version 2: Improved prompt with {{variable}} and {{context}}\", \"tags\": {\"version\": \"v2\", \"status\": \"testing\"}, \"commit_message\": \"Added context parameter\"},\n",
    "        {\"template\": \"Version 3: Production-ready prompt with {{variable}}, {{context}}, and {{instructions}}\", \"tags\": {\"version\": \"v3\", \"status\": \"production\"}, \"commit_message\": \"Production release\"}\n",
    "    ]\n",
    "    \n",
    "    created_versions = []\n",
    "    for version_config in versions:\n",
    "        prompt = create_prompt(prompt_name, version_config[\"template\"], version_config[\"tags\"], version_config[\"commit_message\"])\n",
    "        if prompt:\n",
    "            created_versions.append(prompt)\n",
    "    \n",
    "    print(f\"\\nCreated {len(created_versions)} versions of {prompt_name}\")\n",
    "    return created_versions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "run-prompt-creation",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create sample prompts for migration testing\n",
    "prompts = create_test_prompts()\n",
    "versions = create_prompt_versions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37800b06-bddc-442a-9d14-b1f9a64ecbc0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
