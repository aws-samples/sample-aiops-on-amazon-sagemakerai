{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Agent offline Evaluation Dataset (Optional)\n",
    "This notebook demonstrates how to structure and generate an agent evaluation dataset using LangGraph.\n",
    "Such datasets let you systematically benchmark agent and tool performance before production deployment.\n",
    "\n",
    "> Note: An example dataset created in this notebook is already available at `./data/agent_evaluation_dataset.json`. You can use it directly or customize by following the instructions below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install following dependencies. Ignore any warnings\n",
    "!pip install --upgrade -r requirements.txt -q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reusing LangGraph Agent & Tool Setup\n",
    "Import existing LangGraph IT support agent. \n",
    "\n",
    "This setup mirrors agent logic from your main workflow you used in the sagemaker-mlflow-agents-introduction notebook lab. This re-use of the agent ensures the evaluation dataset matches real-world interactions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-08T23:32:32.604400Z",
     "iopub.status.busy": "2025-10-08T23:32:32.604117Z",
     "iopub.status.idle": "2025-10-08T23:32:32.671600Z",
     "shell.execute_reply": "2025-10-08T23:32:32.670864Z",
     "shell.execute_reply.started": "2025-10-08T23:32:32.604381Z"
    }
   },
   "outputs": [],
   "source": [
    "# Reuse the LangGraph Agents for ETL error resolution in 04-sagemaker-mlflow-agents-introduction\n",
    "\n",
    "import boto3\n",
    "import os\n",
    "\n",
    "## Import Sample Data \n",
    "from data.data import log_data_set, log_data\n",
    "from data.solution_book import solution_book\n",
    "\n",
    "import mlflow\n",
    "\n",
    "## Check AWS Credentials \n",
    "try:\n",
    "    boto3.client('bedrock-runtime')\n",
    "except Exception as e:\n",
    "    print(f\"Error configuring AWS credentials: {e}\")\n",
    "    print(\"Please set your AWS credentials before proceeding.\")\n",
    "\n",
    "## Set up MLflow \n",
    "# tracking_server_arn = \"ENTER YOUR MLFLOW TRACKING SERVER ARN HERE\" \n",
    "# experiment_name = \"agent-mlflow-demo\"\n",
    "# mlflow.set_tracking_uri(tracking_server_arn) \n",
    "# mlflow.set_experiment(experiment_name)\n",
    "\n",
    "# ## Set up LangChain Autolog \n",
    "# mlflow.langchain.autolog()\n",
    "\n",
    "## Import LangGraph Packages\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "from langchain.chat_models import init_chat_model\n",
    "from langchain_core.tools import tool\n",
    "\n",
    "## Define LangGraph Tools \n",
    "@tool \n",
    "def log_identifier(ticket_id: str) -> str:\n",
    "    \"\"\"Get error type from ticket number\n",
    "\n",
    "    Args:\n",
    "        ticket_id: ticket id\n",
    "\n",
    "    Returns:\n",
    "        an error type\n",
    "\n",
    "    \"\"\"\n",
    "    if ticket_id not in log_data_set:\n",
    "        return \"ticket id not found in the database\"\n",
    "    \n",
    "    for item in log_data:\n",
    "        if item[\"id\"] == ticket_id:\n",
    "            return item['error_name']\n",
    "\n",
    "@tool(return_direct=True)\n",
    "def information_retriever(error_type: str) -> str:\n",
    "    \"\"\"Retriever error solution based on error type\n",
    "\n",
    "    Args:\n",
    "        error_type: user input error type\n",
    "    \n",
    "    Returns:\n",
    "        a str of steps \n",
    "    \"\"\"\n",
    "\n",
    "    if error_type not in solution_book.keys():\n",
    "        return \"error type not found in the knowledge base, please use your own knowledge\"\n",
    "    \n",
    "    return solution_book[error_type]\n",
    "\n",
    "## Init LLM \n",
    "llm = init_chat_model(\n",
    "    model= \"us.anthropic.claude-3-5-haiku-20241022-v1:0\",\n",
    "    model_provider=\"bedrock_converse\",\n",
    ")\n",
    "\n",
    "## Build System Prompt \n",
    "system_prompt = \"\"\"\n",
    "You are an expert a resolving ETL errors. You are equiped with two tools: \n",
    "1. log_identifier: Get error type from ticket number\n",
    "2. information_retriever: Retriever error solution based on error type\n",
    "\n",
    "You will use the ticket ID to gather information about the error using the log_identifier tool. \n",
    "Then you should search the database for information on how to resolve the error using the information_retriever tool\n",
    "\n",
    "Return ONLY the numbered steps without any introduction or conclusion. Format as:\n",
    "1. step 1 text\n",
    "2. step 2 text\n",
    "...\n",
    "\"\"\"\n",
    "\n",
    "## Create ReAct Agent \n",
    "agent = create_react_agent(\n",
    "    model=llm,\n",
    "    tools= [log_identifier, information_retriever], \n",
    "    prompt=system_prompt\n",
    ")\n",
    "\n",
    "def get_langGraph_agent_response(user_prompt):\n",
    "    # Prepare input for the agent\n",
    "    agent_input = {\"messages\": [{\"role\": \"user\", \"content\": user_prompt}]}\n",
    "    response = agent.invoke(agent_input)\n",
    "    return response['messages'][-1].content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare and Structure Evaluation Test Cases\n",
    "Define your evaluation test cases, capturing expected agent input/output, tool usage, and stepwise solutions\n",
    "\n",
    "Now we will use the test cases we have defined, to guide the evaluation dataset curation. Below we show one examples for test_case_1\n",
    "\n",
    "Each test case is a dictionary contains separates feilds: \n",
    "- ticket_id: ticket id (this is the input to the agent)\n",
    "- error_name: this is expected error name based on the ticekt id \n",
    "- solution: this is expected solution steps \n",
    "- expected_tools: expected tools to use, store in python list and order matters\n",
    "- expected_arguments: expected arguements for the above tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-08T18:45:06.058791Z",
     "iopub.status.busy": "2025-10-08T18:45:06.058155Z",
     "iopub.status.idle": "2025-10-08T18:45:06.067465Z",
     "shell.execute_reply": "2025-10-08T18:45:06.066871Z",
     "shell.execute_reply.started": "2025-10-08T18:45:06.058763Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'user_prompt': 'Can you help me solve the ticket: TICKET-001?',\n",
       " 'error_name': 'Connection Timeout',\n",
       " 'solution': ['1. Check network connectivity between client and server',\n",
       "  '2. Verify if the server is running and accessible',\n",
       "  '3. Increase the connection timeout settings',\n",
       "  '4. Check for firewall rules blocking the connection',\n",
       "  '5. Monitor network latency and bandwidth'],\n",
       " 'expected_tools': ['log_identifier', 'information_retriever'],\n",
       " 'expected_arguments': [{'ticket_id': 'TICKET-001'},\n",
       "  {'error_type': 'Connection Timeout'}]}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from data.test_cases_mlflow import TEST_CASES\n",
    "\n",
    "# Display first test case\n",
    "TEST_CASES[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Offline Evaluation Dataset\n",
    "Use the agent to answer each test case.\n",
    "We will invoke agent for each of the test cases, we will store them into a dataframe, this dataframe will be used in evaluation. The dataframe will contains: \n",
    "1. inputs: this is the user input\n",
    "2. actual_output: Agent generated output \n",
    "3. expected_output: ground truth output "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-08T18:45:15.110805Z",
     "iopub.status.busy": "2025-10-08T18:45:15.110538Z",
     "iopub.status.idle": "2025-10-08T18:52:41.552691Z",
     "shell.execute_reply": "2025-10-08T18:52:41.552030Z",
     "shell.execute_reply.started": "2025-10-08T18:45:15.110786Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing test cases: 100%|██████████| 11/11 [07:26<00:00, 40.58s/it]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "\n",
    "result = []\n",
    "\n",
    "for test_case in tqdm(TEST_CASES, desc=\"Processing test cases\"):\n",
    "    user_input = test_case['user_prompt']\n",
    "    error_solution = \"\\n\".join(test_case['solution'])\n",
    "    agent_response = get_langGraph_agent_response(user_input)\n",
    "\n",
    "    result.append({\n",
    "        'inputs': user_input,\n",
    "        'actual_output': agent_response,\n",
    "        'expected_output': error_solution\n",
    "    })\n",
    "\n",
    "eval_df = pd.DataFrame(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-08T18:55:28.127661Z",
     "iopub.status.busy": "2025-10-08T18:55:28.127382Z",
     "iopub.status.idle": "2025-10-08T18:55:28.132653Z",
     "shell.execute_reply": "2025-10-08T18:55:28.131886Z",
     "shell.execute_reply.started": "2025-10-08T18:55:28.127639Z"
    }
   },
   "outputs": [],
   "source": [
    "# store eval dataset as json\n",
    "eval_df.to_json(\"data/agent_evaluation_dataset.json\", orient=\"records\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "After your notebook runs it saves the file to data/agent_evaluation_dataset.json, and you will ready for the evaluation workshop lab.\n",
    "1. Check the `data/` folder for the offline evaluation dataset `agent_evaluation_dataset.json`\n",
    "2. Consume the offline evaluation dataset `agent_evaluation_dataset.json` in your evaluation step "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
