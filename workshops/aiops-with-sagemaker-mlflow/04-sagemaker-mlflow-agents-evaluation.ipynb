{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agent offline Evalution with MLflow \n",
    "\n",
    "Evaluation is a critical methodology used to assess the performance of AI agents using historical groundtruth data. This approach serves multiple essential purposes in the development and deployment lifecycle of AI systems. By testing agent behavior in a controlled environment, users can mitigate risks, avoid costly mistakes, and protect against potential negative impacts in production environments.\n",
    "\n",
    "MLflow provides comprehensive tools and functionalities for experiment tracking, allowing developers to log metrics, parameters, and artifacts while comparing different agent versions. This tutorial we will introduce you how to do offline evaluation on MLflow. We will use the same example we showed in the previous sections\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import os\n",
    "\n",
    "import sagemaker_mlflow\n",
    "import mlflow\n",
    "import pandas as pd \n",
    "\n",
    "tracking_server_arn = \"ENTER YOUR MLFLOW TRACKIHG SERVER ARN HERE\" \n",
    "experiment_name = \"agent-mlflow-demo\"\n",
    "mlflow.set_tracking_uri(tracking_server_arn) \n",
    "mlflow.set_experiment(experiment_name)\n",
    "\n",
    "mlflow.langchain.autolog()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you build your Agent, you might want to test your agent performance based on some pre-defined groudtruth dataset. The following example shows the user inputs, agent actual outputs and expected outputs. You can find the dataset creation at [04-how-to-create-evaluation-dataset.ipynb](./04-how-to-create-evaluation-dataset.ipynb)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_df = pd.DataFrame(\n",
    "    {\n",
    "        \"user_input\": [\n",
    "            \"Can you help me solve the ticket: TICKET-001?\",\n",
    "            \"Can you help me with this ticket id: TICKET-002?\",\n",
    "            \"I got a ticket: TICKET-003, can you help me with this?\",\n",
    "            \"Help me solve the ticekt: TICKET-004\"\n",
    "        ],\n",
    "        \"actual_output\": [\n",
    "            '1. Check network connectivity between client and server\\n2. Verify if the server is running and accessible\\n3. Increase the connection timeout settings\\n4. Check for firewall rules blocking the connection\\n5. Monitor network latency and bandwidth\\n    ',\n",
    "            '1. Verify database credentials are correct\\n2. Check if the database user account is locked\\n3. Ensure database service is running\\n4. Review database access permissions\\n5. Check for recent password changes\\n    ',\n",
    "            '1. Remove temporary and unnecessary files\\n2. Implement log rotation\\n3. Archive old data\\n4. Expand disk space\\n5. Monitor disk usage trends\\n    ',\n",
    "            '1. Review user access rights\\n2. Check file and directory permissions\\n3. Verify group memberships\\n4. Update security policies\\n5. Audit access control lists\\n    '\n",
    "        ],\n",
    "        \"expected_output\":[\n",
    "            '1. Check network connectivity between client and server\\n2. Verify if the server is running and accessible\\n3. Increase the connection timeout settings\\n4. Check for firewall rules blocking the connection\\n5. Monitor network latency and bandwidth\\n    ',\n",
    "            '1. Verify database credentials are correct\\n2. Check if the database user account is locked\\n3. Ensure database service is running\\n4. Review database access permissions\\n5. Check for recent password changes\\n    ',\n",
    "            '1. Remove temporary and unnecessary files\\n2. Implement log rotation\\n3. Archive old data\\n4. Expand disk space\\n5. Monitor disk usage trends\\n    ',\n",
    "            '1. Review user access rights\\n2. Check file and directory permissions\\n3. Verify group memberships\\n4. Update security policies\\n5. Audit access control lists\\n    '\n",
    "        ]\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create evaluation metrics for MLflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MLflow have both built-in [Heuristic](https://mlflow.org/docs/2.21.3/llms/llm-evaluate/#heuristic-based-metrics) and [LLM as a Judge metrics](https://mlflow.org/docs/2.21.3/llms/llm-evaluate/#llm-as-a-judge-metrics). In this workflow, we will show you one example using [ROUGE score](https://huggingface.co/spaces/evaluate-metric/rouge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "rouge_metric = mlflow.metrics.rougeL()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLflow Custom Metrics: \n",
    "\n",
    "While MLflow has some built-in [Heuristic](https://mlflow.org/docs/2.21.3/llms/llm-evaluate/#heuristic-based-metrics) and [LLM as a Judge metrics](https://mlflow.org/docs/2.21.3/llms/llm-evaluate/#llm-as-a-judge-metrics), those metrics are limited and may not be able to fit all situations. This section we will show you how to build your own custom metrics.\n",
    "\n",
    "1. You need to define your custom metric method with two arguments `predictions` and `targets`. \"predictions\" is a list of predicted results from you agent and \"targets\" is a list of ground truth. You can define any custom certeria for judgement. Here we design it to be exact match \n",
    "2. Use `make_metrics` method to make our customer metrics to be a MLflow metrics. Notices that here we use `greater_is_better = True` This indicate the score is higher the better. In future if you have similar metrics like incorrectness, you can set `greater_is_better = False`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlflow.metrics import MetricValue \n",
    "from mlflow.models import make_metric\n",
    "\n",
    "def custom_correctness(predictions, targets):\n",
    "    '''\n",
    "    custom correctness based on exact match \n",
    "\n",
    "    Args:\n",
    "        precitions: list of agent predicted values \n",
    "        targets: list of ground truth values \n",
    "\n",
    "    Returns: \n",
    "        A list of MetricValue scores\n",
    "    '''\n",
    "    scores = [] \n",
    "\n",
    "    for prediction, target in zip(predictions, targets):\n",
    "        if prediction.strip() == target.strip():\n",
    "            scores.append(1)\n",
    "        else:\n",
    "            scores.append(0)\n",
    "\n",
    "    return MetricValue(scores= scores)\n",
    "\n",
    "custom_correctness_metric = make_metric(\n",
    "    eval_fn=custom_correctness, greater_is_better= True , name=\"custom_answer_correctness\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can use `mlflow.evaluate()` to see the performance of the chain on an evaluation dataset we created. we will use both build in metrics and custom metric \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/09/27 11:36:03 INFO mlflow.models.evaluation.default_evaluator: Testing metrics on first row...\n",
      "/opt/anaconda3/envs/mlflow/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üèÉ View run skittish-pug-937 at: https://us-east-1.experiments.sagemaker.aws/#/experiments/1/runs/3aa375956d0b42cf8b4d0fe35c5a167d\n",
      "üß™ View experiment at: https://us-east-1.experiments.sagemaker.aws/#/experiments/1\n"
     ]
    }
   ],
   "source": [
    "with mlflow.start_run() as evaluation_run:\n",
    "    eval_dataset = mlflow.data.from_pandas(\n",
    "        df=eval_df,\n",
    "        name=\"eval_dataset\",\n",
    "        targets=\"expected_output\",\n",
    "        predictions=\"actual_output\",\n",
    "    )\n",
    "    mlflow.log_input(dataset=eval_dataset)\n",
    "\n",
    "\n",
    "    result = mlflow.evaluate(\n",
    "        data=eval_dataset,\n",
    "        extra_metrics=[\n",
    "            custom_correctness_metric,\n",
    "            rouge_metric\n",
    "        ]\n",
    "    )   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Offline Evaluation Results\n",
    "\n",
    "The results will be attached to your evaluation dataset frameworks with additional columns for scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "See aggregated evaluation results below: \n",
      "{'custom_answer_correctness/mean': np.float64(1.0), 'custom_answer_correctness/variance': np.float64(0.0), 'custom_answer_correctness/p90': np.float64(1.0), 'rougeL/v1/mean': np.float64(1.0), 'rougeL/v1/variance': np.float64(0.0), 'rougeL/v1/p90': np.float64(1.0)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading artifacts: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  9.50it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_input</th>\n",
       "      <th>expected_output</th>\n",
       "      <th>actual_output</th>\n",
       "      <th>custom_answer_correctness/score</th>\n",
       "      <th>rougeL/v1/score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Can you help me solve the ticket: TICKET-001?</td>\n",
       "      <td>1. Check network connectivity between client a...</td>\n",
       "      <td>1. Check network connectivity between client a...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Can you help me with this ticket id: TICKET-002?</td>\n",
       "      <td>1. Verify database credentials are correct\\n2....</td>\n",
       "      <td>1. Verify database credentials are correct\\n2....</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I got a ticket: TICKET-003, can you help me wi...</td>\n",
       "      <td>1. Remove temporary and unnecessary files\\n2. ...</td>\n",
       "      <td>1. Remove temporary and unnecessary files\\n2. ...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Help me solve the ticekt: TICKET-004</td>\n",
       "      <td>1. Review user access rights\\n2. Check file an...</td>\n",
       "      <td>1. Review user access rights\\n2. Check file an...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          user_input  \\\n",
       "0      Can you help me solve the ticket: TICKET-001?   \n",
       "1   Can you help me with this ticket id: TICKET-002?   \n",
       "2  I got a ticket: TICKET-003, can you help me wi...   \n",
       "3               Help me solve the ticekt: TICKET-004   \n",
       "\n",
       "                                     expected_output  \\\n",
       "0  1. Check network connectivity between client a...   \n",
       "1  1. Verify database credentials are correct\\n2....   \n",
       "2  1. Remove temporary and unnecessary files\\n2. ...   \n",
       "3  1. Review user access rights\\n2. Check file an...   \n",
       "\n",
       "                                       actual_output  \\\n",
       "0  1. Check network connectivity between client a...   \n",
       "1  1. Verify database credentials are correct\\n2....   \n",
       "2  1. Remove temporary and unnecessary files\\n2. ...   \n",
       "3  1. Review user access rights\\n2. Check file an...   \n",
       "\n",
       "   custom_answer_correctness/score  rougeL/v1/score  \n",
       "0                                1                1  \n",
       "1                                1                1  \n",
       "2                                1                1  \n",
       "3                                1                1  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f\"See aggregated evaluation results below: \\n{result.metrics}\")\n",
    "result.tables[\"eval_results_table\"]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
