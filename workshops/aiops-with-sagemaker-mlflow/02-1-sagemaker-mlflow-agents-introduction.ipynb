{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SageMaker AI Managed MLflow Agent Introduction\n",
    "Welcome to the hands-on SageMaker AI Managed MLflow workshop lab! This notebook will guide you through how to create an agent via LangGraph framework and use MLflow to log trace. We'll build an intelligent ETL error resolution Agent that can analyze error tickets and provide step-by-step solutions. Our agent will be equipped with two agent tools (log_identifer, information_retriever) to: \n",
    "1. Identify errors from ticket IDs \n",
    "2. Retrieve relevant solutions from a solution book.\n",
    "\n",
    "#### Prerequisites\n",
    "\n",
    "**Important:** You need the SageMaker AI Managed MLflow tracking server ARN from the workshop prerequisites section. Replace the placeholder below with your actual tracking server ARN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building Intelligent IT support Agents with MLflow \n",
    "\n",
    "In this notebook lab, you will develop an IT support agent where the agent is designed for taking user IT support questions, automated ETL (Extract, Transform, Load) of the support tickets and provide actionable resolution to the user question. The support tickets are regarding issues of an IT system and the support agent helps users with ticket resolution. This intelligent agent is implemented via the LangGraph framework and instrumented with MLflow tracing, demonstrating how modern agentic architectures can elevate IT system incident response and operational reliability.\n",
    "1. Identify errors from ticket IDs \n",
    "2. Retrieve relevant solutions from a solution book."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Sample_Architect](./static/05-ITSupportAgentTracing.png) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Scenario Overview\n",
    "The agent is built to assist data engineers or support staff by analyzing helpdesk tickets reporting ETL pipeline errors. Upon receiving a ticket ID, it will diagnose the underlying issue and deliver targeted, step-by-step solutions—reducing downtime and improving user satisfaction.\n",
    "\n",
    "**Agent Tools**\n",
    "- `log_identifier`: Analyzes supplied ticket IDs and extracts error type, diagnostics, and metadata from pipeline logs.\n",
    "- `information_retriever`: Looks up solutions in a curated solution book and compiles actionable instructions for error resolution and mitigation.\n",
    "\n",
    "Here is the ideal resolution steps:  \n",
    "1. The user requests assistance by providing a `ticket_id`.\n",
    "2. The agent calls the `log_identifier` tool to classify the error from backend logs associated with the ticket.\n",
    "3. The agent invokes the information_retriever tool to fetch detailed solution steps based on error type and context.\n",
    "4. The user receives clear, actionable step-by-step instructions for resolving the ETL error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data and libaries "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install required libraries. You might get some dependency conflict errors but it shouldn't affect the functionality of the rest of this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Install following dependencies. Ignore any warnings and residual dependency errors.\n",
    "!pip install -r requirements-langgraph.txt -q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Note: The latest MLflow version (November, 2025 at the time of this workshop release) supported in Sagemaker AI Managed MLflow tracking server is MLflow 3.0.0 and python 3.9 or later, You can find more information [here](https://docs.aws.amazon.com/sagemaker/latest/dg/mlflow.html).\n",
    "> To make sure you can successfuly run this notebook you will need use the compatible version, i.e. install `sagemaker_mlflow==0.1.0` and `mlflow=>3.0.0`. This is already provided as part of the `requirements.txt` installation. The Sagemaker AI Managed MLflow tracking server can be different than the mlflow SDK python version used as long as the APIs used in the Sagemaker AI Managed MLflow tracking server MLflow versions are supported. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import os\n",
    "\n",
    "## Import Data \n",
    "from data.data import log_data_set, log_data\n",
    "from data.solution_book import solution_book\n",
    "\n",
    "## Import MLflow Libs\n",
    "import sagemaker_mlflow\n",
    "import mlflow\n",
    "\n",
    "## Check AWS Credentials \n",
    "try:\n",
    "    boto3.client('bedrock-runtime')\n",
    "except Exception as e:\n",
    "    print(f\"Error configuring AWS credentials: {e}\")\n",
    "    print(\"Please set your AWS credentials before proceeding.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's examine our sample synthesis data that represents a typical support ticket system. `log_data` is a list of dictionaries contains ticked id and error name, for this workshop, we will use a simplified version to extract the error_name based on the ticket_id. `solution_book` is a dictionary where the key is the error name, the value is solution steps. Here we use the dictionary to mimic the real world use case solution which normally use Vector Store Knowledge Base to retrieve relevant solutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "solution_book"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sagemaker AI MLflow\n",
    "We will configure MLflow integration with Amazon SageMaker AI for tracking and tracing agent-based workflows. Frist we establishes a connection to a specific SageMaker AI MLflow Tracking Server using an ARN (Amazon Resource Name). \n",
    "The `mlflow.set_tracking_uri()` method directs all subsequent MLflow logging operations to this server, ensuring experiment metadata is stored in the secure backend maintained by SageMaker AI.\n",
    "\n",
    "You can organize all your MLFlow observibility data by setting `mlflow.set_experiment(\"<YOUR_MLFLOW_EXPERIMENT_NAME>\")`. MLflow allows you to group runs under experiments which can be useful for comparing runs intended to tackle a particular task.\n",
    "\n",
    "We will retrieve the stored notebook values containing the SageMaker AI MLflow Tracking Server ARN. If the stored value is empty you can enter your tracking server arn to the following place holder string `TRACKING_SERVER_ARN`. Additionally, you can give any MLflow experiment name, in this workshop, we will give an experiment name: agent-mlflow-demo "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve values stored from previous labs\n",
    "# If the stored value (or if you get NameError) is empty, set your SageMaker AI Managed MLflow tracking server ARN copied from prerequisites\n",
    "%store -r \n",
    "\n",
    "%store\n",
    "if TRACKING_SERVER_ARN == \"\":\n",
    "    print(\"ENTER YOUR MLFLOW TRACKING SERVER ARN\")\n",
    "TRACKING_SERVER_ARN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If the stored value is empty set your SageMaker AI Managed MLflow tracking server ARN by uncommenting the below line\n",
    "#TRACKING_SERVER_ARN = \"ENTER YOUR MLFLOW TRACKIHG SERVER ARN HERE\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_name = \"agent-mlflow-demo\"\n",
    "\n",
    "# Set MLflow SDK to your configured tracking server \n",
    "mlflow.set_tracking_uri(TRACKING_SERVER_ARN) \n",
    "# Create or select an MLflow experiment\n",
    "mlflow.set_experiment(experiment_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we will use LangGraph agent, let's set up the auto tracing for LangGraph. \n",
    "\n",
    "> `mlflow.langchain.autolog()` is a function within the MLflow LangChain flavor that enables automatic logging of crucial details about LangChain models and their execution. This feature simplifies experiment tracking and analysis by eliminating the need for explicit logging statements. By default, `mlflow.langchain.autolog()`automatically logs traces of your LangChain components, providing a visual representation of data flow through chains, agents, and retrievers. This includes invocations of methods like invoke, batch, stream, ainvoke, abatch, astream, get_relevant_documents (for retrievers), and `__call__` (for Chains and AgentExecutors).\n",
    "\n",
    "> Note: You can use the the generic high-level autologging function mlflow.autolog() to capture the traces, however, when mlflow.autolog() called, it attempts to enable autologging for all supported standard ML libraries you have installed including the agentic libraries which can make it difficuilt to debug agentic behaviour and deviates the primary focus of helping with agentic framework tracing. Where you use the agentic framework specific autolog like mlflow.langchain.autolog(), Traces (detailed execution flow) are the primary log item with scope specific to focuse only on the agentic framework integration like LangChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mlflow.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.langchain.autolog()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialize sample data for the agent tool \n",
    "To build a practical ETL error resolution agent, we first need to provide sample lookup data for each tool operation. In this demonstration:\n",
    "- `log_data` mimics a support ticket backend, mapping ticket IDs to error types.\n",
    "- `solution_book` acts as a simple knowledge base, mapping error names to resolution steps.\n",
    "\n",
    "In production, these would typically be external databases, knowledge stores, or APIs. Here, we simplify with in-notebook Python objects for demonstration clarity.\n",
    "\n",
    "> Note: This sample data demonstrates a typical support ticket system. `log_data` is a list of dictionaries contains ticked id and error name, for this workshop lab, we will use a simplified version to extract the error_name based on the ticket_id. `solution_book` is a python dictionary where the key is the error name, the value is resolution steps. Here we use the dictionary to mimic the real world use case solution which normally use data store like a database or a Vector Store Knowledge Base to retrieve relevant solutions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize sample log data\n",
    "# Each entry in log_data is a ticket with a unique ID and its error type\n",
    "log_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize sample solution book\n",
    "# Each error type maps to a list of step-by-step instructions for resolution\n",
    "solution_book"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These staged dictionaries form the dynamic data foundation for the agent tools you’ll implement and invoke within your LangGraph workflow:\n",
    "- The log_identifier tool will look up a ticket ID in log_data to extract the relevant error type.\n",
    "- The information_retriever tool will fetch detailed, actionable remediation steps from the solution_book, given that error type.\n",
    "\n",
    "> In a real-world scenario, these would likely be microservices, API calls, or vector search queries to production data stores. For this workshop, the simple data structures let you focus on agent logic, tool call orchestration, and MLflow-powered trace monitoring.\n",
    "\n",
    "You’re now ready to define, register, and test your agent tools in the LangGraph agent framework."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining Agent Tools in LangGraph\n",
    "In LangGraph, agent tools let your LLM-powered agent interact with the world beyond language—retrieving information, performing searches, or calling APIs. Tools are simply Python functions, but with the @tool decorator they become part of the agent’s action space: the agent can decide when and how to use them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First Lets import libaries from LangGraph Framework. here: \n",
    "1. `create_react_agent`: Create an agent that uses ReAct prompting.\n",
    "2. `init_chat_model`: Initialize a ChatModel in a single line using the model’s name and provider.\n",
    "3. `langchain_core.tools`: Tool that takes in function or coroutine directly. You can customize your tool with `@tool` decoration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.prebuilt import create_react_agent\n",
    "from langchain.chat_models import init_chat_model\n",
    "from langchain_core.tools import tool"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The log_identifier Tool\n",
    "This function allows the agent to look up the error type associated with a given support ticket ID. This is your agent’s primary hook into your simulated support system.\n",
    "- The @tool decorator registers this as an agent-available tool. The @tool decorator is core: it annotates a function so that your agent can see (and use) it as part of its available toolkit.\n",
    "- The agent will call this function when it needs to map a ticket ID to a specific error type.\n",
    "- A clear docstring is critical: the LLM references these descriptions to decide when the tool is appropriate to use.\n",
    "- The function loops through log_data to find the corresponding error, allowing easy simulation and interpretation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool \n",
    "def log_identifier(ticket_id: str) -> str:\n",
    "    \"\"\"Get error type from ticket number\n",
    "\n",
    "    Args:\n",
    "        ticket_id: ticket id\n",
    "\n",
    "    Returns:\n",
    "        an error type\n",
    "\n",
    "    \"\"\"\n",
    "    if ticket_id not in log_data_set:\n",
    "        return \"ticket id not found in the database\"\n",
    "    \n",
    "    for item in log_data:\n",
    "        if item[\"id\"] == ticket_id:\n",
    "            return item['error_name']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The information_retriever Tool\n",
    "After an error type is found, the agent can fetch the appropriate, step-by-step solution using this retrieval tool:\n",
    "- The @tool(return_direct=True) tells LangGraph to return this tool’s output to the user directly—useful for final answers (e.g., resolution steps).\n",
    "- Like before, the docstring describes the tool’s intent and usage.\n",
    "- This tool looks up solution steps as a string, making it perfect for the agent to hand off clear resolution instructions directly to the end-user.\n",
    "- If the error type isn't found, it provides a safe fallback message."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool(return_direct=True)\n",
    "def information_retriever(error_type: str) -> str:\n",
    "    \"\"\"Retriever error solution based on error type\n",
    "\n",
    "    Args:\n",
    "        error_type: user input error type\n",
    "    \n",
    "    Returns:\n",
    "        a str of steps \n",
    "    \"\"\"\n",
    "\n",
    "    if error_type not in solution_book.keys():\n",
    "        return \"error type not found in the knowledge base, please use your own knowledge\"\n",
    "    \n",
    "    return solution_book[error_type]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agent Workflow:\n",
    "- Agent receives a user input (e.g., help request with ticket id)\n",
    "- Calls log_identifier to transform the ticket id to an error type\n",
    "- Uses information_retriever to fetch actionable guidance for the error\n",
    "- Returns steps directly to the user (since return_direct=True)\n",
    "\n",
    "You can now proceed to wire these tools into your LangGraph agent with create_react_agent. Each tool call will be traced in MLflow for transparency and assessment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialize Language Model with Amazon Bedrock\n",
    "We use the init_chat_model function to create a consistent interface for initializing various LLM providers. In this workshop, we'll use Amazon Bedrock anthropic Claude model as our LLM to power the agent's reasoning and decision-making."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Note: You will need IAM permissions to access to Amazon bedrock model with Cross-region inference. If you completed the pre-requisite section you will have access to the bedrock model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = init_chat_model(\n",
    "    model= \"global.anthropic.claude-haiku-4-5-20251001-v1:0\",\n",
    "    model_provider=\"bedrock_converse\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- model: Specifies the exact model identifier for the agent. Here a Amazon Bedrock anthropic Claude model.\n",
    "- model_provider: Indicates the provider service, here bedrock_converse.\n",
    "\n",
    "This LLM instance will be the brain of the agent, generating responses, deciding tool invocations, and following the reasoning workflow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define the System Prompt for Agent Behavior\n",
    "The system prompt acts as the agent’s personality and instruction manual. We define it clearly to instruct the agent that it is an expert in resolving ETL errors, equipped with two tools, and specify the expected workflow and output formatting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The system prompt acts as the agent’s personality and instruction manual. We define it clearly to instruct the agent that it is an expert in resolving ETL errors, equipped with two tools, and specify the expected workflow and output formatting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"\"\"\n",
    "You are an expert a resolving ETL errors. You are equiped with two tools: \n",
    "1. log_identifier: Get error type from ticket number\n",
    "2. information_retriever: Retriever error solution based on error type\n",
    "\n",
    "You will use the ticket ID to gather information about the error using the log_identifier tool. \n",
    "Then you should search the database for information on how to resolve the error using the information_retriever tool\n",
    "\n",
    "Return ONLY the numbered steps without any introduction or conclusion. Format as:\n",
    "1. step 1 text\n",
    "2. step 2 text\n",
    "...\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- This prompt ensures the agent uses the right tools in sequence.\n",
    "- It emphasizes output format consistency, making it easier for end users to follow.\n",
    "- Provides context to the LLM for optimized, goal-directed behavior."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We next utilize the create_react_agent function, which builds a ReAct agent: a Reasoning and Acting agent that can iteratively decide which tool to call and what responses to generate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = create_react_agent(\n",
    "    model=llm,\n",
    "    tools= [log_identifier, information_retriever], \n",
    "    prompt=system_prompt\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- model: The LLM instance driving the agent.\n",
    "- tools: List of pre-defined tools (functions marked with @tool) the agent can call during reasoning.\n",
    "- prompt: The system prompt that sets agent instructions and behavior.\n",
    "\n",
    "`create_react_agent` abstracts away the complexity of integrating multiple tools and manages the reasoning cycle per the ReAct paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (Optional) Draw agent graph to local file \n",
    "# agent.get_graph(xray=True).draw_mermaid_png(\n",
    "#     output_file_path=\"graph_diagram.png\",  # Specify where to save the PNG image\n",
    "#     background_color=\"white\", \n",
    "#     padding=10\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's define a clean interface for interacting with our agent. It formats the input as a message (following the chat format), invokes the agent, and extracts the final response content. The agent will automatically use the tools in the correct sequence to resolve the ticket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_langGraph_agent_response(user_prompt):\n",
    "    # Prepare input for the agent\n",
    "    agent_input = {\"messages\": [{\"role\": \"user\", \"content\": user_prompt}]}\n",
    "    response = agent.invoke(agent_input)\n",
    "    return response['messages'][-1].content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we test our agent with a sample user prompt including the ticket ID. The agent should:\n",
    "\n",
    "1. Use the log_identifier tool to find that TICKET-001 corresponds to \"Connection Timeout\"\n",
    "2. Use the information_retriever tool to get the solution steps\n",
    "3. Return the formatted solution steps to the user\n",
    "\n",
    "When you run the next code cell, you will see the agent reponse to the user prompt with a numbered list of steps for resolving the ticket issue, demonstrating that your agent successfully chained the tools together to provide a complete solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "langGraph_agent_response = get_langGraph_agent_response(user_prompt = 'Can you help me with this ticket_id : TICKET-001?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(langGraph_agent_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you will be able to see the tracing in the MLflow. \n",
    "\n",
    "1. Open your SageMaker AI Managed MLflow UI\n",
    "2. In the MLflow UI navigate to the `Traces` tab to see traces of the agent’s reasoning steps and tool calls, enabling rich observability for analysis and debugging.\n",
    "3. If you used the default values in this notebook, your MLflow experiment will `experiment_name = \"agent-mlflow-demo\"`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
