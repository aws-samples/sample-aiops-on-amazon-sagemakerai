{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eafea00f",
   "metadata": {},
   "source": [
    "## Evaluating LLMs with MLflow 📏\n",
    "\n",
    "MLflow provides a robust framework for tracking and managing machine learning experiments. In this notebook, we will explore how to evaluate large language models (LLMs) using MLflow, focusing on key metrics such as bleu, rouge, and LLM-as-a-judge metrics.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a04f5328",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "import boto3\n",
    "import torch\n",
    "import mlflow\n",
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "from transformers import pipeline\n",
    "from mlflow.metrics import (\n",
    "    bleu,\n",
    "    rouge1,\n",
    "    rouge2,\n",
    "    rougeL,\n",
    "    rougeLsum,\n",
    "    latency,\n",
    "    token_count,\n",
    ")\n",
    "from mlflow.metrics.genai import (\n",
    "    EvaluationExample,\n",
    "    answer_correctness,\n",
    "    answer_relevance,\n",
    "    answer_similarity,\n",
    "    faithfulness,\n",
    "    relevance,\n",
    "    make_genai_metric,\n",
    ")\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import PeftModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ff4c29eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting fixed hyperparameters\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    hyperparameters = json.loads(os.environ[\"SM_HPS\"])\n",
    "    print(\"Hyperparameters:\", hyperparameters)\n",
    "    model_id = hyperparameters.get('model_id', None)\n",
    "    adapter_path = hyperparameters.get('adapter_path', None)\n",
    "    experiment_name = hyperparameters.get('experiment_name', None)\n",
    "    run_name = hyperparameters.get('run_name', None)\n",
    "\n",
    "except:\n",
    "    print(\"Setting fixed hyperparameters\")\n",
    "    model_id = 'Qwen/Qwen3-0.6B'\n",
    "    adapter_path = None\n",
    "    experiment_name = 'qwen3-06b-lora-ft-finance'\n",
    "    run_name = 'base-model-eval'\n",
    "\n",
    "session = boto3.Session(region_name='us-east-1')\n",
    "credentials = session.get_credentials()\n",
    "credentials = credentials.get_frozen_credentials()\n",
    "access_key, secret_key = credentials.access_key, credentials.secret_key\n",
    "\n",
    "if credentials.token:\n",
    "    token = credentials.token\n",
    "    os.environ[\"AWS_SESSION_TOKEN\"] = token\n",
    "\n",
    "os.environ[\"AWS_REGION\"] = \"us-east-1\"\n",
    "os.environ[\"AWS_ACCESS_KEY_ID\"] = access_key\n",
    "os.environ[\"AWS_SECRET_ACCESS_KEY\"] = secret_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dafd8701",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored variables and their in-db values:\n"
     ]
    }
   ],
   "source": [
    "%store -r \n",
    "\n",
    "%store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d35b6f13",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow_tracking_uri = 'arn:aws:sagemaker:us-east-1:364430515305:mlflow-tracking-server/100725'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f097ea34",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.set_tracking_uri(mlflow_tracking_uri)\n",
    "mlflow.set_experiment(experiment_name)\n",
    "mlflow.enable_system_metrics_logging()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "58dc900b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Qwen/Qwen3-0.6B'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8facc307",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# base model\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m pipe = \u001b[43mpipeline\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtext-generation\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mauto\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mauto\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     10\u001b[39m pipe.generation_config.pad_token_id = pipe.tokenizer.eos_token_id\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpredict\u001b[39m(questions):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/sample-aiops-on-amazon-sagemakerai/.venv/lib/python3.12/site-packages/transformers/pipelines/__init__.py:883\u001b[39m, in \u001b[36mpipeline\u001b[39m\u001b[34m(task, model, config, tokenizer, feature_extractor, image_processor, processor, framework, revision, use_fast, token, device, device_map, dtype, trust_remote_code, model_kwargs, pipeline_class, **kwargs)\u001b[39m\n\u001b[32m    879\u001b[39m     pretrained_model_name_or_path = model\n\u001b[32m    881\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(config, PretrainedConfig) \u001b[38;5;129;01mand\u001b[39;00m pretrained_model_name_or_path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    882\u001b[39m     \u001b[38;5;66;03m# We make a call to the config file first (which may be absent) to get the commit hash as soon as possible\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m883\u001b[39m     resolved_config_file = \u001b[43mcached_file\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    884\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    885\u001b[39m \u001b[43m        \u001b[49m\u001b[43mCONFIG_NAME\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    886\u001b[39m \u001b[43m        \u001b[49m\u001b[43m_raise_exceptions_for_gated_repo\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    887\u001b[39m \u001b[43m        \u001b[49m\u001b[43m_raise_exceptions_for_missing_entries\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    888\u001b[39m \u001b[43m        \u001b[49m\u001b[43m_raise_exceptions_for_connection_errors\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    889\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcache_dir\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    890\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    891\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    892\u001b[39m     hub_kwargs[\u001b[33m\"\u001b[39m\u001b[33m_commit_hash\u001b[39m\u001b[33m\"\u001b[39m] = extract_commit_hash(resolved_config_file, commit_hash)\n\u001b[32m    893\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/sample-aiops-on-amazon-sagemakerai/.venv/lib/python3.12/site-packages/transformers/utils/hub.py:322\u001b[39m, in \u001b[36mcached_file\u001b[39m\u001b[34m(path_or_repo_id, filename, **kwargs)\u001b[39m\n\u001b[32m    264\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcached_file\u001b[39m(\n\u001b[32m    265\u001b[39m     path_or_repo_id: Union[\u001b[38;5;28mstr\u001b[39m, os.PathLike],\n\u001b[32m    266\u001b[39m     filename: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m    267\u001b[39m     **kwargs,\n\u001b[32m    268\u001b[39m ) -> Optional[\u001b[38;5;28mstr\u001b[39m]:\n\u001b[32m    269\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    270\u001b[39m \u001b[33;03m    Tries to locate a file in a local folder and repo, downloads and cache it if necessary.\u001b[39;00m\n\u001b[32m    271\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    320\u001b[39m \u001b[33;03m    ```\u001b[39;00m\n\u001b[32m    321\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m322\u001b[39m     file = \u001b[43mcached_files\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilenames\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    323\u001b[39m     file = file[\u001b[32m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m file\n\u001b[32m    324\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m file\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/sample-aiops-on-amazon-sagemakerai/.venv/lib/python3.12/site-packages/transformers/utils/hub.py:479\u001b[39m, in \u001b[36mcached_files\u001b[39m\u001b[34m(path_or_repo_id, filenames, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[39m\n\u001b[32m    476\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    477\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(full_filenames) == \u001b[32m1\u001b[39m:\n\u001b[32m    478\u001b[39m         \u001b[38;5;66;03m# This is slightly better for only 1 file\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m479\u001b[39m         \u001b[43mhf_hub_download\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    480\u001b[39m \u001b[43m            \u001b[49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    481\u001b[39m \u001b[43m            \u001b[49m\u001b[43mfilenames\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    482\u001b[39m \u001b[43m            \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    483\u001b[39m \u001b[43m            \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    484\u001b[39m \u001b[43m            \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    485\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    486\u001b[39m \u001b[43m            \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[43m=\u001b[49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    487\u001b[39m \u001b[43m            \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m=\u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    488\u001b[39m \u001b[43m            \u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m=\u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    489\u001b[39m \u001b[43m            \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    490\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    491\u001b[39m \u001b[43m            \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    492\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    493\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    494\u001b[39m         snapshot_download(\n\u001b[32m    495\u001b[39m             path_or_repo_id,\n\u001b[32m    496\u001b[39m             allow_patterns=full_filenames,\n\u001b[32m   (...)\u001b[39m\u001b[32m    505\u001b[39m             local_files_only=local_files_only,\n\u001b[32m    506\u001b[39m         )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/sample-aiops-on-amazon-sagemakerai/.venv/lib/python3.12/site-packages/huggingface_hub/utils/_validators.py:114\u001b[39m, in \u001b[36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    111\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m check_use_auth_token:\n\u001b[32m    112\u001b[39m     kwargs = smoothly_deprecate_use_auth_token(fn_name=fn.\u001b[34m__name__\u001b[39m, has_token=has_token, kwargs=kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m114\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/sample-aiops-on-amazon-sagemakerai/.venv/lib/python3.12/site-packages/huggingface_hub/file_download.py:1010\u001b[39m, in \u001b[36mhf_hub_download\u001b[39m\u001b[34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, user_agent, force_download, proxies, etag_timeout, token, local_files_only, headers, endpoint, resume_download, force_filename, local_dir_use_symlinks)\u001b[39m\n\u001b[32m    990\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _hf_hub_download_to_local_dir(\n\u001b[32m    991\u001b[39m         \u001b[38;5;66;03m# Destination\u001b[39;00m\n\u001b[32m    992\u001b[39m         local_dir=local_dir,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1007\u001b[39m         local_files_only=local_files_only,\n\u001b[32m   1008\u001b[39m     )\n\u001b[32m   1009\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1010\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_hf_hub_download_to_cache_dir\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1011\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Destination\u001b[39;49;00m\n\u001b[32m   1012\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1013\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# File info\u001b[39;49;00m\n\u001b[32m   1014\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1015\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1016\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1017\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1018\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# HTTP info\u001b[39;49;00m\n\u001b[32m   1019\u001b[39m \u001b[43m        \u001b[49m\u001b[43mendpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mendpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1020\u001b[39m \u001b[43m        \u001b[49m\u001b[43metag_timeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43metag_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1021\u001b[39m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhf_headers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1022\u001b[39m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m=\u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1023\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1024\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Additional options\u001b[39;49;00m\n\u001b[32m   1025\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1026\u001b[39m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m=\u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1027\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/sample-aiops-on-amazon-sagemakerai/.venv/lib/python3.12/site-packages/huggingface_hub/file_download.py:1171\u001b[39m, in \u001b[36m_hf_hub_download_to_cache_dir\u001b[39m\u001b[34m(cache_dir, repo_id, filename, repo_type, revision, endpoint, etag_timeout, headers, proxies, token, local_files_only, force_download)\u001b[39m\n\u001b[32m   1168\u001b[39m \u001b[38;5;66;03m# Local file doesn't exist or etag isn't a match => retrieve file from remote (or cache)\u001b[39;00m\n\u001b[32m   1170\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m WeakFileLock(lock_path):\n\u001b[32m-> \u001b[39m\u001b[32m1171\u001b[39m     \u001b[43m_download_to_tmp_and_move\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1172\u001b[39m \u001b[43m        \u001b[49m\u001b[43mincomplete_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43mPath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mblob_path\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m.incomplete\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1173\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdestination_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43mPath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mblob_path\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1174\u001b[39m \u001b[43m        \u001b[49m\u001b[43murl_to_download\u001b[49m\u001b[43m=\u001b[49m\u001b[43murl_to_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1175\u001b[39m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m=\u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1176\u001b[39m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1177\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexpected_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mexpected_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1178\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1179\u001b[39m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m=\u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1180\u001b[39m \u001b[43m        \u001b[49m\u001b[43metag\u001b[49m\u001b[43m=\u001b[49m\u001b[43metag\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1181\u001b[39m \u001b[43m        \u001b[49m\u001b[43mxet_file_data\u001b[49m\u001b[43m=\u001b[49m\u001b[43mxet_file_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1182\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1183\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os.path.exists(pointer_path):\n\u001b[32m   1184\u001b[39m         _create_symlink(blob_path, pointer_path, new_blob=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/sample-aiops-on-amazon-sagemakerai/.venv/lib/python3.12/site-packages/huggingface_hub/file_download.py:1738\u001b[39m, in \u001b[36m_download_to_tmp_and_move\u001b[39m\u001b[34m(incomplete_path, destination_path, url_to_download, proxies, headers, expected_size, filename, force_download, etag, xet_file_data)\u001b[39m\n\u001b[32m   1731\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m xet_file_data \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m constants.HF_HUB_DISABLE_XET:\n\u001b[32m   1732\u001b[39m             logger.warning(\n\u001b[32m   1733\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mXet Storage is enabled for this repo, but the \u001b[39m\u001b[33m'\u001b[39m\u001b[33mhf_xet\u001b[39m\u001b[33m'\u001b[39m\u001b[33m package is not installed. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1734\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mFalling back to regular HTTP download. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1735\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mFor better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1736\u001b[39m             )\n\u001b[32m-> \u001b[39m\u001b[32m1738\u001b[39m         \u001b[43mhttp_get\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1739\u001b[39m \u001b[43m            \u001b[49m\u001b[43murl_to_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1740\u001b[39m \u001b[43m            \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1741\u001b[39m \u001b[43m            \u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m=\u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1742\u001b[39m \u001b[43m            \u001b[49m\u001b[43mresume_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1743\u001b[39m \u001b[43m            \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1744\u001b[39m \u001b[43m            \u001b[49m\u001b[43mexpected_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mexpected_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1745\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1747\u001b[39m logger.info(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mDownload complete. Moving file to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdestination_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m   1748\u001b[39m _chmod_and_move(incomplete_path, destination_path)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/sample-aiops-on-amazon-sagemakerai/.venv/lib/python3.12/site-packages/huggingface_hub/file_download.py:451\u001b[39m, in \u001b[36mhttp_get\u001b[39m\u001b[34m(url, temp_file, proxies, resume_size, headers, expected_size, displayed_filename, _nb_retries, _tqdm_bar)\u001b[39m\n\u001b[32m    444\u001b[39m     displayed_filename = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m(…)\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdisplayed_filename[-\u001b[32m40\u001b[39m:]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    446\u001b[39m consistency_error_message = (\n\u001b[32m    447\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mConsistency check failed: file should be of size \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexpected_size\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m but has size\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    448\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m \u001b[39m\u001b[38;5;130;01m{{\u001b[39;00m\u001b[33mactual_size\u001b[39m\u001b[38;5;130;01m}}\u001b[39;00m\u001b[33m (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdisplayed_filename\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m).\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mThis is usually due to network issues while downloading the file.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    449\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33m Please retry with `force_download=True`.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    450\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m451\u001b[39m progress_cm = \u001b[43m_get_progress_bar_context\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    452\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdesc\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdisplayed_filename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    453\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlog_level\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlogger\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgetEffectiveLevel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    454\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtotal\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtotal\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    455\u001b[39m \u001b[43m    \u001b[49m\u001b[43minitial\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    456\u001b[39m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mhuggingface_hub.http_get\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    457\u001b[39m \u001b[43m    \u001b[49m\u001b[43m_tqdm_bar\u001b[49m\u001b[43m=\u001b[49m\u001b[43m_tqdm_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    458\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    460\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m progress_cm \u001b[38;5;28;01mas\u001b[39;00m progress:\n\u001b[32m    461\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m hf_transfer \u001b[38;5;129;01mand\u001b[39;00m total \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m total > \u001b[32m5\u001b[39m * constants.DOWNLOAD_CHUNK_SIZE:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/sample-aiops-on-amazon-sagemakerai/.venv/lib/python3.12/site-packages/huggingface_hub/utils/tqdm.py:299\u001b[39m, in \u001b[36m_get_progress_bar_context\u001b[39m\u001b[34m(desc, log_level, total, initial, unit, unit_scale, name, _tqdm_bar)\u001b[39m\n\u001b[32m    294\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m nullcontext(_tqdm_bar)\n\u001b[32m    295\u001b[39m     \u001b[38;5;66;03m# ^ `contextlib.nullcontext` mimics a context manager that does nothing\u001b[39;00m\n\u001b[32m    296\u001b[39m     \u001b[38;5;66;03m#   Makes it easier to use the same code path for both cases but in the later\u001b[39;00m\n\u001b[32m    297\u001b[39m     \u001b[38;5;66;03m#   case, the progress bar is not closed when exiting the context manager.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m299\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtqdm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    300\u001b[39m \u001b[43m    \u001b[49m\u001b[43munit\u001b[49m\u001b[43m=\u001b[49m\u001b[43munit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    301\u001b[39m \u001b[43m    \u001b[49m\u001b[43munit_scale\u001b[49m\u001b[43m=\u001b[49m\u001b[43munit_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    302\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtotal\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtotal\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    303\u001b[39m \u001b[43m    \u001b[49m\u001b[43minitial\u001b[49m\u001b[43m=\u001b[49m\u001b[43minitial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    304\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdesc\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdesc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    305\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdisable\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_tqdm_disabled\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlog_level\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlog_level\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    306\u001b[39m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\u001b[43m=\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    307\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/sample-aiops-on-amazon-sagemakerai/.venv/lib/python3.12/site-packages/tqdm/std.py:665\u001b[39m, in \u001b[36mtqdm.__new__\u001b[39m\u001b[34m(cls, *_, **__)\u001b[39m\n\u001b[32m    663\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__new__\u001b[39m(\u001b[38;5;28mcls\u001b[39m, *_, **__):\n\u001b[32m    664\u001b[39m     instance = \u001b[38;5;28mobject\u001b[39m.\u001b[34m__new__\u001b[39m(\u001b[38;5;28mcls\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m665\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mwith\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_lock\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# also constructs lock if non-existent\u001b[39;49;00m\n\u001b[32m    666\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_instances\u001b[49m\u001b[43m.\u001b[49m\u001b[43madd\u001b[49m\u001b[43m(\u001b[49m\u001b[43minstance\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    667\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# create monitoring thread\u001b[39;49;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/sample-aiops-on-amazon-sagemakerai/.venv/lib/python3.12/site-packages/tqdm/std.py:111\u001b[39m, in \u001b[36mTqdmDefaultWriteLock.__enter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    110\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__enter__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m111\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/sample-aiops-on-amazon-sagemakerai/.venv/lib/python3.12/site-packages/tqdm/std.py:104\u001b[39m, in \u001b[36mTqdmDefaultWriteLock.acquire\u001b[39m\u001b[34m(self, *a, **k)\u001b[39m\n\u001b[32m    102\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34macquire\u001b[39m(\u001b[38;5;28mself\u001b[39m, *a, **k):\n\u001b[32m    103\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m lock \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.locks:\n\u001b[32m--> \u001b[39m\u001b[32m104\u001b[39m         \u001b[43mlock\u001b[49m\u001b[43m.\u001b[49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mk\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# base model\n",
    "\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model_id,\n",
    "    dtype='auto',\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "pipe.generation_config.pad_token_id = pipe.tokenizer.eos_token_id\n",
    "\n",
    "\n",
    "def predict(questions):\n",
    "    responses = []\n",
    "    for question in questions['inputs']:\n",
    "        messages = [\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"ou are a financial reasoning assistant. Read the user’s query, restate the key data, and solve step by step. Show calculations clearly, explain any rounding or adjustments, and present the final answer in a concise and professional manner.\",\n",
    "            },\n",
    "            {\"role\": \"user\", \"content\": question},\n",
    "        ]\n",
    "        output = pipe(\n",
    "            messages,\n",
    "            max_new_tokens=256,\n",
    "        )\n",
    "        responses.append(output[0][\"generated_text\"][-1]['content'])\n",
    "    return responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8fe1da3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset\n",
    "\n",
    "dataset = load_dataset(os.environ.get(\"HF_DATASET\"), split='train[:10]')\n",
    "eval_df = pd.DataFrame(dataset)\n",
    "eval_df = eval_df.rename(columns={\"user\": \"inputs\"})\n",
    "eval_df = eval_df.rename(columns={\"assistant\": \"ground_truth\"})\n",
    "eval_df = eval_df[[\"inputs\", \"ground_truth\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4755af86",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0819132",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fine-tuned adapter\n",
    "import os\n",
    "print(os.getcwd())\n",
    "print(os.listdir('./'))\n",
    "# source = adapter_path\n",
    "target = 'adapter'\n",
    "\n",
    "os.makedirs(target, exist_ok=True)\n",
    "cmd = f'aws s3 sync {adapter_path} {target}'\n",
    "_ = os.system(cmd)\n",
    "\n",
    "\n",
    "base_model_name = \"Qwen/Qwen3-0.6B\" # e.g., \"meta-llama/Llama-2-7b-hf\"\n",
    "adapter_path = \"./adapter/\" # The path where you saved the adapter\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(base_model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_name)\n",
    "tuned_model = PeftModel.from_pretrained(base_model, adapter_path)\n",
    "\n",
    "tuned_pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=tuned_model,\n",
    "    dtype='auto',\n",
    "    device_map=\"auto\",\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "tuned_pipe.generation_config.pad_token_id = tuned_pipe.tokenizer.eos_token_id\n",
    "\n",
    "\n",
    "def tuned_predict(questions):\n",
    "    responses = []\n",
    "    for question in questions['inputs']:\n",
    "        messages = [\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"ou are a financial reasoning assistant. Read the user’s query, restate the key data, and solve step by step. Show calculations clearly, explain any rounding or adjustments, and present the final answer in a concise and professional manner.\",\n",
    "            },\n",
    "            {\"role\": \"user\", \"content\": question},\n",
    "        ]\n",
    "        output = tuned_pipe(\n",
    "            messages,\n",
    "            max_new_tokens=256,\n",
    "        )\n",
    "        responses.append(output[0][\"generated_text\"][-1]['content'])\n",
    "    return responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6218b22d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an example to describe what answer_similarity means like for this problem.\n",
    "example = EvaluationExample(\n",
    "    input=\"What is MLflow?\",\n",
    "    output=\"MLflow is an open-source platform for managing machine \"\n",
    "    \"learning workflows, including experiment tracking, model packaging, \"\n",
    "    \"versioning, and deployment, simplifying the ML lifecycle.\",\n",
    "    score=4,\n",
    "    justification=\"The definition effectively explains what MLflow is \"\n",
    "    \"its purpose, and its developer. It could be more concise for a 5-score.\",\n",
    "    grading_context={\n",
    "        \"targets\": \"MLflow is an open-source platform for managing \"\n",
    "        \"the end-to-end machine learning (ML) lifecycle. It was developed by Databricks, \"\n",
    "        \"a company that specializes in big data and machine learning solutions. MLflow is \"\n",
    "        \"designed to address the challenges that data scientists and machine learning \"\n",
    "        \"engineers face when developing, training, and deploying machine learning models.\"\n",
    "    },\n",
    ")\n",
    "\n",
    "\n",
    "judge = \"bedrock:/us.anthropic.claude-3-5-haiku-20241022-v1:0\"\n",
    "judge_parameters = {\n",
    "    \"temperature\": 0,\n",
    "    \"max_tokens\": 256,\n",
    "    \"anthropic_version\": \"bedrock-2023-05-31\",\n",
    "}\n",
    "\n",
    "\n",
    "answer_similarity_metric = answer_similarity(\n",
    "    model=judge, parameters=judge_parameters, examples=[example]\n",
    ")\n",
    "\n",
    "answer_correctness_metric = answer_correctness(\n",
    "    model=judge,\n",
    "    parameters=judge_parameters,\n",
    ")\n",
    "\n",
    "answer_relevance_metric = answer_relevance(\n",
    "    model=judge,\n",
    "    parameters=judge_parameters,\n",
    ")\n",
    "\n",
    "relevance_metric = relevance(\n",
    "    model=judge,\n",
    "    parameters=judge_parameters,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6795e7ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "faithfulness_examples = [\n",
    "    EvaluationExample(\n",
    "        input=\"How do I disable MLflow autologging?\",\n",
    "        output=\"mlflow.autolog(disable=True) will disable autologging for all functions. In Databricks, autologging is enabled by default. \",\n",
    "        score=2,\n",
    "        justification=\"The output provides a working solution, using the mlflow.autolog() function that is provided in the context.\",\n",
    "        grading_context={\n",
    "            \"context\": \"mlflow.autolog(log_input_examples: bool = False, log_model_signatures: bool = True, log_models: bool = True, log_datasets: bool = True, disable: bool = False, exclusive: bool = False, disable_for_unsupported_versions: bool = False, silent: bool = False, extra_tags: Optional[Dict[str, str]] = None) → None[source] Enables (or disables) and configures autologging for all supported integrations. The parameters are passed to any autologging integrations that support them. See the tracking docs for a list of supported autologging integrations. Note that framework-specific configurations set at any point will take precedence over any configurations set by this function.\"\n",
    "        },\n",
    "    ),\n",
    "    EvaluationExample(\n",
    "        input=\"How do I disable MLflow autologging?\",\n",
    "        output=\"mlflow.autolog(disable=True) will disable autologging for all functions.\",\n",
    "        score=5,\n",
    "        justification=\"The output provides a solution that is using the mlflow.autolog() function that is provided in the context.\",\n",
    "        grading_context={\n",
    "            \"context\": \"mlflow.autolog(log_input_examples: bool = False, log_model_signatures: bool = True, log_models: bool = True, log_datasets: bool = True, disable: bool = False, exclusive: bool = False, disable_for_unsupported_versions: bool = False, silent: bool = False, extra_tags: Optional[Dict[str, str]] = None) → None[source] Enables (or disables) and configures autologging for all supported integrations. The parameters are passed to any autologging integrations that support them. See the tracking docs for a list of supported autologging integrations. Note that framework-specific configurations set at any point will take precedence over any configurations set by this function.\"\n",
    "        },\n",
    "    ),\n",
    "]\n",
    "\n",
    "faithfulness_metric = faithfulness(\n",
    "    model=judge, parameters=judge_parameters, examples=faithfulness_examples\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f29fb51c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Create a metric for professionalism\n",
    "professionalism_metric = make_genai_metric(\n",
    "    name=\"professionalism\",\n",
    "    definition=(\n",
    "        \"Professionalism refers to the use of a formal, respectful, and appropriate style of communication that is tailored to the context and audience. It often involves avoiding overly casual language, slang, or colloquialisms, and instead using clear, concise, and respectful language\"\n",
    "    ),\n",
    "    grading_prompt=(\n",
    "        \"Professionalism: If the answer is written using a professional tone, below \"\n",
    "        \"are the details for different scores: \"\n",
    "        \"- Score 1: Language is extremely casual, informal, and may include slang or colloquialisms. Not suitable for professional contexts.\"\n",
    "        \"- Score 2: Language is casual but generally respectful and avoids strong informality or slang. Acceptable in some informal professional settings.\"\n",
    "        \"- Score 3: Language is balanced and avoids extreme informality or formality. Suitable for most professional contexts. \"\n",
    "        \"- Score 4: Language is noticeably formal, respectful, and avoids casual elements. Appropriate for business or academic settings. \"\n",
    "        \"- Score 5: Language is excessively formal, respectful, and avoids casual elements. Appropriate for the most formal settings such as textbooks. \"\n",
    "    ),\n",
    "    examples=[\n",
    "        EvaluationExample(\n",
    "            input=\"What is MLflow?\",\n",
    "            output=(\n",
    "                \"MLflow is like your friendly neighborhood toolkit for managing your machine learning projects. It helps you track experiments, package your code and models, and collaborate with your team, making the whole ML workflow smoother. It's like your Swiss Army knife for machine learning!\"\n",
    "            ),\n",
    "            score=2,\n",
    "            justification=(\n",
    "                \"The response is written in a casual tone. It uses contractions, filler words such as 'like', and exclamation points, which make it sound less professional. \"\n",
    "            ),\n",
    "        )\n",
    "    ],\n",
    "    version=\"v1\",\n",
    "    model=judge,\n",
    "    parameters=judge_parameters,\n",
    "    grading_context_columns=[],\n",
    "    aggregations=[\"mean\", \"variance\", \"p90\"],\n",
    "    greater_is_better=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de770d5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Create a metric for helpfulness\n",
    "helpfulness_metric = make_genai_metric(\n",
    "    name=\"helpfulness\",\n",
    "    definition=(\n",
    "        \"Helpfulness evaluates how well the generated response directly addresses the user's query. \"\n",
    "        \"A helpful response is accurate, relevant, clear, and provides all necessary information to solve the user's problem or answer their question.\"\n",
    "    ),\n",
    "    grading_prompt=(\n",
    "        \"Helpfulness: If the answer is direct, accurate, and comprehensive in response to the input, score it based on the following scale:\"\n",
    "        \"- Score 1: The response is unhelpful. It is incorrect, irrelevant, or fails to address the user's query.\"\n",
    "        \"- Score 2: The response is somewhat helpful but lacks key information or is partially incorrect.\"\n",
    "        \"- Score 3: The response is generally helpful and correct, but it is not comprehensive or could be better organized.\"\n",
    "        \"- Score 4: The response is very helpful. It is accurate, relevant, and provides a clear and complete answer to the query.\"\n",
    "        \"- Score 5: The response is exceptionally helpful. It is accurate, comprehensive, and provides extra relevant details or context that exceed the user's expectations.\"\n",
    "    ),\n",
    "    examples=[\n",
    "        EvaluationExample(\n",
    "            input=\"How do I install MLflow?\",\n",
    "            output=(\n",
    "                \"You can easily install the MLflow library by running the following command in your terminal or command prompt: `pip install mlflow`.\"\n",
    "            ),\n",
    "            score=4,\n",
    "            justification=(\n",
    "                \"The response is a direct and accurate answer to the user's question. It provides the exact command needed, making it highly useful.\"\n",
    "            ),\n",
    "        ),\n",
    "        EvaluationExample(\n",
    "            input=\"Explain what a neural network is.\",\n",
    "            output=(\n",
    "                \"A neural network is a computational model inspired by the human brain. It's composed of interconnected nodes (neurons) organized in layers that process and pass information to make predictions or find patterns in data. Think of it like a series of interconnected filters, each one learning to recognize a different feature of the data.\"\n",
    "            ),\n",
    "            score=5,\n",
    "            justification=(\n",
    "                \"The response is accurate, comprehensive, and uses an analogy ('interconnected filters') to make the complex topic easier to understand, which adds significant value.\"\n",
    "            ),\n",
    "        ),\n",
    "    ],\n",
    "    version=\"v1\",\n",
    "    model=judge,\n",
    "    parameters=judge_parameters,\n",
    "    grading_context_columns=[],\n",
    "    aggregations=[\"mean\", \"variance\", \"p90\"],\n",
    "    greater_is_better=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c24e1635",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# Create a metric for politeness\n",
    "politeness_metric = make_genai_metric(\n",
    "    name=\"politeness\",\n",
    "    definition=(\n",
    "        \"Politeness measures the degree to which a response is courteous, respectful, and friendly. \"\n",
    "        \"A polite response uses appropriate salutations, avoids dismissive language, and maintains a positive tone.\"\n",
    "    ),\n",
    "    grading_prompt=(\n",
    "        \"Politeness: Score the following response based on how polite, respectful, and friendly it is. \"\n",
    "        \"Use the following scale:\"\n",
    "        \"- Score 1: The response is rude, dismissive, or contains disrespectful language.\"\n",
    "        \"- Score 2: The response is neutral or terse, showing a lack of courtesy.\"\n",
    "        \"- Score 3: The response is generally polite but lacks warmth or friendliness.\"\n",
    "        \"- Score 4: The response is polite and friendly, using respectful language and a positive tone.\"\n",
    "        \"- Score 5: The response is exceptionally polite and warm, using thoughtful and courteous language throughout.\"\n",
    "    ),\n",
    "    examples=[\n",
    "        EvaluationExample(\n",
    "            input=\"Can you tell me how to find the nearest coffee shop?\",\n",
    "            output=(\n",
    "                \"Please provide your current location, and I will be happy to help you find the nearest coffee shop.\"\n",
    "            ),\n",
    "            score=4,\n",
    "            justification=(\n",
    "                \"The response is polite and helpful. It uses 'Please' and 'I will be happy to help,' which contributes to a positive tone.\"\n",
    "            ),\n",
    "        ),\n",
    "        EvaluationExample(\n",
    "            input=\"I have a question about my order.\",\n",
    "            output=(\"What's the order number? Just tell me.\"),\n",
    "            score=2,\n",
    "            justification=(\n",
    "                \"The response is direct but terse and lacks politeness. It comes across as neutral and slightly demanding.\"\n",
    "            ),\n",
    "        ),\n",
    "    ],\n",
    "    version=\"v1\",\n",
    "    model=judge,\n",
    "    parameters=judge_parameters,\n",
    "    grading_context_columns=[],\n",
    "    aggregations=[\"mean\", \"variance\", \"p90\"],\n",
    "    greater_is_better=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec37b715",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_run_id_from_name(experiment_name: str, run_name: str) -> str:\n",
    "    # Look up the experiment ID\n",
    "    experiment = mlflow.get_experiment_by_name(experiment_name)\n",
    "    if experiment is None:\n",
    "        raise ValueError(f\"Experiment '{experiment_name}' not found\")\n",
    "\n",
    "    # Search runs in that experiment by run_name (stored as tag)\n",
    "    runs = mlflow.search_runs(\n",
    "        experiment_ids=[experiment.experiment_id],\n",
    "        filter_string=f\"tags.mlflow.runName = '{run_name}'\"\n",
    "    )\n",
    "\n",
    "    if runs.empty:\n",
    "        raise ValueError(f\"No run found with name '{run_name}' in experiment '{experiment_name}'\")\n",
    "\n",
    "    # Take the first match (assuming run_name is unique per experiment)\n",
    "    return runs.iloc[0].run_id\n",
    "\n",
    "parent_run_id = get_run_id_from_name(experiment_name, run_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a788ce15",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with mlflow.start_run(run_id=parent_run_id):\n",
    "    with mlflow.start_run(run_name='base-model-eval', nested=True):\n",
    "        results = mlflow.evaluate(\n",
    "            predict,\n",
    "            eval_df.head(int(os.environ.get(\"NUM_SAMPLES\", 5))),\n",
    "            evaluators=\"default\",\n",
    "            targets=\"ground_truth\",\n",
    "            extra_metrics=[\n",
    "                bleu(),\n",
    "                rouge1(),\n",
    "                rouge2(),\n",
    "                rougeL(),\n",
    "                rougeLsum(),\n",
    "                latency(),\n",
    "                token_count(),\n",
    "                answer_similarity_metric,\n",
    "                answer_correctness_metric,\n",
    "                answer_relevance_metric,\n",
    "                # relevance_metric, # requires context column\n",
    "                # faithfulness_metric, # requires context column\n",
    "                professionalism_metric,\n",
    "                helpfulness_metric,\n",
    "                politeness_metric,\n",
    "            ],\n",
    "        )\n",
    "\n",
    "        try:\n",
    "            mlflow.log_metrics(results.metrics)\n",
    "        except:\n",
    "            print('could not log metrics to mlflow')\n",
    "\n",
    "        print(results.metrics)\n",
    "        results.tables['eval_results_table'].to_csv(\n",
    "            '/opt/ml/output/data/eval_results_table.csv'\n",
    "        )\n",
    "        # results.tables['eval_results_table'].to_csv('results.csv')\n",
    "        print(results.tables['eval_results_table'])\n",
    "\n",
    "\n",
    "    with mlflow.start_run(run_name='tuned-model-eval', nested=True):\n",
    "        tuned_results = mlflow.evaluate(\n",
    "            tuned_predict,\n",
    "            eval_df.head(int(os.environ.get(\"NUM_SAMPLES\", 5))),\n",
    "            evaluators=\"default\",\n",
    "            targets=\"ground_truth\",\n",
    "            extra_metrics=[\n",
    "                bleu(),\n",
    "                rouge1(),\n",
    "                rouge2(),\n",
    "                rougeL(),\n",
    "                rougeLsum(),\n",
    "                latency(),\n",
    "                token_count(),\n",
    "                answer_similarity_metric,\n",
    "                answer_correctness_metric,\n",
    "                answer_relevance_metric,\n",
    "                # relevance_metric, # requires context column\n",
    "                # faithfulness_metric, # requires context column\n",
    "                professionalism_metric,\n",
    "                helpfulness_metric,\n",
    "                politeness_metric,\n",
    "            ],\n",
    "        )\n",
    "\n",
    "        try:\n",
    "            mlflow.log_metrics(tuned_results.metrics)\n",
    "        except:\n",
    "            print('could not log metrics to mlflow')\n",
    "\n",
    "        print(tuned_results.metrics)\n",
    "        tuned_results.tables['eval_results_table'].to_csv(\n",
    "            '/opt/ml/output/data/eval_results_table.csv'\n",
    "        )\n",
    "        # results.tables['eval_results_table'].to_csv('results.csv')\n",
    "        print(tuned_results.tables['eval_results_table'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4693fa9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c8b0b1f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c4fbe9f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d7916e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "968c6b51",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Appendix: ModelTrainer API for a SageMaker Eval Job"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89697bf1",
   "metadata": {},
   "source": [
    "Installed required libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2b1d2c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade -r requirements-fine-tuning.txt -q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f1818af",
   "metadata": {},
   "source": [
    "Import the necessary libraries and modules\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07c08c66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from dotenv import load_dotenv\n",
    "# load_dotenv()\n",
    "import os\n",
    "import boto3\n",
    "from scripts.utils import get_mlflow_server_arn\n",
    "from sagemaker.modules.train import ModelTrainer\n",
    "from sagemaker.modules.configs import Compute, SourceCode, StoppingCondition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91508105",
   "metadata": {},
   "outputs": [],
   "source": [
    "tracking_server_name = \"vlm-finetuning-server\"\n",
    "tracking_server_arn = get_mlflow_server_arn(tracking_server_name)\n",
    "tracking_server_arn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "440d0fdf",
   "metadata": {},
   "source": [
    "SageMaker AI provides containers for popular ML frameworks, including PyTorch, TensorFlow, and Hugging Face. These containers are optimized for performance and security, allowing you to focus on building and deploying your models without worrying about the underlying infrastructure.\n",
    "\n",
    "We are using the pytorch 2.8.0 container with Python 3.12. Then we are setting up the ModelTrainer API from the SageMaker SDK to run our training job. We are specifying the instance type, instance count, role, and source code location. We are also setting the stopping condition to 1 hour.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c119baf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "pytorch_image = '763104351884.dkr.ecr.us-east-1.amazonaws.com/pytorch-training:2.8.0-gpu-py312-cu129-ubuntu22.04-sagemaker'\n",
    "# define the script to be run\n",
    "source_code = SourceCode(\n",
    "    source_dir=\"scripts\",\n",
    "    entry_script=\"eval.sh\",\n",
    ")\n",
    "\n",
    "from huggingface_hub import HfFolder\n",
    "\n",
    "model_id = 'Qwen/Qwen3-0.6B'\n",
    "dataset_name = 'Josephgflowers/Finance-Instruct-500k'\n",
    "\n",
    "environment = {\n",
    "    'HF_TOKEN': HfFolder.get_token(),\n",
    "    \"HF_DATASET\": dataset_name,\n",
    "    \"MODEL_ID\": model_id,\n",
    "    \"MLFLOW_TRACKING_URI\": os.getenv(\n",
    "        'MLFLOW_TRACKING_URI',\n",
    "        'arn:aws:sagemaker:us-east-1:198346569064:mlflow-tracking-server/vlm-finetuning-server',\n",
    "    ),\n",
    "    \"MLFLOW_EXPERIMENT_NAME\": \"qwen3-06b-lora-ft-finance\",\n",
    "    \"NUM_SAMPLES\": '10',\n",
    "}\n",
    "\n",
    "# experiment_name = 'qwen3-06b-lora-ft-finance'\n",
    "# run_name = 'qwen3-06b-finance'\n",
    "\n",
    "hyperparameters = {\n",
    "    \"model_id\": model_id,\n",
    "    \"adapter_path\": 's3://sagemaker-us-east-1-198346569064/qwen3-06b-fine-tuned/',\n",
    "    'dataset_name': dataset_name,\n",
    "    'experiment_name': 'qwen3-06b-lora-ft-finance',\n",
    "    'run_name': 'fine-tuning-run-1',\n",
    "}\n",
    "\n",
    "assert (\n",
    "    environment[\"MLFLOW_TRACKING_URI\"] != \"XXX\"\n",
    "), \"Please set your MLFLOW_TRACKING_URI in the environment variable\"\n",
    "\n",
    "assert (\n",
    "    environment[\"HF_TOKEN\"] is not None\n",
    "), \"Please set your HF_TOKEN in the environment variable\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30a57da0",
   "metadata": {},
   "source": [
    "We are using a G5 2xlarge instance for our training job. This instance type is optimized for machine learning workloads and provides a good balance of compute, memory, and networking resources. The g5 instances are powered by NVIDIA A10G Tensor Core GPUs, which are well-suited for training large language models.\n",
    "\n",
    "The G5 has a single NVIDIA A10G GPU, 8 vCPUs, and 32 GB of memory. This instance type is a good choice for training large language models, as it provides enough compute power to handle the training workload while still being cost-effective.\n",
    "\n",
    "If you are running multiple experiments you can speed up the container download time by using warm pools. Warm pools are configured with the `keep_alive_period_in_seconds` parameter. This parameter specifies the amount of time that the warm pool will keep the container alive after the training job has completed. This can help to reduce the time it takes to start a new training job, as the container will already be downloaded and ready to use.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88c24185",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopping_condition = StoppingCondition(\n",
    "    max_runtime_in_seconds=60 * 60 * 10,  # seconds * minutes * hours\n",
    ")\n",
    "\n",
    "compute = Compute(\n",
    "    instance_count=1,\n",
    "    instance_type=\"ml.g5.2xlarge\",\n",
    "    # volume_size_in_gb=96,\n",
    "    keep_alive_period_in_seconds=3600,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a56e2da5",
   "metadata": {},
   "source": [
    "We tie together all the previous objects and configurations to create a ModelTrainer instance. This instance is responsible for orchestrating the training job on SageMaker. We pass in the role, source code configuration, compute configuration, and stopping condition to the ModelTrainer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3d7789c",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_job_name = \"mlflow-eval-llmaaj\"\n",
    "\n",
    "# define the ModelTrainer\n",
    "model_trainer = ModelTrainer(\n",
    "    training_image=pytorch_image,\n",
    "    source_code=source_code,\n",
    "    stopping_condition=stopping_condition,\n",
    "    base_job_name=base_job_name,\n",
    "    compute=compute,\n",
    "    environment=environment,\n",
    "    hyperparameters=hyperparameters,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1460e434",
   "metadata": {},
   "source": [
    "`model_trainer.fit()` is the command that initiates the training job on SageMaker. This method will package the source code, upload it to S3, and start the training job using the specified compute resources and configurations. The training job will run the `train.sh` script, which essentially installs some python libraries and then executes the `sft.py` script to fine-tune the model and log the results to MLflow.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a81fb69a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_trainer.train(wait=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sample-aiops-on-amazon-sagemakerai (3.13.7)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
