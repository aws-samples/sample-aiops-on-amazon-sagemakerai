{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eafea00f",
   "metadata": {},
   "source": [
    "## Evaluating LLMs with MLflow üìè\n",
    "\n",
    "MLflow provides a robust framework for tracking and managing machine learning experiments. In this notebook, we will explore how to evaluate large language models (LLMs) using MLflow, focusing on key metrics such as bleu, rouge, and LLM-as-a-judge metrics.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76a28d2c",
   "metadata": {},
   "source": [
    "Installed required libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2b1d2c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade -r requirements-fine-tuning.txt -q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f1818af",
   "metadata": {},
   "source": [
    "Import the necessary libraries and modules\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07c08c66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from dotenv import load_dotenv\n",
    "# load_dotenv()\n",
    "import os\n",
    "import boto3\n",
    "from sagemaker.modules.train import ModelTrainer\n",
    "from sagemaker.modules.configs import Compute, SourceCode, StoppingCondition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "440d0fdf",
   "metadata": {},
   "source": [
    "SageMaker AI provides containers for popular ML frameworks, including PyTorch, TensorFlow, and Hugging Face. These containers are optimized for performance and security, allowing you to focus on building and deploying your models without worrying about the underlying infrastructure.\n",
    "\n",
    "We are using the pytorch 2.8.0 container with Python 3.12. Then we are setting up the ModelTrainer API from the SageMaker SDK to run our training job. We are specifying the instance type, instance count, role, and source code location. We are also setting the stopping condition to 1 hour.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c119baf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "pytorch_image = '763104351884.dkr.ecr.us-east-1.amazonaws.com/pytorch-training:2.8.0-gpu-py312-cu129-ubuntu22.04-sagemaker'\n",
    "# define the script to be run\n",
    "source_code = SourceCode(\n",
    "    source_dir=\"scripts\",\n",
    "    entry_script=\"eval.sh\",\n",
    ")\n",
    "\n",
    "from huggingface_hub import HfFolder\n",
    "\n",
    "model_id = 'Qwen/Qwen3-0.6B'\n",
    "dataset_name = 'Josephgflowers/Finance-Instruct-500k'\n",
    "\n",
    "environment = {\n",
    "    'HF_TOKEN': HfFolder.get_token(),\n",
    "    \"HF_DATASET\": dataset_name,\n",
    "    \"MODEL_ID\": model_id,\n",
    "    \"MLFLOW_TRACKING_URI\": os.getenv(\n",
    "        'MLFLOW_TRACKING_URI',\n",
    "        'arn:aws:sagemaker:us-east-1:198346569064:mlflow-tracking-server/vlm-finetuning-server',\n",
    "    ),\n",
    "    \"MLFLOW_EXPERIMENT_NAME\": \"qwen3-06b-lora-ft-finance\",\n",
    "    \"NUM_SAMPLES\": '10',\n",
    "}\n",
    "\n",
    "# experiment_name = 'qwen3-06b-lora-ft-finance'\n",
    "# run_name = 'qwen3-06b-finance'\n",
    "\n",
    "hyperparameters = {\n",
    "    \"model_id\": model_id,\n",
    "    \"adapter_path\": 's3://sagemaker-us-east-1-198346569064/qwen3-06b-fine-tuned/',\n",
    "    'dataset_name': dataset_name,\n",
    "    'experiment_name': 'qwen3-06b-lora-ft-finance',\n",
    "    'run_name': 'fine-tuning-run-1',\n",
    "}\n",
    "\n",
    "assert (\n",
    "    environment[\"MLFLOW_TRACKING_URI\"] != \"XXX\"\n",
    "), \"Please set your MLFLOW_TRACKING_URI in the environment variable\"\n",
    "\n",
    "assert (\n",
    "    environment[\"HF_TOKEN\"] is not None\n",
    "), \"Please set your HF_TOKEN in the environment variable\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30a57da0",
   "metadata": {},
   "source": [
    "We are using a G5 2xlarge instance for our training job. This instance type is optimized for machine learning workloads and provides a good balance of compute, memory, and networking resources. The g5 instances are powered by NVIDIA A10G Tensor Core GPUs, which are well-suited for training large language models.\n",
    "\n",
    "The G5 has a single NVIDIA A10G GPU, 8 vCPUs, and 32 GB of memory. This instance type is a good choice for training large language models, as it provides enough compute power to handle the training workload while still being cost-effective.\n",
    "\n",
    "If you are running multiple experiments you can speed up the container download time by using warm pools. Warm pools are configured with the `keep_alive_period_in_seconds` parameter. This parameter specifies the amount of time that the warm pool will keep the container alive after the training job has completed. This can help to reduce the time it takes to start a new training job, as the container will already be downloaded and ready to use.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88c24185",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopping_condition = StoppingCondition(\n",
    "    max_runtime_in_seconds=60 * 60 * 10,  # seconds * minutes * hours\n",
    ")\n",
    "\n",
    "compute = Compute(\n",
    "    instance_count=1,\n",
    "    instance_type=\"ml.g5.2xlarge\",\n",
    "    # volume_size_in_gb=96,\n",
    "    keep_alive_period_in_seconds=3600,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a56e2da5",
   "metadata": {},
   "source": [
    "We tie together all the previous objects and configurations to create a ModelTrainer instance. This instance is responsible for orchestrating the training job on SageMaker. We pass in the role, source code configuration, compute configuration, and stopping condition to the ModelTrainer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3d7789c",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_job_name = \"mlflow-eval-llmaaj\"\n",
    "\n",
    "# define the ModelTrainer\n",
    "model_trainer = ModelTrainer(\n",
    "    training_image=pytorch_image,\n",
    "    source_code=source_code,\n",
    "    stopping_condition=stopping_condition,\n",
    "    base_job_name=base_job_name,\n",
    "    compute=compute,\n",
    "    environment=environment,\n",
    "    hyperparameters=hyperparameters,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1460e434",
   "metadata": {},
   "source": [
    "`model_trainer.fit()` is the command that initiates the training job on SageMaker. This method will package the source code, upload it to S3, and start the training job using the specified compute resources and configurations. The training job will run the `train.sh` script, which essentially installs some python libraries and then executes the `sft.py` script to fine-tune the model and log the results to MLflow.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a81fb69a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_trainer.train(wait=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "uv (3.12.11)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
