{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eafea00f",
   "metadata": {},
   "source": [
    "## Evaluating LLMs with MLflow ðŸ“\n",
    "\n",
    "MLflow provides a robust framework for tracking and managing machine learning experiments. In this notebook, we will explore how to evaluate large language models (LLMs) using MLflow, focusing on key metrics such as bleu, rouge, and LLM-as-a-judge metrics.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a04f5328",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'uv (3.12.11) (Python 3.12.11)' requires the ipykernel package.\n",
      "\u001b[1;31mInstall 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/home/ubuntu/sample-aiops-on-amazon-sagemakerai/.venv/bin/python -m pip install ipykernel -U --force-reinstall'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "import boto3\n",
    "import torch\n",
    "import mlflow\n",
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "from transformers import pipeline\n",
    "from mlflow.metrics import (\n",
    "    bleu,\n",
    "    rouge1,\n",
    "    rouge2,\n",
    "    rougeL,\n",
    "    rougeLsum,\n",
    "    latency,\n",
    "    token_count,\n",
    ")\n",
    "from mlflow.metrics.genai import (\n",
    "    EvaluationExample,\n",
    "    answer_correctness,\n",
    "    answer_relevance,\n",
    "    answer_similarity,\n",
    "    faithfulness,\n",
    "    relevance,\n",
    "    make_genai_metric,\n",
    ")\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import PeftModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff4c29eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting fixed hyperparameters\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    hyperparameters = json.loads(os.environ[\"SM_HPS\"])\n",
    "    print(\"Hyperparameters:\", hyperparameters)\n",
    "    model_id = hyperparameters.get('model_id', None)\n",
    "    adapter_path = hyperparameters.get('adapter_path', None)\n",
    "    experiment_name = hyperparameters.get('experiment_name', None)\n",
    "    run_name = hyperparameters.get('run_name', None)\n",
    "\n",
    "except:\n",
    "    print(\"Setting fixed hyperparameters\")\n",
    "    model_id = 'Qwen/Qwen3-0.6B'\n",
    "    adapter_path = None\n",
    "    experiment_name = 'qwen3-06b-lora-ft-finance'\n",
    "    run_name = 'base-model-eval'\n",
    "\n",
    "session = boto3.Session(region_name='us-east-1')\n",
    "credentials = session.get_credentials()\n",
    "credentials = credentials.get_frozen_credentials()\n",
    "access_key, secret_key = credentials.access_key, credentials.secret_key\n",
    "\n",
    "if credentials.token:\n",
    "    token = credentials.token\n",
    "    os.environ[\"AWS_SESSION_TOKEN\"] = token\n",
    "\n",
    "os.environ[\"AWS_REGION\"] = \"us-east-1\"\n",
    "os.environ[\"AWS_ACCESS_KEY_ID\"] = access_key\n",
    "os.environ[\"AWS_SECRET_ACCESS_KEY\"] = secret_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dafd8701",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored variables and their in-db values:\n"
     ]
    }
   ],
   "source": [
    "%store -r \n",
    "\n",
    "%store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d35b6f13",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow_tracking_uri = 'arn:aws:sagemaker:us-east-1:364430515305:mlflow-tracking-server/100725'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f097ea34",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.set_tracking_uri(mlflow_tracking_uri)\n",
    "mlflow.set_experiment(experiment_name)\n",
    "mlflow.enable_system_metrics_logging()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58dc900b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Qwen/Qwen3-0.6B'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8facc307",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# base model\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m pipe = \u001b[43mpipeline\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtext-generation\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mauto\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mauto\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     10\u001b[39m pipe.generation_config.pad_token_id = pipe.tokenizer.eos_token_id\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpredict\u001b[39m(questions):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/sample-aiops-on-amazon-sagemakerai/.venv/lib/python3.12/site-packages/transformers/pipelines/__init__.py:1027\u001b[39m, in \u001b[36mpipeline\u001b[39m\u001b[34m(task, model, config, tokenizer, feature_extractor, image_processor, processor, framework, revision, use_fast, token, device, device_map, dtype, trust_remote_code, model_kwargs, pipeline_class, **kwargs)\u001b[39m\n\u001b[32m   1025\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(model, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m framework \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1026\u001b[39m     model_classes = {\u001b[33m\"\u001b[39m\u001b[33mtf\u001b[39m\u001b[33m\"\u001b[39m: targeted_task[\u001b[33m\"\u001b[39m\u001b[33mtf\u001b[39m\u001b[33m\"\u001b[39m], \u001b[33m\"\u001b[39m\u001b[33mpt\u001b[39m\u001b[33m\"\u001b[39m: targeted_task[\u001b[33m\"\u001b[39m\u001b[33mpt\u001b[39m\u001b[33m\"\u001b[39m]}\n\u001b[32m-> \u001b[39m\u001b[32m1027\u001b[39m     framework, model = \u001b[43minfer_framework_load_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1028\u001b[39m \u001b[43m        \u001b[49m\u001b[43madapter_path\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43madapter_path\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1029\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel_classes\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel_classes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1030\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1031\u001b[39m \u001b[43m        \u001b[49m\u001b[43mframework\u001b[49m\u001b[43m=\u001b[49m\u001b[43mframework\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1032\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1033\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1034\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1035\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1037\u001b[39m hub_kwargs[\u001b[33m\"\u001b[39m\u001b[33m_commit_hash\u001b[39m\u001b[33m\"\u001b[39m] = model.config._commit_hash\n\u001b[32m   1039\u001b[39m \u001b[38;5;66;03m# Check which preprocessing classes the pipeline uses\u001b[39;00m\n\u001b[32m   1040\u001b[39m \u001b[38;5;66;03m# None values indicate optional classes that the pipeline can run without, we don't raise errors if loading fails\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/sample-aiops-on-amazon-sagemakerai/.venv/lib/python3.12/site-packages/transformers/pipelines/base.py:293\u001b[39m, in \u001b[36minfer_framework_load_model\u001b[39m\u001b[34m(model, config, model_classes, task, framework, **model_kwargs)\u001b[39m\n\u001b[32m    287\u001b[39m     logger.warning(\n\u001b[32m    288\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mModel might be a PyTorch model (ending with `.bin`) but PyTorch is not available. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    289\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mTrying to load the model with Tensorflow.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    290\u001b[39m     )\n\u001b[32m    292\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m293\u001b[39m     model = \u001b[43mmodel_class\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    294\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(model, \u001b[33m\"\u001b[39m\u001b[33meval\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m    295\u001b[39m         model = model.eval()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/sample-aiops-on-amazon-sagemakerai/.venv/lib/python3.12/site-packages/transformers/models/auto/auto_factory.py:604\u001b[39m, in \u001b[36m_BaseAutoModelClass.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[39m\n\u001b[32m    602\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m model_class.config_class == config.sub_configs.get(\u001b[33m\"\u001b[39m\u001b[33mtext_config\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m    603\u001b[39m         config = config.get_text_config()\n\u001b[32m--> \u001b[39m\u001b[32m604\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_class\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    605\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    606\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    607\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    608\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig.\u001b[34m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    609\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m, \u001b[39m\u001b[33m'\u001b[39m.join(c.\u001b[34m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m._model_mapping)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    610\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/sample-aiops-on-amazon-sagemakerai/.venv/lib/python3.12/site-packages/transformers/modeling_utils.py:277\u001b[39m, in \u001b[36mrestore_default_dtype.<locals>._wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    275\u001b[39m old_dtype = torch.get_default_dtype()\n\u001b[32m    276\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m277\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    278\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    279\u001b[39m     torch.set_default_dtype(old_dtype)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/sample-aiops-on-amazon-sagemakerai/.venv/lib/python3.12/site-packages/transformers/modeling_utils.py:4900\u001b[39m, in \u001b[36mPreTrainedModel.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[39m\n\u001b[32m   4890\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   4891\u001b[39m     gguf_file\n\u001b[32m   4892\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m device_map \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   4893\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m ((\u001b[38;5;28misinstance\u001b[39m(device_map, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mdisk\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m device_map.values()) \u001b[38;5;129;01mor\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mdisk\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m device_map)\n\u001b[32m   4894\u001b[39m ):\n\u001b[32m   4895\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m   4896\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mOne or more modules is configured to be mapped to disk. Disk offload is not supported for models \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   4897\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mloaded from GGUF files.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   4898\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m4900\u001b[39m checkpoint_files, sharded_metadata = \u001b[43m_get_resolved_checkpoint_files\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   4901\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4902\u001b[39m \u001b[43m    \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m=\u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4903\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvariant\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvariant\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4904\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgguf_file\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgguf_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4905\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfrom_tf\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfrom_tf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4906\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfrom_flax\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfrom_flax\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4907\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_safetensors\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_safetensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4908\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4909\u001b[39m \u001b[43m    \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m=\u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4910\u001b[39m \u001b[43m    \u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m=\u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4911\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4912\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4913\u001b[39m \u001b[43m    \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[43m=\u001b[49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4914\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4915\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcommit_hash\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcommit_hash\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4916\u001b[39m \u001b[43m    \u001b[49m\u001b[43mis_remote_code\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_auto_class\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   4917\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtransformers_explicit_filename\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtransformers_explicit_filename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4918\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4920\u001b[39m is_sharded = sharded_metadata \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   4921\u001b[39m is_quantized = hf_quantizer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/sample-aiops-on-amazon-sagemakerai/.venv/lib/python3.12/site-packages/transformers/modeling_utils.py:1037\u001b[39m, in \u001b[36m_get_resolved_checkpoint_files\u001b[39m\u001b[34m(pretrained_model_name_or_path, subfolder, variant, gguf_file, from_tf, from_flax, use_safetensors, cache_dir, force_download, proxies, local_files_only, token, user_agent, revision, commit_hash, is_remote_code, transformers_explicit_filename)\u001b[39m\n\u001b[32m   1022\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1023\u001b[39m     \u001b[38;5;66;03m# Load from URL or cache if already cached\u001b[39;00m\n\u001b[32m   1024\u001b[39m     cached_file_kwargs = {\n\u001b[32m   1025\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mcache_dir\u001b[39m\u001b[33m\"\u001b[39m: cache_dir,\n\u001b[32m   1026\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mforce_download\u001b[39m\u001b[33m\"\u001b[39m: force_download,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1035\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m_commit_hash\u001b[39m\u001b[33m\"\u001b[39m: commit_hash,\n\u001b[32m   1036\u001b[39m     }\n\u001b[32m-> \u001b[39m\u001b[32m1037\u001b[39m     resolved_archive_file = \u001b[43mcached_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mcached_file_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1039\u001b[39m     \u001b[38;5;66;03m# Since we set _raise_exceptions_for_missing_entries=False, we don't get an exception but a None\u001b[39;00m\n\u001b[32m   1040\u001b[39m     \u001b[38;5;66;03m# result when internet is up, the repo and revision exist, but the file does not.\u001b[39;00m\n\u001b[32m   1041\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m resolved_archive_file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m filename == _add_variant(SAFE_WEIGHTS_NAME, variant):\n\u001b[32m   1042\u001b[39m         \u001b[38;5;66;03m# Maybe the checkpoint is sharded, we try to grab the index name in this case.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/sample-aiops-on-amazon-sagemakerai/.venv/lib/python3.12/site-packages/transformers/utils/hub.py:322\u001b[39m, in \u001b[36mcached_file\u001b[39m\u001b[34m(path_or_repo_id, filename, **kwargs)\u001b[39m\n\u001b[32m    264\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcached_file\u001b[39m(\n\u001b[32m    265\u001b[39m     path_or_repo_id: Union[\u001b[38;5;28mstr\u001b[39m, os.PathLike],\n\u001b[32m    266\u001b[39m     filename: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m    267\u001b[39m     **kwargs,\n\u001b[32m    268\u001b[39m ) -> Optional[\u001b[38;5;28mstr\u001b[39m]:\n\u001b[32m    269\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    270\u001b[39m \u001b[33;03m    Tries to locate a file in a local folder and repo, downloads and cache it if necessary.\u001b[39;00m\n\u001b[32m    271\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    320\u001b[39m \u001b[33;03m    ```\u001b[39;00m\n\u001b[32m    321\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m322\u001b[39m     file = \u001b[43mcached_files\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilenames\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    323\u001b[39m     file = file[\u001b[32m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m file\n\u001b[32m    324\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m file\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/sample-aiops-on-amazon-sagemakerai/.venv/lib/python3.12/site-packages/transformers/utils/hub.py:479\u001b[39m, in \u001b[36mcached_files\u001b[39m\u001b[34m(path_or_repo_id, filenames, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[39m\n\u001b[32m    476\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    477\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(full_filenames) == \u001b[32m1\u001b[39m:\n\u001b[32m    478\u001b[39m         \u001b[38;5;66;03m# This is slightly better for only 1 file\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m479\u001b[39m         \u001b[43mhf_hub_download\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    480\u001b[39m \u001b[43m            \u001b[49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    481\u001b[39m \u001b[43m            \u001b[49m\u001b[43mfilenames\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    482\u001b[39m \u001b[43m            \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    483\u001b[39m \u001b[43m            \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    484\u001b[39m \u001b[43m            \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    485\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    486\u001b[39m \u001b[43m            \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[43m=\u001b[49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    487\u001b[39m \u001b[43m            \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m=\u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    488\u001b[39m \u001b[43m            \u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m=\u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    489\u001b[39m \u001b[43m            \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    490\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    491\u001b[39m \u001b[43m            \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    492\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    493\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    494\u001b[39m         snapshot_download(\n\u001b[32m    495\u001b[39m             path_or_repo_id,\n\u001b[32m    496\u001b[39m             allow_patterns=full_filenames,\n\u001b[32m   (...)\u001b[39m\u001b[32m    505\u001b[39m             local_files_only=local_files_only,\n\u001b[32m    506\u001b[39m         )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/sample-aiops-on-amazon-sagemakerai/.venv/lib/python3.12/site-packages/huggingface_hub/utils/_validators.py:114\u001b[39m, in \u001b[36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    111\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m check_use_auth_token:\n\u001b[32m    112\u001b[39m     kwargs = smoothly_deprecate_use_auth_token(fn_name=fn.\u001b[34m__name__\u001b[39m, has_token=has_token, kwargs=kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m114\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/sample-aiops-on-amazon-sagemakerai/.venv/lib/python3.12/site-packages/huggingface_hub/file_download.py:1010\u001b[39m, in \u001b[36mhf_hub_download\u001b[39m\u001b[34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, user_agent, force_download, proxies, etag_timeout, token, local_files_only, headers, endpoint, resume_download, force_filename, local_dir_use_symlinks)\u001b[39m\n\u001b[32m    990\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _hf_hub_download_to_local_dir(\n\u001b[32m    991\u001b[39m         \u001b[38;5;66;03m# Destination\u001b[39;00m\n\u001b[32m    992\u001b[39m         local_dir=local_dir,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1007\u001b[39m         local_files_only=local_files_only,\n\u001b[32m   1008\u001b[39m     )\n\u001b[32m   1009\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1010\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_hf_hub_download_to_cache_dir\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1011\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Destination\u001b[39;49;00m\n\u001b[32m   1012\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1013\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# File info\u001b[39;49;00m\n\u001b[32m   1014\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1015\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1016\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1017\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1018\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# HTTP info\u001b[39;49;00m\n\u001b[32m   1019\u001b[39m \u001b[43m        \u001b[49m\u001b[43mendpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mendpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1020\u001b[39m \u001b[43m        \u001b[49m\u001b[43metag_timeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43metag_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1021\u001b[39m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhf_headers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1022\u001b[39m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m=\u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1023\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1024\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Additional options\u001b[39;49;00m\n\u001b[32m   1025\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1026\u001b[39m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m=\u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1027\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/sample-aiops-on-amazon-sagemakerai/.venv/lib/python3.12/site-packages/huggingface_hub/file_download.py:1171\u001b[39m, in \u001b[36m_hf_hub_download_to_cache_dir\u001b[39m\u001b[34m(cache_dir, repo_id, filename, repo_type, revision, endpoint, etag_timeout, headers, proxies, token, local_files_only, force_download)\u001b[39m\n\u001b[32m   1168\u001b[39m \u001b[38;5;66;03m# Local file doesn't exist or etag isn't a match => retrieve file from remote (or cache)\u001b[39;00m\n\u001b[32m   1170\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m WeakFileLock(lock_path):\n\u001b[32m-> \u001b[39m\u001b[32m1171\u001b[39m     \u001b[43m_download_to_tmp_and_move\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1172\u001b[39m \u001b[43m        \u001b[49m\u001b[43mincomplete_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43mPath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mblob_path\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m.incomplete\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1173\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdestination_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43mPath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mblob_path\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1174\u001b[39m \u001b[43m        \u001b[49m\u001b[43murl_to_download\u001b[49m\u001b[43m=\u001b[49m\u001b[43murl_to_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1175\u001b[39m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m=\u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1176\u001b[39m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1177\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexpected_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mexpected_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1178\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1179\u001b[39m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m=\u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1180\u001b[39m \u001b[43m        \u001b[49m\u001b[43metag\u001b[49m\u001b[43m=\u001b[49m\u001b[43metag\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1181\u001b[39m \u001b[43m        \u001b[49m\u001b[43mxet_file_data\u001b[49m\u001b[43m=\u001b[49m\u001b[43mxet_file_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1182\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1183\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os.path.exists(pointer_path):\n\u001b[32m   1184\u001b[39m         _create_symlink(blob_path, pointer_path, new_blob=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/sample-aiops-on-amazon-sagemakerai/.venv/lib/python3.12/site-packages/huggingface_hub/file_download.py:1723\u001b[39m, in \u001b[36m_download_to_tmp_and_move\u001b[39m\u001b[34m(incomplete_path, destination_path, url_to_download, proxies, headers, expected_size, filename, force_download, etag, xet_file_data)\u001b[39m\n\u001b[32m   1721\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m xet_file_data \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m is_xet_available():\n\u001b[32m   1722\u001b[39m     logger.debug(\u001b[33m\"\u001b[39m\u001b[33mXet Storage is enabled for this repo. Downloading file from Xet Storage..\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1723\u001b[39m     \u001b[43mxet_get\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1724\u001b[39m \u001b[43m        \u001b[49m\u001b[43mincomplete_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43mincomplete_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1725\u001b[39m \u001b[43m        \u001b[49m\u001b[43mxet_file_data\u001b[49m\u001b[43m=\u001b[49m\u001b[43mxet_file_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1726\u001b[39m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1727\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexpected_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mexpected_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1728\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdisplayed_filename\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1729\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1730\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1731\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m xet_file_data \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m constants.HF_HUB_DISABLE_XET:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/sample-aiops-on-amazon-sagemakerai/.venv/lib/python3.12/site-packages/huggingface_hub/file_download.py:615\u001b[39m, in \u001b[36mxet_get\u001b[39m\u001b[34m(incomplete_path, xet_file_data, headers, expected_size, displayed_filename, _tqdm_bar)\u001b[39m\n\u001b[32m    612\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(displayed_filename) > \u001b[32m40\u001b[39m:\n\u001b[32m    613\u001b[39m     displayed_filename = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdisplayed_filename[:\u001b[32m40\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m(â€¦)\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m615\u001b[39m progress_cm = \u001b[43m_get_progress_bar_context\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    616\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdesc\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdisplayed_filename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    617\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlog_level\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlogger\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgetEffectiveLevel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    618\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtotal\u001b[49m\u001b[43m=\u001b[49m\u001b[43mexpected_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    619\u001b[39m \u001b[43m    \u001b[49m\u001b[43minitial\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    620\u001b[39m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mhuggingface_hub.xet_get\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    621\u001b[39m \u001b[43m    \u001b[49m\u001b[43m_tqdm_bar\u001b[49m\u001b[43m=\u001b[49m\u001b[43m_tqdm_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    622\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    624\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m progress_cm \u001b[38;5;28;01mas\u001b[39;00m progress:\n\u001b[32m    626\u001b[39m     \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mprogress_updater\u001b[39m(progress_bytes: \u001b[38;5;28mfloat\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/sample-aiops-on-amazon-sagemakerai/.venv/lib/python3.12/site-packages/huggingface_hub/utils/tqdm.py:299\u001b[39m, in \u001b[36m_get_progress_bar_context\u001b[39m\u001b[34m(desc, log_level, total, initial, unit, unit_scale, name, _tqdm_bar)\u001b[39m\n\u001b[32m    294\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m nullcontext(_tqdm_bar)\n\u001b[32m    295\u001b[39m     \u001b[38;5;66;03m# ^ `contextlib.nullcontext` mimics a context manager that does nothing\u001b[39;00m\n\u001b[32m    296\u001b[39m     \u001b[38;5;66;03m#   Makes it easier to use the same code path for both cases but in the later\u001b[39;00m\n\u001b[32m    297\u001b[39m     \u001b[38;5;66;03m#   case, the progress bar is not closed when exiting the context manager.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m299\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtqdm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    300\u001b[39m \u001b[43m    \u001b[49m\u001b[43munit\u001b[49m\u001b[43m=\u001b[49m\u001b[43munit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    301\u001b[39m \u001b[43m    \u001b[49m\u001b[43munit_scale\u001b[49m\u001b[43m=\u001b[49m\u001b[43munit_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    302\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtotal\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtotal\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    303\u001b[39m \u001b[43m    \u001b[49m\u001b[43minitial\u001b[49m\u001b[43m=\u001b[49m\u001b[43minitial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    304\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdesc\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdesc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    305\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdisable\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_tqdm_disabled\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlog_level\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlog_level\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    306\u001b[39m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\u001b[43m=\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    307\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/sample-aiops-on-amazon-sagemakerai/.venv/lib/python3.12/site-packages/tqdm/std.py:665\u001b[39m, in \u001b[36mtqdm.__new__\u001b[39m\u001b[34m(cls, *_, **__)\u001b[39m\n\u001b[32m    663\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__new__\u001b[39m(\u001b[38;5;28mcls\u001b[39m, *_, **__):\n\u001b[32m    664\u001b[39m     instance = \u001b[38;5;28mobject\u001b[39m.\u001b[34m__new__\u001b[39m(\u001b[38;5;28mcls\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m665\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mwith\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_lock\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# also constructs lock if non-existent\u001b[39;49;00m\n\u001b[32m    666\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_instances\u001b[49m\u001b[43m.\u001b[49m\u001b[43madd\u001b[49m\u001b[43m(\u001b[49m\u001b[43minstance\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    667\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# create monitoring thread\u001b[39;49;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/sample-aiops-on-amazon-sagemakerai/.venv/lib/python3.12/site-packages/tqdm/std.py:111\u001b[39m, in \u001b[36mTqdmDefaultWriteLock.__enter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    110\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__enter__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m111\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/sample-aiops-on-amazon-sagemakerai/.venv/lib/python3.12/site-packages/tqdm/std.py:104\u001b[39m, in \u001b[36mTqdmDefaultWriteLock.acquire\u001b[39m\u001b[34m(self, *a, **k)\u001b[39m\n\u001b[32m    102\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34macquire\u001b[39m(\u001b[38;5;28mself\u001b[39m, *a, **k):\n\u001b[32m    103\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m lock \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.locks:\n\u001b[32m--> \u001b[39m\u001b[32m104\u001b[39m         \u001b[43mlock\u001b[49m\u001b[43m.\u001b[49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mk\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# base model\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model_id,\n",
    "    dtype='auto',\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "pipe.generation_config.pad_token_id = pipe.tokenizer.eos_token_id\n",
    "\n",
    "\n",
    "def predict(questions):\n",
    "    responses = []\n",
    "    for question in questions['inputs']:\n",
    "        messages = [\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"ou are a financial reasoning assistant. Read the userâ€™s query, restate the key data, and solve step by step. Show calculations clearly, explain any rounding or adjustments, and present the final answer in a concise and professional manner.\",\n",
    "            },\n",
    "            {\"role\": \"user\", \"content\": question},\n",
    "        ]\n",
    "        output = pipe(\n",
    "            messages,\n",
    "            max_new_tokens=256,\n",
    "        )\n",
    "        responses.append(output[0][\"generated_text\"][-1]['content'])\n",
    "    return responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8fe1da3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset\n",
    "\n",
    "dataset = load_dataset(os.environ.get(\"HF_DATASET\"), split='train[:10]')\n",
    "eval_df = pd.DataFrame(dataset)\n",
    "eval_df = eval_df.rename(columns={\"user\": \"inputs\"})\n",
    "eval_df = eval_df.rename(columns={\"assistant\": \"ground_truth\"})\n",
    "eval_df = eval_df[[\"inputs\", \"ground_truth\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4755af86",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0819132",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fine-tuned adapter\n",
    "import os\n",
    "print(os.getcwd())\n",
    "print(os.listdir('./'))\n",
    "# source = adapter_path\n",
    "target = 'adapter'\n",
    "\n",
    "os.makedirs(target, exist_ok=True)\n",
    "cmd = f'aws s3 sync {adapter_path} {target}'\n",
    "_ = os.system(cmd)\n",
    "\n",
    "\n",
    "base_model_name = \"Qwen/Qwen3-0.6B\" # e.g., \"meta-llama/Llama-2-7b-hf\"\n",
    "adapter_path = \"./adapter/\" # The path where you saved the adapter\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(base_model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_name)\n",
    "tuned_model = PeftModel.from_pretrained(base_model, adapter_path)\n",
    "\n",
    "tuned_pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=tuned_model,\n",
    "    dtype='auto',\n",
    "    device_map=\"auto\",\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "tuned_pipe.generation_config.pad_token_id = tuned_pipe.tokenizer.eos_token_id\n",
    "\n",
    "\n",
    "def tuned_predict(questions):\n",
    "    responses = []\n",
    "    for question in questions['inputs']:\n",
    "        messages = [\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"ou are a financial reasoning assistant. Read the userâ€™s query, restate the key data, and solve step by step. Show calculations clearly, explain any rounding or adjustments, and present the final answer in a concise and professional manner.\",\n",
    "            },\n",
    "            {\"role\": \"user\", \"content\": question},\n",
    "        ]\n",
    "        output = tuned_pipe(\n",
    "            messages,\n",
    "            max_new_tokens=256,\n",
    "        )\n",
    "        responses.append(output[0][\"generated_text\"][-1]['content'])\n",
    "    return responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6218b22d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an example to describe what answer_similarity means like for this problem.\n",
    "example = EvaluationExample(\n",
    "    input=\"What is MLflow?\",\n",
    "    output=\"MLflow is an open-source platform for managing machine \"\n",
    "    \"learning workflows, including experiment tracking, model packaging, \"\n",
    "    \"versioning, and deployment, simplifying the ML lifecycle.\",\n",
    "    score=4,\n",
    "    justification=\"The definition effectively explains what MLflow is \"\n",
    "    \"its purpose, and its developer. It could be more concise for a 5-score.\",\n",
    "    grading_context={\n",
    "        \"targets\": \"MLflow is an open-source platform for managing \"\n",
    "        \"the end-to-end machine learning (ML) lifecycle. It was developed by Databricks, \"\n",
    "        \"a company that specializes in big data and machine learning solutions. MLflow is \"\n",
    "        \"designed to address the challenges that data scientists and machine learning \"\n",
    "        \"engineers face when developing, training, and deploying machine learning models.\"\n",
    "    },\n",
    ")\n",
    "\n",
    "\n",
    "judge = \"bedrock:/us.anthropic.claude-3-5-haiku-20241022-v1:0\"\n",
    "judge_parameters = {\n",
    "    \"temperature\": 0,\n",
    "    \"max_tokens\": 256,\n",
    "    \"anthropic_version\": \"bedrock-2023-05-31\",\n",
    "}\n",
    "\n",
    "\n",
    "answer_similarity_metric = answer_similarity(\n",
    "    model=judge, parameters=judge_parameters, examples=[example]\n",
    ")\n",
    "\n",
    "answer_correctness_metric = answer_correctness(\n",
    "    model=judge,\n",
    "    parameters=judge_parameters,\n",
    ")\n",
    "\n",
    "answer_relevance_metric = answer_relevance(\n",
    "    model=judge,\n",
    "    parameters=judge_parameters,\n",
    ")\n",
    "\n",
    "relevance_metric = relevance(\n",
    "    model=judge,\n",
    "    parameters=judge_parameters,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6795e7ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "faithfulness_examples = [\n",
    "    EvaluationExample(\n",
    "        input=\"How do I disable MLflow autologging?\",\n",
    "        output=\"mlflow.autolog(disable=True) will disable autologging for all functions. In Databricks, autologging is enabled by default. \",\n",
    "        score=2,\n",
    "        justification=\"The output provides a working solution, using the mlflow.autolog() function that is provided in the context.\",\n",
    "        grading_context={\n",
    "            \"context\": \"mlflow.autolog(log_input_examples: bool = False, log_model_signatures: bool = True, log_models: bool = True, log_datasets: bool = True, disable: bool = False, exclusive: bool = False, disable_for_unsupported_versions: bool = False, silent: bool = False, extra_tags: Optional[Dict[str, str]] = None) â†’ None[source] Enables (or disables) and configures autologging for all supported integrations. The parameters are passed to any autologging integrations that support them. See the tracking docs for a list of supported autologging integrations. Note that framework-specific configurations set at any point will take precedence over any configurations set by this function.\"\n",
    "        },\n",
    "    ),\n",
    "    EvaluationExample(\n",
    "        input=\"How do I disable MLflow autologging?\",\n",
    "        output=\"mlflow.autolog(disable=True) will disable autologging for all functions.\",\n",
    "        score=5,\n",
    "        justification=\"The output provides a solution that is using the mlflow.autolog() function that is provided in the context.\",\n",
    "        grading_context={\n",
    "            \"context\": \"mlflow.autolog(log_input_examples: bool = False, log_model_signatures: bool = True, log_models: bool = True, log_datasets: bool = True, disable: bool = False, exclusive: bool = False, disable_for_unsupported_versions: bool = False, silent: bool = False, extra_tags: Optional[Dict[str, str]] = None) â†’ None[source] Enables (or disables) and configures autologging for all supported integrations. The parameters are passed to any autologging integrations that support them. See the tracking docs for a list of supported autologging integrations. Note that framework-specific configurations set at any point will take precedence over any configurations set by this function.\"\n",
    "        },\n",
    "    ),\n",
    "]\n",
    "\n",
    "faithfulness_metric = faithfulness(\n",
    "    model=judge, parameters=judge_parameters, examples=faithfulness_examples\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f29fb51c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Create a metric for professionalism\n",
    "professionalism_metric = make_genai_metric(\n",
    "    name=\"professionalism\",\n",
    "    definition=(\n",
    "        \"Professionalism refers to the use of a formal, respectful, and appropriate style of communication that is tailored to the context and audience. It often involves avoiding overly casual language, slang, or colloquialisms, and instead using clear, concise, and respectful language\"\n",
    "    ),\n",
    "    grading_prompt=(\n",
    "        \"Professionalism: If the answer is written using a professional tone, below \"\n",
    "        \"are the details for different scores: \"\n",
    "        \"- Score 1: Language is extremely casual, informal, and may include slang or colloquialisms. Not suitable for professional contexts.\"\n",
    "        \"- Score 2: Language is casual but generally respectful and avoids strong informality or slang. Acceptable in some informal professional settings.\"\n",
    "        \"- Score 3: Language is balanced and avoids extreme informality or formality. Suitable for most professional contexts. \"\n",
    "        \"- Score 4: Language is noticeably formal, respectful, and avoids casual elements. Appropriate for business or academic settings. \"\n",
    "        \"- Score 5: Language is excessively formal, respectful, and avoids casual elements. Appropriate for the most formal settings such as textbooks. \"\n",
    "    ),\n",
    "    examples=[\n",
    "        EvaluationExample(\n",
    "            input=\"What is MLflow?\",\n",
    "            output=(\n",
    "                \"MLflow is like your friendly neighborhood toolkit for managing your machine learning projects. It helps you track experiments, package your code and models, and collaborate with your team, making the whole ML workflow smoother. It's like your Swiss Army knife for machine learning!\"\n",
    "            ),\n",
    "            score=2,\n",
    "            justification=(\n",
    "                \"The response is written in a casual tone. It uses contractions, filler words such as 'like', and exclamation points, which make it sound less professional. \"\n",
    "            ),\n",
    "        )\n",
    "    ],\n",
    "    version=\"v1\",\n",
    "    model=judge,\n",
    "    parameters=judge_parameters,\n",
    "    grading_context_columns=[],\n",
    "    aggregations=[\"mean\", \"variance\", \"p90\"],\n",
    "    greater_is_better=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de770d5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Create a metric for helpfulness\n",
    "helpfulness_metric = make_genai_metric(\n",
    "    name=\"helpfulness\",\n",
    "    definition=(\n",
    "        \"Helpfulness evaluates how well the generated response directly addresses the user's query. \"\n",
    "        \"A helpful response is accurate, relevant, clear, and provides all necessary information to solve the user's problem or answer their question.\"\n",
    "    ),\n",
    "    grading_prompt=(\n",
    "        \"Helpfulness: If the answer is direct, accurate, and comprehensive in response to the input, score it based on the following scale:\"\n",
    "        \"- Score 1: The response is unhelpful. It is incorrect, irrelevant, or fails to address the user's query.\"\n",
    "        \"- Score 2: The response is somewhat helpful but lacks key information or is partially incorrect.\"\n",
    "        \"- Score 3: The response is generally helpful and correct, but it is not comprehensive or could be better organized.\"\n",
    "        \"- Score 4: The response is very helpful. It is accurate, relevant, and provides a clear and complete answer to the query.\"\n",
    "        \"- Score 5: The response is exceptionally helpful. It is accurate, comprehensive, and provides extra relevant details or context that exceed the user's expectations.\"\n",
    "    ),\n",
    "    examples=[\n",
    "        EvaluationExample(\n",
    "            input=\"How do I install MLflow?\",\n",
    "            output=(\n",
    "                \"You can easily install the MLflow library by running the following command in your terminal or command prompt: `pip install mlflow`.\"\n",
    "            ),\n",
    "            score=4,\n",
    "            justification=(\n",
    "                \"The response is a direct and accurate answer to the user's question. It provides the exact command needed, making it highly useful.\"\n",
    "            ),\n",
    "        ),\n",
    "        EvaluationExample(\n",
    "            input=\"Explain what a neural network is.\",\n",
    "            output=(\n",
    "                \"A neural network is a computational model inspired by the human brain. It's composed of interconnected nodes (neurons) organized in layers that process and pass information to make predictions or find patterns in data. Think of it like a series of interconnected filters, each one learning to recognize a different feature of the data.\"\n",
    "            ),\n",
    "            score=5,\n",
    "            justification=(\n",
    "                \"The response is accurate, comprehensive, and uses an analogy ('interconnected filters') to make the complex topic easier to understand, which adds significant value.\"\n",
    "            ),\n",
    "        ),\n",
    "    ],\n",
    "    version=\"v1\",\n",
    "    model=judge,\n",
    "    parameters=judge_parameters,\n",
    "    grading_context_columns=[],\n",
    "    aggregations=[\"mean\", \"variance\", \"p90\"],\n",
    "    greater_is_better=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c24e1635",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# Create a metric for politeness\n",
    "politeness_metric = make_genai_metric(\n",
    "    name=\"politeness\",\n",
    "    definition=(\n",
    "        \"Politeness measures the degree to which a response is courteous, respectful, and friendly. \"\n",
    "        \"A polite response uses appropriate salutations, avoids dismissive language, and maintains a positive tone.\"\n",
    "    ),\n",
    "    grading_prompt=(\n",
    "        \"Politeness: Score the following response based on how polite, respectful, and friendly it is. \"\n",
    "        \"Use the following scale:\"\n",
    "        \"- Score 1: The response is rude, dismissive, or contains disrespectful language.\"\n",
    "        \"- Score 2: The response is neutral or terse, showing a lack of courtesy.\"\n",
    "        \"- Score 3: The response is generally polite but lacks warmth or friendliness.\"\n",
    "        \"- Score 4: The response is polite and friendly, using respectful language and a positive tone.\"\n",
    "        \"- Score 5: The response is exceptionally polite and warm, using thoughtful and courteous language throughout.\"\n",
    "    ),\n",
    "    examples=[\n",
    "        EvaluationExample(\n",
    "            input=\"Can you tell me how to find the nearest coffee shop?\",\n",
    "            output=(\n",
    "                \"Please provide your current location, and I will be happy to help you find the nearest coffee shop.\"\n",
    "            ),\n",
    "            score=4,\n",
    "            justification=(\n",
    "                \"The response is polite and helpful. It uses 'Please' and 'I will be happy to help,' which contributes to a positive tone.\"\n",
    "            ),\n",
    "        ),\n",
    "        EvaluationExample(\n",
    "            input=\"I have a question about my order.\",\n",
    "            output=(\"What's the order number? Just tell me.\"),\n",
    "            score=2,\n",
    "            justification=(\n",
    "                \"The response is direct but terse and lacks politeness. It comes across as neutral and slightly demanding.\"\n",
    "            ),\n",
    "        ),\n",
    "    ],\n",
    "    version=\"v1\",\n",
    "    model=judge,\n",
    "    parameters=judge_parameters,\n",
    "    grading_context_columns=[],\n",
    "    aggregations=[\"mean\", \"variance\", \"p90\"],\n",
    "    greater_is_better=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec37b715",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_run_id_from_name(experiment_name: str, run_name: str) -> str:\n",
    "    # Look up the experiment ID\n",
    "    experiment = mlflow.get_experiment_by_name(experiment_name)\n",
    "    if experiment is None:\n",
    "        raise ValueError(f\"Experiment '{experiment_name}' not found\")\n",
    "\n",
    "    # Search runs in that experiment by run_name (stored as tag)\n",
    "    runs = mlflow.search_runs(\n",
    "        experiment_ids=[experiment.experiment_id],\n",
    "        filter_string=f\"tags.mlflow.runName = '{run_name}'\"\n",
    "    )\n",
    "\n",
    "    if runs.empty:\n",
    "        raise ValueError(f\"No run found with name '{run_name}' in experiment '{experiment_name}'\")\n",
    "\n",
    "    # Take the first match (assuming run_name is unique per experiment)\n",
    "    return runs.iloc[0].run_id\n",
    "\n",
    "parent_run_id = get_run_id_from_name(experiment_name, run_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a788ce15",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with mlflow.start_run(run_id=parent_run_id):\n",
    "    with mlflow.start_run(run_name='base-model-eval', nested=True):\n",
    "        results = mlflow.evaluate(\n",
    "            predict,\n",
    "            eval_df.head(int(os.environ.get(\"NUM_SAMPLES\", 5))),\n",
    "            evaluators=\"default\",\n",
    "            targets=\"ground_truth\",\n",
    "            extra_metrics=[\n",
    "                bleu(),\n",
    "                rouge1(),\n",
    "                rouge2(),\n",
    "                rougeL(),\n",
    "                rougeLsum(),\n",
    "                latency(),\n",
    "                token_count(),\n",
    "                answer_similarity_metric,\n",
    "                answer_correctness_metric,\n",
    "                answer_relevance_metric,\n",
    "                # relevance_metric, # requires context column\n",
    "                # faithfulness_metric, # requires context column\n",
    "                professionalism_metric,\n",
    "                helpfulness_metric,\n",
    "                politeness_metric,\n",
    "            ],\n",
    "        )\n",
    "\n",
    "        try:\n",
    "            mlflow.log_metrics(results.metrics)\n",
    "        except:\n",
    "            print('could not log metrics to mlflow')\n",
    "\n",
    "        print(results.metrics)\n",
    "        results.tables['eval_results_table'].to_csv(\n",
    "            '/opt/ml/output/data/eval_results_table.csv'\n",
    "        )\n",
    "        # results.tables['eval_results_table'].to_csv('results.csv')\n",
    "        print(results.tables['eval_results_table'])\n",
    "\n",
    "\n",
    "    with mlflow.start_run(run_name='tuned-model-eval', nested=True):\n",
    "        tuned_results = mlflow.evaluate(\n",
    "            tuned_predict,\n",
    "            eval_df.head(int(os.environ.get(\"NUM_SAMPLES\", 5))),\n",
    "            evaluators=\"default\",\n",
    "            targets=\"ground_truth\",\n",
    "            extra_metrics=[\n",
    "                bleu(),\n",
    "                rouge1(),\n",
    "                rouge2(),\n",
    "                rougeL(),\n",
    "                rougeLsum(),\n",
    "                latency(),\n",
    "                token_count(),\n",
    "                answer_similarity_metric,\n",
    "                answer_correctness_metric,\n",
    "                answer_relevance_metric,\n",
    "                # relevance_metric, # requires context column\n",
    "                # faithfulness_metric, # requires context column\n",
    "                professionalism_metric,\n",
    "                helpfulness_metric,\n",
    "                politeness_metric,\n",
    "            ],\n",
    "        )\n",
    "\n",
    "        try:\n",
    "            mlflow.log_metrics(tuned_results.metrics)\n",
    "        except:\n",
    "            print('could not log metrics to mlflow')\n",
    "\n",
    "        print(tuned_results.metrics)\n",
    "        tuned_results.tables['eval_results_table'].to_csv(\n",
    "            '/opt/ml/output/data/eval_results_table.csv'\n",
    "        )\n",
    "        # results.tables['eval_results_table'].to_csv('results.csv')\n",
    "        print(tuned_results.tables['eval_results_table'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4693fa9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c8b0b1f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c4fbe9f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d7916e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "968c6b51",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Appendix: ModelTrainer API for a SageMaker Eval Job"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89697bf1",
   "metadata": {},
   "source": [
    "Installed required libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2b1d2c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade -r requirements-fine-tuning.txt -q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f1818af",
   "metadata": {},
   "source": [
    "Import the necessary libraries and modules\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07c08c66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from dotenv import load_dotenv\n",
    "# load_dotenv()\n",
    "import os\n",
    "import boto3\n",
    "from scripts.utils import get_mlflow_server_arn\n",
    "from sagemaker.modules.train import ModelTrainer\n",
    "from sagemaker.modules.configs import Compute, SourceCode, StoppingCondition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91508105",
   "metadata": {},
   "outputs": [],
   "source": [
    "tracking_server_name = \"vlm-finetuning-server\"\n",
    "tracking_server_arn = get_mlflow_server_arn(tracking_server_name)\n",
    "tracking_server_arn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "440d0fdf",
   "metadata": {},
   "source": [
    "SageMaker AI provides containers for popular ML frameworks, including PyTorch, TensorFlow, and Hugging Face. These containers are optimized for performance and security, allowing you to focus on building and deploying your models without worrying about the underlying infrastructure.\n",
    "\n",
    "We are using the pytorch 2.8.0 container with Python 3.12. Then we are setting up the ModelTrainer API from the SageMaker SDK to run our training job. We are specifying the instance type, instance count, role, and source code location. We are also setting the stopping condition to 1 hour.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c119baf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "pytorch_image = '763104351884.dkr.ecr.us-east-1.amazonaws.com/pytorch-training:2.8.0-gpu-py312-cu129-ubuntu22.04-sagemaker'\n",
    "# define the script to be run\n",
    "source_code = SourceCode(\n",
    "    source_dir=\"scripts\",\n",
    "    entry_script=\"eval.sh\",\n",
    ")\n",
    "\n",
    "from huggingface_hub import HfFolder\n",
    "\n",
    "model_id = 'Qwen/Qwen3-0.6B'\n",
    "dataset_name = 'Josephgflowers/Finance-Instruct-500k'\n",
    "\n",
    "environment = {\n",
    "    'HF_TOKEN': HfFolder.get_token(),\n",
    "    \"HF_DATASET\": dataset_name,\n",
    "    \"MODEL_ID\": model_id,\n",
    "    \"MLFLOW_TRACKING_URI\": os.getenv(\n",
    "        'MLFLOW_TRACKING_URI',\n",
    "        'arn:aws:sagemaker:us-east-1:198346569064:mlflow-tracking-server/vlm-finetuning-server',\n",
    "    ),\n",
    "    \"MLFLOW_EXPERIMENT_NAME\": \"qwen3-06b-lora-ft-finance\",\n",
    "    \"NUM_SAMPLES\": '10',\n",
    "}\n",
    "\n",
    "# experiment_name = 'qwen3-06b-lora-ft-finance'\n",
    "# run_name = 'qwen3-06b-finance'\n",
    "\n",
    "hyperparameters = {\n",
    "    \"model_id\": model_id,\n",
    "    \"adapter_path\": 's3://sagemaker-us-east-1-198346569064/qwen3-06b-fine-tuned/',\n",
    "    'dataset_name': dataset_name,\n",
    "    'experiment_name': 'qwen3-06b-lora-ft-finance',\n",
    "    'run_name': 'fine-tuning-run-1',\n",
    "}\n",
    "\n",
    "assert (\n",
    "    environment[\"MLFLOW_TRACKING_URI\"] != \"XXX\"\n",
    "), \"Please set your MLFLOW_TRACKING_URI in the environment variable\"\n",
    "\n",
    "assert (\n",
    "    environment[\"HF_TOKEN\"] is not None\n",
    "), \"Please set your HF_TOKEN in the environment variable\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30a57da0",
   "metadata": {},
   "source": [
    "We are using a G5 2xlarge instance for our training job. This instance type is optimized for machine learning workloads and provides a good balance of compute, memory, and networking resources. The g5 instances are powered by NVIDIA A10G Tensor Core GPUs, which are well-suited for training large language models.\n",
    "\n",
    "The G5 has a single NVIDIA A10G GPU, 8 vCPUs, and 32 GB of memory. This instance type is a good choice for training large language models, as it provides enough compute power to handle the training workload while still being cost-effective.\n",
    "\n",
    "If you are running multiple experiments you can speed up the container download time by using warm pools. Warm pools are configured with the `keep_alive_period_in_seconds` parameter. This parameter specifies the amount of time that the warm pool will keep the container alive after the training job has completed. This can help to reduce the time it takes to start a new training job, as the container will already be downloaded and ready to use.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88c24185",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopping_condition = StoppingCondition(\n",
    "    max_runtime_in_seconds=60 * 60 * 10,  # seconds * minutes * hours\n",
    ")\n",
    "\n",
    "compute = Compute(\n",
    "    instance_count=1,\n",
    "    instance_type=\"ml.g5.2xlarge\",\n",
    "    # volume_size_in_gb=96,\n",
    "    keep_alive_period_in_seconds=3600,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a56e2da5",
   "metadata": {},
   "source": [
    "We tie together all the previous objects and configurations to create a ModelTrainer instance. This instance is responsible for orchestrating the training job on SageMaker. We pass in the role, source code configuration, compute configuration, and stopping condition to the ModelTrainer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3d7789c",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_job_name = \"mlflow-eval-llmaaj\"\n",
    "\n",
    "# define the ModelTrainer\n",
    "model_trainer = ModelTrainer(\n",
    "    training_image=pytorch_image,\n",
    "    source_code=source_code,\n",
    "    stopping_condition=stopping_condition,\n",
    "    base_job_name=base_job_name,\n",
    "    compute=compute,\n",
    "    environment=environment,\n",
    "    hyperparameters=hyperparameters,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1460e434",
   "metadata": {},
   "source": [
    "`model_trainer.fit()` is the command that initiates the training job on SageMaker. This method will package the source code, upload it to S3, and start the training job using the specified compute resources and configurations. The training job will run the `train.sh` script, which essentially installs some python libraries and then executes the `sft.py` script to fine-tune the model and log the results to MLflow.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a81fb69a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_trainer.train(wait=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "uv (3.12.11)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
