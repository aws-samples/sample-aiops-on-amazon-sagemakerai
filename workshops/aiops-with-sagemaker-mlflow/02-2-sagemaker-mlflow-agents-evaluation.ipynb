{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agent Offline Evaluation with SageMaker Managed MLflow\n",
    "In this lab, you will assess the performance of your IT support GenAI agent using SageMaker Managed MLflow. Offline evaluation is a critical method for testing agent behavior against historical, labeled data before deploying into production.\n",
    "\n",
    "You’ll configure MLflow, review metric types, run evaluation on prepared data, and interpret results using the MLflow tracking server.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-08T21:17:07.231060Z",
     "iopub.status.busy": "2025-10-08T21:17:07.230763Z",
     "iopub.status.idle": "2025-10-08T21:17:07.237271Z",
     "shell.execute_reply": "2025-10-08T21:17:07.236473Z",
     "shell.execute_reply.started": "2025-10-08T21:17:07.231037Z"
    }
   },
   "source": [
    "## Install Dependencies\n",
    "First, ensure your notebook environment has all necessary packages. Ignore any warnings from pre-installed packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install following dependencies. Ignore any warnings and residual dependency errors.\n",
    "!pip install -r requirements-langgraph.txt -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-08T21:17:25.103720Z",
     "iopub.status.busy": "2025-10-08T21:17:25.103444Z",
     "iopub.status.idle": "2025-10-08T21:17:25.109750Z",
     "shell.execute_reply": "2025-10-08T21:17:25.109222Z",
     "shell.execute_reply.started": "2025-10-08T21:17:25.103700Z"
    }
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "import os\n",
    "\n",
    "import sagemaker_mlflow\n",
    "import sagemaker\n",
    "import mlflow\n",
    "import pandas as pd \n",
    "\n",
    "print(mlflow.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure SagemakerAI managed MLFlow\n",
    "We will configure MLflow for experiment tracking by setting `mlflow.set_tracking_uri()` and `mlflow.set_experiment(\"<YOUR_MLFLOW_EXPERIMENT_NAME>\")` methods.\n",
    "\n",
    "We will retrieve the stored notebook values containing the SageMakerAI MLflow Tracking Server ARN. If the stored value is empty you can enter your tracking server arn to the following place holder string TRACKING_SERVER_ARN. Additionally, you can give any MLflow experiment name, in this workshop, we will give an experiment name: agent-mlflow-demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-08T21:17:48.872896Z",
     "iopub.status.busy": "2025-10-08T21:17:48.872596Z",
     "iopub.status.idle": "2025-10-08T21:17:48.879676Z",
     "shell.execute_reply": "2025-10-08T21:17:48.878886Z",
     "shell.execute_reply.started": "2025-10-08T21:17:48.872853Z"
    }
   },
   "outputs": [],
   "source": [
    "# Retrieve values stored from previous labs\n",
    "# If the stored value (or if you get NameError) is empty, set your SageMaker Managed MLflow tracking server ARN copied from prerequisites\n",
    "%store -r \n",
    "\n",
    "%store\n",
    "if TRACKING_SERVER_ARN == \"\":\n",
    "    print(\"ENTER YOUR MLFLOW TRACKING SERVER ARN\")\n",
    "TRACKING_SERVER_ARN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-08T19:18:34.411969Z",
     "iopub.status.busy": "2025-10-08T19:18:34.411680Z",
     "iopub.status.idle": "2025-10-08T19:18:34.422195Z",
     "shell.execute_reply": "2025-10-08T19:18:34.421568Z",
     "shell.execute_reply.started": "2025-10-08T19:18:34.411948Z"
    }
   },
   "outputs": [],
   "source": [
    "# If the stored value is empty set your SageMaker Managed MLflow tracking server ARN by uncommenting the below line\n",
    "#TRACKING_SERVER_ARN = \"ENTER YOUR MLFLOW TRACKIHG SERVER ARN HERE\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-08T19:20:44.098057Z",
     "iopub.status.busy": "2025-10-08T19:20:44.097788Z",
     "iopub.status.idle": "2025-10-08T19:20:44.836873Z",
     "shell.execute_reply": "2025-10-08T19:20:44.836119Z",
     "shell.execute_reply.started": "2025-10-08T19:20:44.098037Z"
    }
   },
   "outputs": [],
   "source": [
    "tracking_server_arn = TRACKING_SERVER_ARN \n",
    "experiment_name = \"agent-mlflow-demo\"\n",
    "\n",
    "# Set MLflow SDK to your configured tracking server \n",
    "mlflow.set_tracking_uri(tracking_server_arn) \n",
    "# Create or select an MLflow experiment\n",
    "mlflow.set_experiment(experiment_name)\n",
    "\n",
    "# Enable autologging module\n",
    "mlflow.langchain.autolog()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Offline Evaluation Dataset curation\n",
    "\n",
    "When you build your Agent, you will want to test your agent performance based on some pre-defined groudtruth dataset (Offline). The following example shows the user inputs, agent actual outputs and expected outputs. This lets you benchmark performance, reliability, and generalization. \n",
    "\n",
    "In this section the Offline Evaluation Dataset is already prepared for you and we will load the the Offline Evaluation Dataset from the dataset `./data/agent_evaluation_dataset.json`\n",
    "\n",
    "(Optional) We prepared the Offline Evaluation Dataset `./data/agent_evaluation_dataset.json` using the langgraph customer support agent you created in the previous lab. You can find this dataset creation in the notebook [/data/02-how-to-create-evaluation-dataset.ipynb](./data/02-create-evaluation-dataset-optional.ipynb).\n",
    "\n",
    "Now load in the json data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-08T20:49:45.390501Z",
     "iopub.status.busy": "2025-10-08T20:49:45.390222Z",
     "iopub.status.idle": "2025-10-08T20:49:45.399247Z",
     "shell.execute_reply": "2025-10-08T20:49:45.398589Z",
     "shell.execute_reply.started": "2025-10-08T20:49:45.390479Z"
    }
   },
   "outputs": [],
   "source": [
    "evaluation_dataset_path = \"./data/agent_evaluation_dataset.json\"\n",
    "eval_df = pd.read_json(evaluation_dataset_path, orient=\"records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full evaluation dataset\n",
    "eval_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Optional/Skip) Or comment in the following block to use the example dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# eval_df = pd.DataFrame([{\"inputs\":\"Can you help me solve the ticket: TICKET-001?\",\"actual_output\":\"1. Check network connectivity between client and server\\n2. Verify if the server is running and accessible\\n3. Increase the connection timeout settings\\n4. Check for firewall rules blocking the connection\\n5. Monitor network latency and bandwidth\\n    \",\"expected_output\":\"1. Check network connectivity between client and server\\n2. Verify if the server is running and accessible\\n3. Increase the connection timeout settings\\n4. Check for firewall rules blocking the connection\\n5. Monitor network latency and bandwidth\"},{\"inputs\":\"Can you help me with this ticket id: TICKET-002?\",\"actual_output\":\"1. Verify database credentials are correct\\n2. Check if the database user account is locked\\n3. Ensure database service is running\\n4. Review database access permissions\\n5. Check for recent password changes\\n    \",\"expected_output\":\"1. Verify database credentials are correct\\n2. Check if the database user account is locked\\n3. Ensure database service is running\\n4. Review database access permissions\\n5. Check for recent password changes\"},{\"inputs\":\"I got a ticket: TICKET-003, can you help me with this?\",\"actual_output\":\"1. Analyze application memory usage patterns\\n2. Increase available memory or swap space\\n3. Look for memory leaks in the application\\n4. Optimize database queries and caching\\n5. Consider implementing memory pooling\\n    \",\"expected_output\":\"1. Analyze application memory usage patterns\\n2. Increase available memory or swap space\\n3. Look for memory leaks in the application\\n4. Optimize database queries and caching\\n5. Consider implementing memory pooling\"},{\"inputs\":\"Help me solve the ticekt: TICKET-004\",\"actual_output\":\"error type not found in the knowledge base, please use your own knowledge\",\"expected_output\":\"1. Implement request throttling\\n2. Use caching to reduce API calls\\n3. Review API usage patterns\\n4. Contact service provider for limit increase\\n5. Optimize API call frequency\"},{\"inputs\":\"How do I solve this ticket: TICKET-005?\",\"actual_output\":\"1. Check certificate expiration date\\n2. Verify certificate chain is complete\\n3. Ensure certificate matches domain name\\n4. Update SSL certificate if expired\\n5. Check certificate authority validity\\n    \",\"expected_output\":\"1. Check certificate expiration date\\n2. Verify certificate chain is complete\\n3. Ensure certificate matches domain name\\n4. Update SSL certificate if expired\\n5. Check certificate authority validity\"},{\"inputs\":\"I need help with this ticket: TICKET-006\",\"actual_output\":\"1. Remove temporary and unnecessary files\\n2. Implement log rotation\\n3. Archive old data\\n4. Expand disk space\\n5. Monitor disk usage trends\\n    \",\"expected_output\":\"1. Remove temporary and unnecessary files\\n2. Implement log rotation\\n3. Archive old data\\n4. Expand disk space\\n5. Monitor disk usage trends\"},{\"inputs\":\"What do I do to resolve TICKET-007\",\"actual_output\":\"1. Check physical network connections\\n2. Verify router and switch status\\n3. Test DNS resolution\\n4. Check for network interface errors\\n5. Monitor network traffic patterns\\n    \",\"expected_output\":\"1. Check physical network connections\\n2. Verify router and switch status\\n3. Test DNS resolution\\n4. Check for network interface errors\\n5. Monitor network traffic patterns\"},{\"inputs\":\"I need help with this ticket: TICKET-008\",\"actual_output\":\"1. Review user access rights\\n2. Check file and directory permissions\\n3. Verify group memberships\\n4. Update security policies\\n5. Audit access control lists\\n    \",\"expected_output\":\"1. Review user access rights\\n2. Check file and directory permissions\\n3. Verify group memberships\\n4. Update security policies\\n5. Audit access control lists\"},{\"inputs\":\"How should I fix TICKET-009?\",\"actual_output\":\"1. Check service status and logs\\n2. Restart the service\\n3. Verify dependencies are running\\n4. Check system source documents. Monitor service health metrics\\n    \",\"expected_output\":\"1. Check service status and logs\\n2. Restart the service\\n3. Verify dependencies are running\\n4. Check system reyour source documents. Monitor service health metrics\"},{\"inputs\":\"I was just assigned TICKET-010, what do I do?\",\"actual_output\":\"1. Locate backup configuration files\\n2. Restore from version control\\n3. Create new configuration file with default settings\\n4. Check file path and permissions\\n5. Verify application deployment process\\n    \",\"expected_output\":\"1. Locate backup configuration files\\n2. Restore from version control\\n3. Create new configuration file with default settings\\n4. Check file path and permissions\\n5. Verify application deployment process\"},{\"inputs\":\"I need help with this ticket\",\"actual_output\":\"I apologize, but you haven't provided the ticket number. Could you please share the specific ticket ID so I can help you identify and resolve the error?\",\"expected_output\":\"I apologize, but I can only provide ETL resolution steps for specific ticket IDs. Please provide a ticket ID.\"}])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <div class=\"alert alert-block alert-info\">\n",
    "<b>Important:</b> We will take a smaller sample dataset for evaluation to avoid Amazon Bedrock throttling errors. Amazon Amazon Bedrock limits how many requests and tokens you can send to a foundation model per second or minute. If you exceed these rates, Amazon Bedrock temporarily rejects your requests with throttling errors.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sampled evaluation dataset\n",
    "eval_df = eval_df[2:4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create evaluation metrics for MLflow\n",
    "\n",
    "MLflow provides a wide range of built-in and custom metric options to flexibly score agent outputs. MLflow has two built-in metrics and :\n",
    "1. [Heuristic Metric](https://mlflow.org/docs/2.21.3/llms/llm-evaluate/#heuristic-based-metrics)\n",
    "2. [LLM as a Judge metrics](https://mlflow.org/docs/2.21.3/llms/llm-evaluate/#llm-as-a-judge-metrics).\n",
    "3. Module to create and capture custom metrics\n",
    "\n",
    "In this section, we will create you one Heuristic Metric [ROUGE score](https://huggingface.co/spaces/evaluate-metric/rouge) and one LLM-as-a-Judge metric [Answer Similarity](https://mlflow.org/docs/2.21.3/api_reference/python_api/mlflow.metrics.html#mlflow.metrics.genai.answer_similarity). Additional, we will show you how to create a custom metric. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Heuristic Metrics\n",
    "Heuristic metrics use rules or direct calculations for scoring. Common examples include ROUGE (for text overlap) and exact match. The MLflow's rougeL score measures how similar a model’s generated text is to a reference (ground truth) text using the ROUGE (Recall-Oriented Understudy for Gisting Evaluation) family of metrics. MLflow rougeL score closer to 1 means the output closely matches reference (high-quality summarization or answer similarity). And near 0 infers a poor overlap, unrelated or noisy output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-08T19:20:54.723673Z",
     "iopub.status.busy": "2025-10-08T19:20:54.723410Z",
     "iopub.status.idle": "2025-10-08T19:20:54.741289Z",
     "shell.execute_reply": "2025-10-08T19:20:54.740383Z",
     "shell.execute_reply.started": "2025-10-08T19:20:54.723653Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define the MLflow heuristic metric for text similarity\n",
    "rouge_metric = mlflow.metrics.rougeL()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-08T19:23:09.630152Z",
     "iopub.status.busy": "2025-10-08T19:23:09.629865Z",
     "iopub.status.idle": "2025-10-08T19:23:09.635302Z",
     "shell.execute_reply": "2025-10-08T19:23:09.634287Z",
     "shell.execute_reply.started": "2025-10-08T19:23:09.630131Z"
    }
   },
   "source": [
    "### Define MLFLow LLM-as-Judege metrics using Bedrock as the evaluator LLM\n",
    "LLM-as-a-Judge metrics use a large language model deployed via Bedrock or another provider to rate answer quality, correctness, and similarity. For this notebook we will use Amazon Bedrock model to configure the MLFLow LLM-as-Judege metric. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-08T19:24:54.455667Z",
     "iopub.status.busy": "2025-10-08T19:24:54.455405Z",
     "iopub.status.idle": "2025-10-08T19:24:54.887897Z",
     "shell.execute_reply": "2025-10-08T19:24:54.887249Z",
     "shell.execute_reply.started": "2025-10-08T19:24:54.455646Z"
    }
   },
   "outputs": [],
   "source": [
    "os.environ[\"AWS_ROLE_ARN\"] = sagemaker.get_execution_role()\n",
    "LLM_EVALUATOR=\"bedrock:/global.anthropic.claude-sonnet-4-20250514-v1:0\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First lets make sure the IAM role and credentials allow mlflow metric to perform evaluation and access Bedrock LLM access. \n",
    "Configure your AWS IAM role to allow access to Bedrock LLM services for evaluation. \n",
    "(Note: You will need to wait for a few minutes for the below IAM policy to take effect before proceeding.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-08T19:59:44.998945Z",
     "iopub.status.busy": "2025-10-08T19:59:44.998664Z",
     "iopub.status.idle": "2025-10-08T19:59:45.110746Z",
     "shell.execute_reply": "2025-10-08T19:59:45.110113Z",
     "shell.execute_reply.started": "2025-10-08T19:59:44.998924Z"
    }
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "import json\n",
    "\n",
    "# Create an IAM client\n",
    "iam = boto3.client('iam')\n",
    "\n",
    "# Define the role name and the new trust policy\n",
    "role_name = os.environ[\"AWS_ROLE_ARN\"].split(\"/\")[-1]  # Replace with your role's name\n",
    "role_arn = os.environ[\"AWS_ROLE_ARN\"]\n",
    "new_trust_policy = {\n",
    "    \"Version\": \"2012-10-17\",\n",
    "    \"Statement\": [\n",
    "        {\n",
    "\t\t\t\"Effect\": \"Allow\",\n",
    "\t\t\t\"Principal\": {\n",
    "\t\t\t\t\"Service\": \"sagemaker.amazonaws.com\"\n",
    "\t\t\t},\n",
    "\t\t\t\"Action\": \"sts:AssumeRole\"\n",
    "\t\t},\n",
    "        {\n",
    "            \"Effect\": \"Allow\",\n",
    "            \"Principal\": {\n",
    "                \"AWS\": role_arn  # Allow mlflow metric to assume the role\n",
    "            },\n",
    "            \"Action\": \"sts:AssumeRole\"\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "try:\n",
    "    # Update the assume role policy\n",
    "    response = iam.update_assume_role_policy(\n",
    "        RoleName=role_name,\n",
    "        PolicyDocument=json.dumps(new_trust_policy)\n",
    "    )\n",
    "    print(f\"Trust policy for role '{role_name}' updated successfully.\")\n",
    "    print(json.dumps(response, indent=4))\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error updating trust policy for role '{role_name}': {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <div class=\"alert alert-block alert-warning\">\n",
    "<b>Important:</b> Wait for a few minutes for the IAM policy to take effect before proceeding to the next notebook cell. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optionally instead of attaching the trust policy as you show in the previous cell, you can instead set the AWS credentials in the environment\n",
    "\n",
    "`AWS_ACCESS_KEY_ID` = `<your-aws-access-key-id>`\n",
    "\n",
    "`AWS_SECRET_ACCESS_KEY` = `<your-aws-secret-access-key>`\n",
    "\n",
    "`AWS_REGION` = `<your aws region>`\n",
    "\n",
    "`AWS_SESSION_TOKEN` = `<your-aws-session-token>`\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Now define the `mlflow.metrics.genai.answer_similarity()` metric which evaluates how similar a agent generated output is compared to the information in the ground truth data.\n",
    "This metric will ask the evaluator LLM to score how close your agent's prediction is to the ideal answer (scale 1-5, low-high). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-08T20:08:45.996855Z",
     "iopub.status.busy": "2025-10-08T20:08:45.996576Z",
     "iopub.status.idle": "2025-10-08T20:08:46.001030Z",
     "shell.execute_reply": "2025-10-08T20:08:46.000240Z",
     "shell.execute_reply.started": "2025-10-08T20:08:45.996835Z"
    }
   },
   "outputs": [],
   "source": [
    "answer_similarity_mlflow_metric_bedrock = mlflow.metrics.genai.answer_similarity(\n",
    "    model=LLM_EVALUATOR,\n",
    "        parameters={\n",
    "        \"temperature\": 0,\n",
    "        \"max_tokens\": 256,\n",
    "        \"anthropic_version\": \"bedrock-2023-05-31\",\n",
    "    },\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-08T20:08:47.440297Z",
     "iopub.status.busy": "2025-10-08T20:08:47.439930Z",
     "iopub.status.idle": "2025-10-08T20:08:50.172905Z",
     "shell.execute_reply": "2025-10-08T20:08:50.171892Z",
     "shell.execute_reply.started": "2025-10-08T20:08:47.440274Z"
    }
   },
   "outputs": [],
   "source": [
    "# Test the metric definition. Score range 1 - 5 / LOW - HIGH\n",
    "answer_similarity_mlflow_metric_bedrock(\n",
    "    inputs=\"What is the largest planet in our solar system?\",\n",
    "    predictions=\"The moon is the largest planet in our solar system.\",\n",
    "    targets=\"The largest planet in our solar system is Jupiter.\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Note: If you see Bedrock model access error verify the IAM Trust policy is attached and re-run the previous cell again. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLflow Custom Metrics: \n",
    "\n",
    "Custom metrics let you tailor evaluation logic to your agent’s workflow, use-case, or business rules.\n",
    "You can design a scorer for any aspect not covered by built-in metrics; for example, exact match correctness as shown below. This section we will show you how to build your own custom metrics.\n",
    "\n",
    "1. You need to define your custom metric method with two arguments `predictions` and `targets`. \"predictions\" is a list of predicted results from you agent and \"targets\" is a list of ground truth. You can define any custom certeria for judgement. Here we design it to be exact match \n",
    "2. Use `make_metrics` method to make our customer metrics to be a MLflow metrics. Notices that here we use `greater_is_better = True` This indicate the score is higher the better. In future if you have similar metrics like incorrectness, you can set `greater_is_better = False`\n",
    "3. This metric returns 1 for predictions that exactly match ground truth and 0 otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-08T20:22:12.775175Z",
     "iopub.status.busy": "2025-10-08T20:22:12.774879Z",
     "iopub.status.idle": "2025-10-08T20:22:12.780423Z",
     "shell.execute_reply": "2025-10-08T20:22:12.779663Z",
     "shell.execute_reply.started": "2025-10-08T20:22:12.775151Z"
    }
   },
   "outputs": [],
   "source": [
    "from mlflow.metrics import MetricValue \n",
    "from mlflow.models import make_metric\n",
    "\n",
    "def custom_correctness(predictions, targets):\n",
    "    '''\n",
    "    custom correctness based on exact match \n",
    "\n",
    "    Args:\n",
    "        precitions: list of agent predicted values \n",
    "        targets: list of ground truth values \n",
    "\n",
    "    Returns: \n",
    "        A list of MetricValue scores\n",
    "    '''\n",
    "    scores = [] \n",
    "\n",
    "    for prediction, target in zip(predictions, targets):\n",
    "        if prediction.strip() == target.strip():\n",
    "            scores.append(1)\n",
    "        else:\n",
    "            scores.append(0)\n",
    "\n",
    "    return MetricValue(scores= scores)\n",
    "\n",
    "custom_correctness_metric = make_metric(\n",
    "    eval_fn=custom_correctness, greater_is_better= True , name=\"custom_answer_correctness\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execute the MLflow Evaluation \n",
    "Now we can use `mlflow.evaluate()` to see the performance of the agent on an evaluation dataset we prepared. We will use the three metrics we defined:\n",
    "1. Heuristic Metric `rouge_metric`\n",
    "2. LLM as a Judge metrics `answer_similarity_mlflow_metric_bedrock`.\n",
    "3. Custom metric `custom_correctness_metric`\n",
    "\n",
    "\n",
    "> Important note: Due to throughput limitation enforced in the AWS workshop lab accounts bedrock models you many see evaluation result scores containing null value `NaN`. Ignore the null value `NaN` as this is due to the temporary high demand throughputs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-08T20:49:48.682300Z",
     "iopub.status.busy": "2025-10-08T20:49:48.682030Z",
     "iopub.status.idle": "2025-10-08T20:50:00.924404Z",
     "shell.execute_reply": "2025-10-08T20:50:00.923713Z",
     "shell.execute_reply.started": "2025-10-08T20:49:48.682280Z"
    }
   },
   "outputs": [],
   "source": [
    "with mlflow.start_run() as evaluation_run:\n",
    "    eval_dataset = mlflow.data.from_pandas(\n",
    "        df=eval_df,\n",
    "        name=\"eval_dataset\",\n",
    "        targets=\"expected_output\",\n",
    "        predictions=\"actual_output\"\n",
    "    )\n",
    "\n",
    "    # Set project level tracking information as desired for governance and lineage\n",
    "    phase = \"offline-agent-evaluation\"\n",
    "    stage = \"offline\"\n",
    "    task = \"it-support\"\n",
    "    version = \"1.0.0\"\n",
    "    user = role_arn\n",
    "    mlflow.log_input(\n",
    "        dataset=eval_dataset,\n",
    "        context=phase,\n",
    "            tags={\n",
    "                \"task\": task,\n",
    "                \"split\": stage,\n",
    "                \"version\": version\n",
    "            }\n",
    "    )\n",
    "    mlflow.log_param(\"data_path\", evaluation_dataset_path)\n",
    "    mlflow.log_param(\"llm-as-a-judge\", LLM_EVALUATOR)\n",
    "    mlflow.set_tag(\"experiment_phase\", phase)\n",
    "    mlflow.set_tag(\"version\", version)\n",
    "    \n",
    "    # Evaluation run in MLflow with results logged for ROUGE, LLM-Judge answer similarity, and custom correctness.\n",
    "    result = mlflow.evaluate(\n",
    "        data=eval_dataset,\n",
    "        extra_metrics=[\n",
    "            rouge_metric,\n",
    "            answer_similarity_mlflow_metric_bedrock,\n",
    "            custom_correctness_metric\n",
    "        ],\n",
    "    )\n",
    "       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Offline Evaluation Results\n",
    "\n",
    "The results of the evaluation will be logged to the MLflow experiment run and also returned to your evaluation function with new columns for result scores along with the original evaluation dataset provided. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Note: If you see `NaN` under the column `answer_similarity/v1/score` it is due to Amazon Bedrock model throttling restrictions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-08T20:29:16.990149Z",
     "iopub.status.busy": "2025-10-08T20:29:16.988985Z",
     "iopub.status.idle": "2025-10-08T20:29:18.116887Z",
     "shell.execute_reply": "2025-10-08T20:29:18.116358Z",
     "shell.execute_reply.started": "2025-10-08T20:29:16.990109Z"
    }
   },
   "outputs": [],
   "source": [
    "print(f\"See aggregated evaluation results below: \\n{result.metrics}\")\n",
    "result.tables[\"eval_results_table\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpreting Results and Error Analysis\n",
    "\n",
    "Let's inspect one of the poor performing test cases:\n",
    "\n",
    "The error,  API Rate Limit Exceeded, for ticket TICKET-004 is not present in the internal storage i.e, the sample solution_book in the workshop lab. This means the `information_retriever` agent tool could not return the resolution steps to resolve the error. Indicating to the agent developers possible areas of improvement. \n",
    "\n",
    "The output from the model is \"error type not found in the knowledge base, please use your own knowledge\" instead of the expected result: \n",
    "1. Implement request throttling\n",
    "2. Use caching to reduce API calls\n",
    "3. Review API usage patterns\n",
    "4. Contact service provider for limit increase\n",
    "5. Optimize API call frequency\n",
    "\n",
    "Thus the LLM as a Judge Metric for answer similarity is low, a 1 out of 5, for this test case, catching the gap in the informatiion_retriever tool. This helps diagnose why a given ticket received a low similarity score—useful for improving agent logic and tool design \n",
    "\n",
    "Hence you can use the evaluation to interpret agent performace and areas that need improvement.retrieval coverage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View Results in the SageMaker managed MLflow UI\n",
    "After completing the evaluation run:\n",
    "- Open SageMaker Studio MLflow UI → view experiment runs, and inspect model metrics.\n",
    "\n",
    "Summary: You have now curated an offline evaluation dataset, defined diverse MLflow metrics, and conducted a full agent evaluation pipeline using SageMaker Managed MLflow. Perform aggregation, artifact browsing, and trace visualizations for deeper analysis. Use these techniques for reliable agent deployment and ongoing improvement. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
