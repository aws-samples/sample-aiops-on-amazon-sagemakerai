{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLflow Fundamentals: Hands-On Workshop lab\n",
    "\n",
    "Welcome to the hands-on MLflow fundamentals workshop lab! This notebook will guide you through all the core MLflow concepts step by step, allowing you to practice and verify each construct on a real MLflow tracking server.\n",
    "\n",
    "#### Prerequisites\n",
    "\n",
    "**Important:** You need the SageMaker Managed MLflow tracking server ARN from the workshop prerequisites section. Replace the placeholder below with your actual tracking server ARN.\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will:\n",
    "- Understand and practice with MLflow Experiments, Runs, Parameters, Metrics, and Artifacts\n",
    "- Learn essential MLflow Python SDK usage with practical examples\n",
    "- Apply best practices for effective ML experiment tracking\n",
    "- Experience automatic logging capabilities\n",
    "- Verify all concepts on a live MLflow tracking server"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Star GitHub repository\n",
    "This notebook is sourced from the public github repository `https://github.com/aws-samples/sample-aiops-on-amazon-sagemakerai#`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-28T18:43:24.405101Z",
     "iopub.status.busy": "2025-09-28T18:43:24.404829Z",
     "iopub.status.idle": "2025-09-28T18:43:24.412106Z",
     "shell.execute_reply": "2025-09-28T18:43:24.411468Z",
     "shell.execute_reply.started": "2025-09-28T18:43:24.405081Z"
    }
   },
   "outputs": [],
   "source": [
    "%%html\n",
    "\n",
    "<a class=\"github-button\" href=\"https://github.com/aws-samples/sample-aiops-on-amazon-sagemakerai#\" data-color-scheme=\"no-preference: light; light: light; dark: dark;\" data-icon=\"octicon-star\" data-size=\"large\" data-show-count=\"true\" aria-label=\"Star Amazon SageMaker AI AIOps on GitHub\">Star</a>\n",
    "<script async defer src=\"https://buttons.github.io/buttons.js\"></script>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-28T18:44:04.828353Z",
     "iopub.status.busy": "2025-09-28T18:44:04.828092Z",
     "iopub.status.idle": "2025-09-28T18:44:04.831394Z",
     "shell.execute_reply": "2025-09-28T18:44:04.830541Z",
     "shell.execute_reply.started": "2025-09-28T18:44:04.828333Z"
    }
   },
   "source": [
    "### Click this button ^^^ above ^^^"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import and setup\n",
    "\n",
    "Let's start by setting up our environment and connecting to the MLflow tracking server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-28T18:40:08.776310Z",
     "iopub.status.busy": "2025-09-28T18:40:08.775966Z",
     "iopub.status.idle": "2025-09-28T18:40:15.178305Z",
     "shell.execute_reply": "2025-09-28T18:40:15.173394Z",
     "shell.execute_reply.started": "2025-09-28T18:40:08.776284Z"
    }
   },
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from mlflow.tracking import MlflowClient\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"‚úÖ All packages imported successfully!\")\n",
    "print(f\"MLflow version: {mlflow.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>Important:</b> Update `TRACKING_SERVER_ARN` with your SageMaker Managed MLflow tracking server ARN.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-28T18:47:32.520204Z",
     "iopub.status.busy": "2025-09-28T18:47:32.519905Z",
     "iopub.status.idle": "2025-09-28T18:47:32.524790Z",
     "shell.execute_reply": "2025-09-28T18:47:32.524044Z",
     "shell.execute_reply.started": "2025-09-28T18:47:32.520180Z"
    }
   },
   "outputs": [],
   "source": [
    "# Configure MLflow tracking server\n",
    "# Replace with your SageMaker Managed MLflow tracking server ARN copied from prerequisites section\n",
    "TRACKING_SERVER_ARN = \"arn:aws:sagemaker:<REGION>:<ACCOUNT-ID>:mlflow-tracking-server/<NAME>\"\n",
    "\n",
    "# Set the tracking URI\n",
    "mlflow.set_tracking_uri(TRACKING_SERVER_ARN)\n",
    "\n",
    "print(f\"‚úÖ MLflow tracking server configured: {mlflow.get_tracking_uri()}\")\n",
    "print(\"\\nüìù Note: Make sure to replace the TRACKING_SERVER_ARN with your actual ARN from the prerequisites section\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-28T18:47:33.552729Z",
     "iopub.status.busy": "2025-09-28T18:47:33.552470Z",
     "iopub.status.idle": "2025-09-28T18:47:33.556335Z",
     "shell.execute_reply": "2025-09-28T18:47:33.555722Z",
     "shell.execute_reply.started": "2025-09-28T18:47:33.552709Z"
    }
   },
   "outputs": [],
   "source": [
    "# Store some variables to keep the value between the notebooks\n",
    "%store TRACKING_SERVER_ARN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample Dataset Preparation\n",
    "\n",
    "Let's create a sample dataset that we'll use throughout this workshop to demonstrate MLflow constructs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-28T18:47:39.604426Z",
     "iopub.status.busy": "2025-09-28T18:47:39.604149Z",
     "iopub.status.idle": "2025-09-28T18:47:39.613980Z",
     "shell.execute_reply": "2025-09-28T18:47:39.613369Z",
     "shell.execute_reply.started": "2025-09-28T18:47:39.604405Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create a sample classification dataset\n",
    "X, y = make_classification(\n",
    "    n_samples=1000,\n",
    "    n_features=10,\n",
    "    n_informative=5,\n",
    "    n_redundant=2,\n",
    "    n_classes=2,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Dataset created:\")\n",
    "print(f\"   Training samples: {X_train.shape[0]}\")\n",
    "print(f\"   Test samples: {X_test.shape[0]}\")\n",
    "print(f\"   Features: {X_train.shape[1]}\")\n",
    "print(f\"   Classes: {len(np.unique(y))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-28T18:50:10.713652Z",
     "iopub.status.busy": "2025-09-28T18:50:10.713387Z",
     "iopub.status.idle": "2025-09-28T18:50:10.718035Z",
     "shell.execute_reply": "2025-09-28T18:50:10.717173Z",
     "shell.execute_reply.started": "2025-09-28T18:50:10.713631Z"
    }
   },
   "source": [
    "Note: The SageMaker managed MLflow's IAM Role will need have sagemaker MLflow IAM permissions.\n",
    "These IAM permissions are added in the pre-requesites sections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 1. MLflow Experiments\n",
    "\n",
    "**What is an Experiment?**\n",
    "An experiment is like a project workspace that organizes all your ML runs for a specific problem or goal. Think of it as a smart folder that stores your model training attempts and helps you compare them.\n",
    "\n",
    "Let's create our first experiment!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-28T18:55:43.114698Z",
     "iopub.status.busy": "2025-09-28T18:55:43.114366Z",
     "iopub.status.idle": "2025-09-28T18:55:43.998393Z",
     "shell.execute_reply": "2025-09-28T18:55:43.997696Z",
     "shell.execute_reply.started": "2025-09-28T18:55:43.114668Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create and set an experiment\n",
    "experiment_name = \"mlflow-fundamentals-workshop\"\n",
    "mlflow.set_experiment(experiment_name)\n",
    "\n",
    "# Get experiment details\n",
    "experiment = mlflow.get_experiment_by_name(experiment_name)\n",
    "print(f\"‚úÖ Experiment created successfully!\")\n",
    "print(f\"   Name: {experiment.name}\")\n",
    "print(f\"   ID: {experiment.experiment_id}\")\n",
    "print(f\"   Artifact Location: {experiment.artifact_location}\")\n",
    "\n",
    "print(\"\\nüéØ Best Practice: Use descriptive experiment names that reflect your project goals\")\n",
    "print(\"   Good: 'customer-churn-v2', 'fraud-detection-baseline'\")\n",
    "print(\"   Avoid: 'experiment1', 'test', 'my_exp'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**üîç Verification Step:**\n",
    "1. Go to your sagemaker managed MLflow tracking server UI (Go to the mlflow app section in your SageMaker AI Studio and click `Open Mlflow` )\n",
    "2. You should see the new experiment \"mlflow-constructs-workshop\" listed. Refresh the page if you do not see the new experiment.\n",
    "3. Click on it to explore (it will be empty for now)\n",
    "\n",
    "---\n",
    "\n",
    "# 2. MLflow Runs\n",
    "\n",
    "**What is a Run?**\n",
    "A run is like a detailed lab notebook entry for a single model training session. It captures everything that happened during that attempt - settings, results, files, and metadata.\n",
    "\n",
    "Let's create our first run!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-28T18:57:47.691560Z",
     "iopub.status.busy": "2025-09-28T18:57:47.691296Z",
     "iopub.status.idle": "2025-09-28T18:57:48.694998Z",
     "shell.execute_reply": "2025-09-28T18:57:48.694325Z",
     "shell.execute_reply.started": "2025-09-28T18:57:47.691539Z"
    }
   },
   "outputs": [],
   "source": [
    "# Basic run example\n",
    "with mlflow.start_run(run_name=\"baseline-model\") as run:\n",
    "    print(f\"‚úÖ Run started successfully!\")\n",
    "    print(f\"   Run ID: {run.info.run_id}\")\n",
    "    print(f\"   Run Name: baseline-model\")\n",
    "    print(f\"   Status: {run.info.status}\")\n",
    "    \n",
    "    # Simple logging for demonstration\n",
    "    mlflow.log_param(\"algorithm\", \"RandomForest\")\n",
    "    mlflow.log_metric(\"accuracy\", 0.85)\n",
    "    \n",
    "    print(\"   ‚úÖ Basic parameters and metrics logged\")\n",
    "\n",
    "print(\"\\nüéØ Best Practice: Always use 'with mlflow.start_run():' to ensure runs are properly closed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**üîç Verification Step:**\n",
    "1. Go back to your SageMaker managed MLflow app and Refresh your MLflow UI\n",
    "2. You should see one run named \"baseline-model\" in your experiment\n",
    "3. Click on the run to see the logged parameter and metric\n",
    "\n",
    "---\n",
    "\n",
    "# 3. Parameters and Metrics\n",
    "\n",
    "**The Input/Output Story:**\n",
    "- **Parameters**: Your \"recipe ingredients\" - settings you choose before training\n",
    "- **Metrics**: Your \"cooking results\" - measurements of how well your model performed\n",
    "\n",
    "Let's practice logging parameters and metrics properly!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-28T18:58:37.293543Z",
     "iopub.status.busy": "2025-09-28T18:58:37.293280Z",
     "iopub.status.idle": "2025-09-28T18:58:38.640502Z",
     "shell.execute_reply": "2025-09-28T18:58:38.639760Z",
     "shell.execute_reply.started": "2025-09-28T18:58:37.293523Z"
    }
   },
   "outputs": [],
   "source": [
    "# Train a Random Forest model with proper parameter and metric logging\n",
    "with mlflow.start_run(run_name=\"random-forest-detailed\") as run:\n",
    "    # Model hyperparameters (Parameters - your choices)\n",
    "    n_estimators = 100\n",
    "    max_depth = 10\n",
    "    random_state = 42\n",
    "    \n",
    "    # Log individual parameters\n",
    "    mlflow.log_param(\"algorithm\", \"RandomForest\")\n",
    "    mlflow.log_param(\"n_estimators\", n_estimators)\n",
    "    mlflow.log_param(\"max_depth\", max_depth)\n",
    "    mlflow.log_param(\"random_state\", random_state)\n",
    "    \n",
    "    # Log multiple parameters at once\n",
    "    mlflow.log_params({\n",
    "        \"train_size\": len(X_train),\n",
    "        \"test_size\": len(X_test),\n",
    "        \"n_features\": X_train.shape[1]\n",
    "    })\n",
    "    \n",
    "    # Train the model\n",
    "    model = RandomForestClassifier(\n",
    "        n_estimators=n_estimators,\n",
    "        max_depth=max_depth,\n",
    "        random_state=random_state\n",
    "    )\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # Calculate metrics (Metrics - your results)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    \n",
    "    # Log individual metrics\n",
    "    mlflow.log_metric(\"accuracy\", accuracy)\n",
    "    mlflow.log_metric(\"precision\", precision)\n",
    "    mlflow.log_metric(\"recall\", recall)\n",
    "    \n",
    "    # Log multiple metrics at once\n",
    "    mlflow.log_metrics({\n",
    "        \"f1_score\": f1,\n",
    "        \"train_samples\": len(X_train),\n",
    "        \"test_samples\": len(X_test)\n",
    "    })\n",
    "    \n",
    "    print(f\"‚úÖ Random Forest model trained and logged!\")\n",
    "    print(f\"   Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"   Precision: {precision:.4f}\")\n",
    "    print(f\"   Recall: {recall:.4f}\")\n",
    "    print(f\"   F1-Score: {f1:.4f}\")\n",
    "\n",
    "print(\"\\nüéØ Best Practice: Log parameters before training and metrics after evaluation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also demonstrate logging training curves with steps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-28T18:58:52.888006Z",
     "iopub.status.busy": "2025-09-28T18:58:52.887742Z",
     "iopub.status.idle": "2025-09-28T18:58:58.525089Z",
     "shell.execute_reply": "2025-09-28T18:58:58.524429Z",
     "shell.execute_reply.started": "2025-09-28T18:58:52.887986Z"
    }
   },
   "outputs": [],
   "source": [
    "# Simulate training with steps (like epochs)\n",
    "with mlflow.start_run(run_name=\"training-curves-demo\") as run:\n",
    "    mlflow.log_param(\"simulation\", \"training_curves\")\n",
    "    \n",
    "    # Simulate training progress over epochs\n",
    "    for epoch in range(10):\n",
    "        # Simulate decreasing loss and increasing accuracy\n",
    "        train_loss = 1.0 - (epoch * 0.08) + np.random.normal(0, 0.02)\n",
    "        val_accuracy = 0.5 + (epoch * 0.04) + np.random.normal(0, 0.01)\n",
    "        \n",
    "        # Log metrics with step parameter for time series\n",
    "        mlflow.log_metric(\"train_loss\", train_loss, step=epoch)\n",
    "        mlflow.log_metric(\"val_accuracy\", val_accuracy, step=epoch)\n",
    "    \n",
    "    print(f\"‚úÖ Training curves logged for 10 epochs\")\n",
    "    print(\"   Check the MLflow UI to see the training curves!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**üîç Verification Step:**\n",
    "1. Go to your MLflow UI and check the latest runs\n",
    "2. Select the latest experiment run and open the Model metrics tab to verify the metrics\n",
    "3. Look at the training curves in the \"training-curves-demo\" run\n",
    "\n",
    "---\n",
    "\n",
    "# 4. Artifacts and Models\n",
    "\n",
    "**What are Artifacts?**\n",
    "Artifacts are the \"evidence files\" from your ML experiments - everything beyond numbers that tells your model's story.\n",
    "\n",
    "Let's create and log various types of artifacts!\n",
    "(Note: Ignore warnings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-28T19:01:14.941753Z",
     "iopub.status.busy": "2025-09-28T19:01:14.941451Z",
     "iopub.status.idle": "2025-09-28T19:01:26.411924Z",
     "shell.execute_reply": "2025-09-28T19:01:26.411300Z",
     "shell.execute_reply.started": "2025-09-28T19:01:14.941732Z"
    }
   },
   "outputs": [],
   "source": [
    "with mlflow.start_run(run_name=\"artifacts-and-models-demo\") as run:\n",
    "    # Train a model\n",
    "    model = RandomForestClassifier(n_estimators=50, random_state=42)\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # Log model parameters\n",
    "    mlflow.log_params({\n",
    "        \"algorithm\": \"RandomForest\",\n",
    "        \"n_estimators\": 50,\n",
    "        \"random_state\": 42\n",
    "    })\n",
    "    \n",
    "    # Log metrics\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    mlflow.log_metric(\"accuracy\", accuracy)\n",
    "    \n",
    "    # 1. Log the trained model\n",
    "    mlflow.sklearn.log_model(model, \"random_forest_model\")\n",
    "    print(\"‚úÖ Model logged as artifact\")\n",
    "    \n",
    "    # 2. Create and log a visualization plot\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    # Feature importance plot\n",
    "    feature_names = [f'feature_{i}' for i in range(X_train.shape[1])]\n",
    "    importance = model.feature_importances_\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.bar(range(len(importance)), importance)\n",
    "    plt.title('Feature Importance')\n",
    "    plt.xlabel('Features')\n",
    "    plt.ylabel('Importance')\n",
    "    \n",
    "    # Training progress simulation\n",
    "    plt.subplot(1, 2, 2)\n",
    "    epochs = range(1, 11)\n",
    "    accuracy_progress = [0.6 + i*0.03 + np.random.normal(0, 0.01) for i in epochs]\n",
    "    plt.plot(epochs, accuracy_progress, 'b-', marker='o')\n",
    "    plt.title('Training Progress')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"model_analysis.png\", dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    # Log the plot as artifact\n",
    "    mlflow.log_artifact(\"model_analysis.png\")\n",
    "    print(\"‚úÖ Visualization plot logged as artifact\")\n",
    "    \n",
    "    # 3. Log configuration as JSON\n",
    "    config = {\n",
    "        \"model_type\": \"RandomForest\",\n",
    "        \"version\": \"1.0\",\n",
    "        \"training_date\": \"2024-01-15\",\n",
    "        \"data_preprocessing\": {\n",
    "            \"scaling\": \"none\",\n",
    "            \"feature_selection\": \"none\"\n",
    "        },\n",
    "        \"performance\": {\n",
    "            \"accuracy\": float(accuracy),\n",
    "            \"model_size_mb\": 0.5\n",
    "        }\n",
    "    }\n",
    "    mlflow.log_dict(config, \"model_config.json\")\n",
    "    print(\"‚úÖ Configuration logged as JSON artifact\")\n",
    "    \n",
    "    # 4. Log a text report\n",
    "    report = f\"\"\"\n",
    "Model Training Report\n",
    "===================\n",
    "Algorithm: Random Forest\n",
    "Training Samples: {len(X_train)}\n",
    "Test Samples: {len(X_test)}\n",
    "Features: {X_train.shape[1]}\n",
    "\n",
    "Hyperparameters:\n",
    "- n_estimators: 50\n",
    "- random_state: 42\n",
    "\n",
    "Results:\n",
    "- Accuracy: {accuracy:.4f}\n",
    "- Top 3 Important Features: {', '.join([f'feature_{i}' for i in np.argsort(importance)[-3:]])}\n",
    "\n",
    "Notes:\n",
    "- Model shows good performance on test set\n",
    "- Feature importance analysis completed\n",
    "- Ready for further evaluation\n",
    "\"\"\"\n",
    "    \n",
    "    with open(\"training_report.txt\", \"w\") as f:\n",
    "        f.write(report)\n",
    "    \n",
    "    mlflow.log_artifact(\"training_report.txt\")\n",
    "    print(\"‚úÖ Training report logged as text artifact\")\n",
    "    \n",
    "    print(f\"\\nüìä Summary of logged artifacts:\")\n",
    "    print(f\"   - Trained model (random_forest_model/)\")\n",
    "    print(f\"   - Visualization plot (model_analysis.png)\")\n",
    "    print(f\"   - Configuration file (model_config.json)\")\n",
    "    print(f\"   - Training report (training_report.txt)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**üîç Verification Step:**\n",
    "1. Go to your MLflow UI and open the \"artifacts-and-models-demo\" run\n",
    "2. Scroll down to the `Logged models` and open the model name.\n",
    "3. Check the \"Artifacts\" section - you should see all logged files\n",
    "3. Note how the model is stored with its metadata and dependencies. The Model Directory `random_forest_model/` acts as the self-contained package for your trained machine learning model.\n",
    "\n",
    "| Filename | Purpose in Model Deployment\n",
    "|-----------|---------|\n",
    "| MLmodel | The Model Contract (YAML): This is the single most important metadata file. It defines the model's \"flavor\" (e.g., python_function, sklearn, pytorch), its signature (required inputs/expected outputs), and crucially, points to all dependent files (like model.pkl). SageMaker uses this file to understand how to load and serve the model without requiring custom code on the inference side. | \n",
    "| model.pkl | The Serialized Model: This file contains the trained machine learning object, usually serialized using Python's pickle or cloudpickle. This is the binary file representing the learned weights, coefficients, or structure of your Random Forest model. |\n",
    "| conda.yaml | Conda Environment Definition: This YAML file specifies the exact Python and library dependencies, including the specific versions required for the model to run correctly. SageMaker uses this file to construct the runtime environment (the Docker container) for the inference endpoint, guaranteeing environment parity with training. | \n",
    "| requirements.txt | Pip Dependencies: A simplified file listing the Python package dependencies (like scikit-learn==1.3.2). This is often used alongside conda.yaml to ensure all required libraries are installed during deployment. | \n",
    "| python_env.yaml| Python Environment Details: Contains the full specification needed to restore the Python environment via virtualenv and pip, complementing the conda.yaml for environment reproducibility.\n",
    "\n",
    "---\n",
    "\n",
    "# 5. Model Registry\n",
    "\n",
    "**What is the Model Registry?**\n",
    "The Model Registry is like a sophisticated library system for your trained models. It provides version control and manages the journey from development to production.\n",
    "\n",
    "Let's register and manage models!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-28T19:11:13.440776Z",
     "iopub.status.busy": "2025-09-28T19:11:13.440476Z",
     "iopub.status.idle": "2025-09-28T19:11:20.252983Z",
     "shell.execute_reply": "2025-09-28T19:11:20.252323Z",
     "shell.execute_reply.started": "2025-09-28T19:11:13.440753Z"
    }
   },
   "outputs": [],
   "source": [
    "# Train and register a model\n",
    "with mlflow.start_run(run_name=\"model-for-registry\") as run:\n",
    "    # Train a model\n",
    "    model = RandomForestClassifier(n_estimators=100, max_depth=8, random_state=42)\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # Log parameters and metrics\n",
    "    mlflow.log_params({\n",
    "        \"algorithm\": \"RandomForest\",\n",
    "        \"n_estimators\": 100,\n",
    "        \"max_depth\": 8\n",
    "    })\n",
    "    \n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    mlflow.log_metric(\"accuracy\", accuracy)\n",
    "    \n",
    "    # Log and register the model in one step\n",
    "    model_name = \"workshop-classifier\"\n",
    "    mlflow.sklearn.log_model(\n",
    "        model, \n",
    "        \"model\",\n",
    "        registered_model_name=model_name\n",
    "    )\n",
    "    \n",
    "    print(f\"‚úÖ Model trained and registered!\")\n",
    "    print(f\"   Model Name: {model_name}\")\n",
    "    print(f\"   Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"   Run ID: {run.info.run_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-28T19:15:56.322954Z",
     "iopub.status.busy": "2025-09-28T19:15:56.322614Z",
     "iopub.status.idle": "2025-09-28T19:15:56.545684Z",
     "shell.execute_reply": "2025-09-28T19:15:56.545075Z",
     "shell.execute_reply.started": "2025-09-28T19:15:56.322929Z"
    }
   },
   "outputs": [],
   "source": [
    "# Model lifecycle management\n",
    "client = MlflowClient()\n",
    "model_name = \"workshop-classifier\"\n",
    "\n",
    "try:\n",
    "    # Get the latest version\n",
    "    latest_versions = client.get_latest_versions(model_name)\n",
    "    if latest_versions:\n",
    "        latest_version = latest_versions[0]\n",
    "        print(f\"‚úÖ Found registered model:\")\n",
    "        print(f\"   Name: {latest_version.name}\")\n",
    "        print(f\"   Version: {latest_version.version}\")\n",
    "        \n",
    "        # Add description\n",
    "        client.update_model_version(\n",
    "            name=model_name,\n",
    "            version=latest_version.version,\n",
    "            description=\"Random Forest classifier trained in MLflow workshop. Shows good performance on test data.\"\n",
    "        )\n",
    "        print(f\"‚úÖ Model description added!\")\n",
    "        \n",
    "    else:\n",
    "        print(\"‚ùå No model versions found. Make sure the previous cell ran successfully.\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error managing model: {e}\")\n",
    "    print(\"This might happen if the model registry is not fully set up yet.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**üîç Verification Step:**\n",
    "1. Go to your MLflow UI and click on \"Models\" in the navigation\n",
    "2. You should see \"workshop-classifier\" in the model registry\n",
    "3. Click on it to see version history and further click on the model version to see more\n",
    "4. Verify the model version with the description\n",
    "\n",
    "---\n",
    "\n",
    "# 6. Organization with Tags\n",
    "\n",
    "**What are Tags?**\n",
    "Tags are like sticky notes for your ML runs - simple labels that help you organize and find experiments later.\n",
    "\n",
    "Let's practice using tags effectively!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-28T19:17:27.723650Z",
     "iopub.status.busy": "2025-09-28T19:17:27.723375Z",
     "iopub.status.idle": "2025-09-28T19:17:30.745850Z",
     "shell.execute_reply": "2025-09-28T19:17:30.745157Z",
     "shell.execute_reply.started": "2025-09-28T19:17:27.723629Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create multiple runs with different tags for organization\n",
    "algorithms = [\"RandomForest\", \"LogisticRegression\"]\n",
    "data_versions = [\"v1.0\", \"v1.1\"]\n",
    "\n",
    "for algorithm in algorithms:\n",
    "    for data_version in data_versions:\n",
    "        with mlflow.start_run(run_name=f\"{algorithm.lower()}-{data_version}\") as run:\n",
    "            # Basic tags\n",
    "            mlflow.set_tag(\"team\", \"data-science\")\n",
    "            mlflow.set_tag(\"environment\", \"development\")\n",
    "            mlflow.set_tag(\"algorithm\", algorithm)\n",
    "            mlflow.set_tag(\"data_version\", data_version)\n",
    "            \n",
    "            # Multiple tags at once\n",
    "            mlflow.set_tags({\n",
    "                \"experiment_type\": \"baseline_comparison\",\n",
    "                \"priority\": \"high\" if algorithm == \"RandomForest\" else \"medium\",\n",
    "                \"reviewer\": \"workshop-participant\",\n",
    "                \"status\": \"completed\"\n",
    "            })\n",
    "            \n",
    "            # Train appropriate model\n",
    "            if algorithm == \"RandomForest\":\n",
    "                model = RandomForestClassifier(n_estimators=50, random_state=42)\n",
    "            else:\n",
    "                model = LogisticRegression(random_state=42, max_iter=1000)\n",
    "            \n",
    "            model.fit(X_train, y_train)\n",
    "            y_pred = model.predict(X_test)\n",
    "            \n",
    "            # Log basic info\n",
    "            mlflow.log_param(\"algorithm\", algorithm)\n",
    "            mlflow.log_param(\"data_version\", data_version)\n",
    "            mlflow.log_metric(\"accuracy\", accuracy_score(y_test, y_pred))\n",
    "            \n",
    "            print(f\"‚úÖ {algorithm} with {data_version} - Tagged and logged\")\n",
    "\n",
    "print(\"\\nüè∑Ô∏è Tags added for easy filtering:\")\n",
    "print(\"   - team: data-science\")\n",
    "print(\"   - environment: development\")\n",
    "print(\"   - algorithm: RandomForest/LogisticRegression\")\n",
    "print(\"   - data_version: v1.0/v1.1\")\n",
    "print(\"   - experiment_type: baseline_comparison\")\n",
    "print(\"   - priority: high/medium\")\n",
    "print(\"   - reviewer: workshop-participant\")\n",
    "print(\"   - status: completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**üîç Verification Step:**\n",
    "1. Go to your MLflow UI and view all runs in your experiment\n",
    "2. Use the filter functionality to filter by tags. For example enter `tags.algorithm = \"RandomForest\"` in the search bar\n",
    "3. Try different tag combinations to see how they help organize experiments\n",
    "4. Notice how tags make it easy to find specific types of runs\n",
    "\n",
    "---\n",
    "\n",
    "# 7. Automatic Logging\n",
    "\n",
    "**What is Automatic Logging?**\n",
    "Automatic logging is MLflow's smart assistant that automatically captures parameters, metrics, models, and artifacts when you use popular ML libraries.\n",
    "\n",
    "Let's see the magic of autolog!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-28T19:19:35.803038Z",
     "iopub.status.busy": "2025-09-28T19:19:35.802677Z",
     "iopub.status.idle": "2025-09-28T19:19:39.229275Z",
     "shell.execute_reply": "2025-09-28T19:19:39.228504Z",
     "shell.execute_reply.started": "2025-09-28T19:19:35.803015Z"
    }
   },
   "outputs": [],
   "source": [
    "# Enable automatic logging for scikit-learn\n",
    "mlflow.sklearn.autolog()\n",
    "\n",
    "print(\"‚úÖ Automatic logging enabled for scikit-learn!\")\n",
    "print(\"   MLflow will now automatically log:\")\n",
    "print(\"   - All hyperparameters\")\n",
    "print(\"   - Training metrics\")\n",
    "print(\"   - Model artifacts\")\n",
    "print(\"   - Feature importance plots (when available)\")\n",
    "print(\"   - And much more!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-28T19:19:53.402173Z",
     "iopub.status.busy": "2025-09-28T19:19:53.401657Z",
     "iopub.status.idle": "2025-09-28T19:19:58.904957Z",
     "shell.execute_reply": "2025-09-28T19:19:58.904335Z",
     "shell.execute_reply.started": "2025-09-28T19:19:53.402149Z"
    }
   },
   "outputs": [],
   "source": [
    "# Train models with automatic logging - no manual logging needed!\n",
    "with mlflow.start_run(run_name=\"autolog-random-forest\") as run:\n",
    "    # Just your regular ML code - MLflow automatically logs everything!\n",
    "    model = RandomForestClassifier(\n",
    "        n_estimators=100, \n",
    "        max_depth=10, \n",
    "        min_samples_split=5,\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    # This single line triggers automatic logging of everything!\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Make predictions (also automatically logged)\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    print(f\"‚úÖ Random Forest trained with automatic logging!\")\n",
    "    print(f\"   Check the MLflow UI to see all the automatically logged information!\")\n",
    "    print(f\"   Run ID: {run.info.run_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-28T19:20:46.448578Z",
     "iopub.status.busy": "2025-09-28T19:20:46.448300Z",
     "iopub.status.idle": "2025-09-28T19:20:51.215116Z",
     "shell.execute_reply": "2025-09-28T19:20:51.214362Z",
     "shell.execute_reply.started": "2025-09-28T19:20:46.448558Z"
    }
   },
   "outputs": [],
   "source": [
    "# Compare with Logistic Regression using autolog\n",
    "with mlflow.start_run(run_name=\"autolog-logistic-regression\") as run:\n",
    "    # Different algorithm, same automatic logging magic!\n",
    "    model = LogisticRegression(\n",
    "        C=1.0,\n",
    "        solver='liblinear',\n",
    "        random_state=42,\n",
    "        max_iter=1000\n",
    "    )\n",
    "    \n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    print(f\"‚úÖ Logistic Regression trained with automatic logging!\")\n",
    "    print(f\"   Run ID: {run.info.run_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-28T19:21:03.826070Z",
     "iopub.status.busy": "2025-09-28T19:21:03.825697Z",
     "iopub.status.idle": "2025-09-28T19:21:09.454859Z",
     "shell.execute_reply": "2025-09-28T19:21:09.454227Z",
     "shell.execute_reply.started": "2025-09-28T19:21:03.826041Z"
    }
   },
   "outputs": [],
   "source": [
    "# You can still add manual logging on top of automatic logging\n",
    "with mlflow.start_run(run_name=\"autolog-plus-manual\") as run:\n",
    "    # Automatic logging handles the basics\n",
    "    model = RandomForestClassifier(n_estimators=150, random_state=42)\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Add your custom tags and metrics\n",
    "    mlflow.set_tags({\n",
    "        \"custom_experiment\": \"autolog_demo\",\n",
    "        \"notes\": \"Combining autolog with manual logging\"\n",
    "    })\n",
    "    \n",
    "    # Add custom metrics\n",
    "    y_pred = model.predict(X_test)\n",
    "    custom_score = accuracy_score(y_test, y_pred) * 100  # Percentage\n",
    "    mlflow.log_metric(\"accuracy_percentage\", custom_score)\n",
    "    \n",
    "    print(f\"‚úÖ Combined automatic + manual logging completed!\")\n",
    "    print(f\"   Automatic: All sklearn parameters, metrics, and model\")\n",
    "    print(f\"   Manual: Custom tags and percentage accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**üîç Verification Step:**\n",
    "1. Go to your MLflow UI and compare the autolog runs with manual logging runs\n",
    "2. Notice how autolog captured many more parameters automatically\n",
    "3. Check if feature importance plots were automatically generated\n",
    "4. Compare the amount of information captured with minimal code\n",
    "\n",
    "---\n",
    "\n",
    "# üéØ Workshop Summary and Best Practices\n",
    "\n",
    "Congratulations! You've successfully practiced all core MLflow constructs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üîó Useful Resources\n",
    "\n",
    "- **MLflow Documentation**: https://mlflow.org/docs/latest/\n",
    "- **MLflow Tracking**: https://mlflow.org/docs/latest/tracking.html\n",
    "- **MLflow Models**: https://mlflow.org/docs/latest/models.html\n",
    "- **MLflow Model Registry**: https://mlflow.org/docs/latest/model-registry.html\n",
    "- **Automatic Logging**: https://mlflow.org/docs/latest/tracking.html#automatic-logging\n",
    "\n",
    "### üéì Workshop lab Complete!\n",
    "\n",
    "You have successfully completed the MLflow fundamentals hands-on workshop lab! You now have practical experience with all core MLflow concepts and are ready to apply them to your machine learning projects.\n",
    "\n",
    "**Remember**: Now start incorporating these constructs into your daily ML workflow, and you'll soon see the benefits of organized, reproducible, and collaborative machine learning experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
