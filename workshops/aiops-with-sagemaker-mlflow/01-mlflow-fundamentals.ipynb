{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLflow Constructs: Hands-On Workshop\n",
    "\n",
    "Welcome to the hands-on MLflow constructs workshop! This notebook will guide you through all the core MLflow concepts step by step, allowing you to practice and verify each construct on a real MLflow tracking server.\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "**Important:** You need the SageMaker Managed MLflow tracking server ARN from the workshop prerequisites section. Replace the placeholder below with your actual tracking server ARN.\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will:\n",
    "- Understand and practice with MLflow Experiments, Runs, Parameters, Metrics, and Artifacts\n",
    "- Learn essential MLflow Python SDK usage with practical examples\n",
    "- Apply best practices for effective ML experiment tracking\n",
    "- Experience automatic logging capabilities\n",
    "- Verify all concepts on a live MLflow tracking server"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Configuration\n",
    "\n",
    "Let's start by setting up our environment and connecting to the MLflow tracking server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install mlflow boto3 scikit-learn matplotlib seaborn pandas numpy -q\n",
    "\n",
    "# Import necessary libraries\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from mlflow.tracking import MlflowClient\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"‚úÖ All packages imported successfully!\")\n",
    "print(f\"MLflow version: {mlflow.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure MLflow tracking server\n",
    "# Replace with your SageMaker Managed MLflow tracking server ARN from prerequisites\n",
    "TRACKING_SERVER_ARN = \"arn:aws:sagemaker:us-east-1:123456789012:mlflow-tracking-server/your-server-name\"\n",
    "\n",
    "# Set the tracking URI\n",
    "mlflow.set_tracking_uri(TRACKING_SERVER_ARN)\n",
    "\n",
    "print(f\"‚úÖ MLflow tracking server configured: {mlflow.get_tracking_uri()}\")\n",
    "print(\"\\nüìù Note: Make sure to replace the TRACKING_SERVER_ARN with your actual ARN from the prerequisites section\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample Dataset Preparation\n",
    "\n",
    "Let's create a sample dataset that we'll use throughout this workshop to demonstrate MLflow constructs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a sample classification dataset\n",
    "X, y = make_classification(\n",
    "    n_samples=1000,\n",
    "    n_features=10,\n",
    "    n_informative=5,\n",
    "    n_redundant=2,\n",
    "    n_classes=2,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Dataset created:\")\n",
    "print(f\"   Training samples: {X_train.shape[0]}\")\n",
    "print(f\"   Test samples: {X_test.shape[0]}\")\n",
    "print(f\"   Features: {X_train.shape[1]}\")\n",
    "print(f\"   Classes: {len(np.unique(y))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 1. MLflow Experiments\n",
    "\n",
    "**What is an Experiment?**\n",
    "An experiment is like a project workspace that organizes all your ML runs for a specific problem or goal. Think of it as a smart folder that stores your model training attempts and helps you compare them.\n",
    "\n",
    "Let's create our first experiment!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and set an experiment\n",
    "experiment_name = \"mlflow-constructs-workshop\"\n",
    "mlflow.set_experiment(experiment_name)\n",
    "\n",
    "# Get experiment details\n",
    "experiment = mlflow.get_experiment_by_name(experiment_name)\n",
    "print(f\"‚úÖ Experiment created successfully!\")\n",
    "print(f\"   Name: {experiment.name}\")\n",
    "print(f\"   ID: {experiment.experiment_id}\")\n",
    "print(f\"   Artifact Location: {experiment.artifact_location}\")\n",
    "\n",
    "print(\"\\nüéØ Best Practice: Use descriptive experiment names that reflect your project goals\")\n",
    "print(\"   Good: 'customer-churn-v2', 'fraud-detection-baseline'\")\n",
    "print(\"   Avoid: 'experiment1', 'test', 'my_exp'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**üîç Verification Step:**\n",
    "1. Go to your MLflow tracking server UI\n",
    "2. You should see the new experiment \"mlflow-constructs-workshop\" listed\n",
    "3. Click on it to explore (it will be empty for now)\n",
    "\n",
    "---\n",
    "\n",
    "# 2. MLflow Runs\n",
    "\n",
    "**What is a Run?**\n",
    "A run is like a detailed lab notebook entry for a single model training session. It captures everything that happened during that attempt - settings, results, files, and metadata.\n",
    "\n",
    "Let's create our first run!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic run example\n",
    "with mlflow.start_run(run_name=\"baseline-model\") as run:\n",
    "    print(f\"‚úÖ Run started successfully!\")\n",
    "    print(f\"   Run ID: {run.info.run_id}\")\n",
    "    print(f\"   Run Name: baseline-model\")\n",
    "    print(f\"   Status: {run.info.status}\")\n",
    "    \n",
    "    # Simple logging for demonstration\n",
    "    mlflow.log_param(\"algorithm\", \"RandomForest\")\n",
    "    mlflow.log_metric(\"accuracy\", 0.85)\n",
    "    \n",
    "    print(\"   ‚úÖ Basic parameters and metrics logged\")\n",
    "\n",
    "print(\"\\nüéØ Best Practice: Always use 'with mlflow.start_run():' to ensure runs are properly closed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**üîç Verification Step:**\n",
    "1. Refresh your MLflow UI\n",
    "2. You should see one run named \"baseline-model\" in your experiment\n",
    "3. Click on the run to see the logged parameter and metric\n",
    "\n",
    "---\n",
    "\n",
    "# 3. Parameters and Metrics\n",
    "\n",
    "**The Input/Output Story:**\n",
    "- **Parameters**: Your \"recipe ingredients\" - settings you choose before training\n",
    "- **Metrics**: Your \"cooking results\" - measurements of how well your model performed\n",
    "\n",
    "Let's practice logging parameters and metrics properly!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a Random Forest model with proper parameter and metric logging\n",
    "with mlflow.start_run(run_name=\"random-forest-detailed\") as run:\n",
    "    # Model hyperparameters (Parameters - your choices)\n",
    "    n_estimators = 100\n",
    "    max_depth = 10\n",
    "    random_state = 42\n",
    "    \n",
    "    # Log individual parameters\n",
    "    mlflow.log_param(\"algorithm\", \"RandomForest\")\n",
    "    mlflow.log_param(\"n_estimators\", n_estimators)\n",
    "    mlflow.log_param(\"max_depth\", max_depth)\n",
    "    mlflow.log_param(\"random_state\", random_state)\n",
    "    \n",
    "    # Log multiple parameters at once\n",
    "    mlflow.log_params({\n",
    "        \"train_size\": len(X_train),\n",
    "        \"test_size\": len(X_test),\n",
    "        \"n_features\": X_train.shape[1]\n",
    "    })\n",
    "    \n",
    "    # Train the model\n",
    "    model = RandomForestClassifier(\n",
    "        n_estimators=n_estimators,\n",
    "        max_depth=max_depth,\n",
    "        random_state=random_state\n",
    "    )\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # Calculate metrics (Metrics - your results)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    \n",
    "    # Log individual metrics\n",
    "    mlflow.log_metric(\"accuracy\", accuracy)\n",
    "    mlflow.log_metric(\"precision\", precision)\n",
    "    mlflow.log_metric(\"recall\", recall)\n",
    "    \n",
    "    # Log multiple metrics at once\n",
    "    mlflow.log_metrics({\n",
    "        \"f1_score\": f1,\n",
    "        \"train_samples\": len(X_train),\n",
    "        \"test_samples\": len(X_test)\n",
    "    })\n",
    "    \n",
    "    print(f\"‚úÖ Random Forest model trained and logged!\")\n",
    "    print(f\"   Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"   Precision: {precision:.4f}\")\n",
    "    print(f\"   Recall: {recall:.4f}\")\n",
    "    print(f\"   F1-Score: {f1:.4f}\")\n",
    "\n",
    "print(\"\\nüéØ Best Practice: Log parameters before training and metrics after evaluation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also demonstrate logging training curves with steps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate training with steps (like epochs)\n",
    "with mlflow.start_run(run_name=\"training-curves-demo\") as run:\n",
    "    mlflow.log_param(\"simulation\", \"training_curves\")\n",
    "    \n",
    "    # Simulate training progress over epochs\n",
    "    for epoch in range(10):\n",
    "        # Simulate decreasing loss and increasing accuracy\n",
    "        train_loss = 1.0 - (epoch * 0.08) + np.random.normal(0, 0.02)\n",
    "        val_accuracy = 0.5 + (epoch * 0.04) + np.random.normal(0, 0.01)\n",
    "        \n",
    "        # Log metrics with step parameter for time series\n",
    "        mlflow.log_metric(\"train_loss\", train_loss, step=epoch)\n",
    "        mlflow.log_metric(\"val_accuracy\", val_accuracy, step=epoch)\n",
    "    \n",
    "    print(f\"‚úÖ Training curves logged for 10 epochs\")\n",
    "    print(\"   Check the MLflow UI to see the training curves!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**üîç Verification Step:**\n",
    "1. Go to your MLflow UI and check the latest runs\n",
    "2. Compare parameters and metrics between different runs\n",
    "3. Look at the training curves in the \"training-curves-demo\" run\n",
    "\n",
    "---\n",
    "\n",
    "# 4. Artifacts and Models\n",
    "\n",
    "**What are Artifacts?**\n",
    "Artifacts are the \"evidence files\" from your ML experiments - everything beyond numbers that tells your model's story.\n",
    "\n",
    "Let's create and log various types of artifacts!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with mlflow.start_run(run_name=\"artifacts-and-models-demo\") as run:\n",
    "    # Train a model\n",
    "    model = RandomForestClassifier(n_estimators=50, random_state=42)\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # Log model parameters\n",
    "    mlflow.log_params({\n",
    "        \"algorithm\": \"RandomForest\",\n",
    "        \"n_estimators\": 50,\n",
    "        \"random_state\": 42\n",
    "    })\n",
    "    \n",
    "    # Log metrics\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    mlflow.log_metric(\"accuracy\", accuracy)\n",
    "    \n",
    "    # 1. Log the trained model\n",
    "    mlflow.sklearn.log_model(model, \"random_forest_model\")\n",
    "    print(\"‚úÖ Model logged as artifact\")\n",
    "    \n",
    "    # 2. Create and log a visualization plot\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    # Feature importance plot\n",
    "    feature_names = [f'feature_{i}' for i in range(X_train.shape[1])]\n",
    "    importance = model.feature_importances_\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.bar(range(len(importance)), importance)\n",
    "    plt.title('Feature Importance')\n",
    "    plt.xlabel('Features')\n",
    "    plt.ylabel('Importance')\n",
    "    \n",
    "    # Training progress simulation\n",
    "    plt.subplot(1, 2, 2)\n",
    "    epochs = range(1, 11)\n",
    "    accuracy_progress = [0.6 + i*0.03 + np.random.normal(0, 0.01) for i in epochs]\n",
    "    plt.plot(epochs, accuracy_progress, 'b-', marker='o')\n",
    "    plt.title('Training Progress')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"model_analysis.png\", dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    # Log the plot as artifact\n",
    "    mlflow.log_artifact(\"model_analysis.png\")\n",
    "    print(\"‚úÖ Visualization plot logged as artifact\")\n",
    "    \n",
    "    # 3. Log configuration as JSON\n",
    "    config = {\n",
    "        \"model_type\": \"RandomForest\",\n",
    "        \"version\": \"1.0\",\n",
    "        \"training_date\": \"2024-01-15\",\n",
    "        \"data_preprocessing\": {\n",
    "            \"scaling\": \"none\",\n",
    "            \"feature_selection\": \"none\"\n",
    "        },\n",
    "        \"performance\": {\n",
    "            \"accuracy\": float(accuracy),\n",
    "            \"model_size_mb\": 0.5\n",
    "        }\n",
    "    }\n",
    "    mlflow.log_dict(config, \"model_config.json\")\n",
    "    print(\"‚úÖ Configuration logged as JSON artifact\")\n",
    "    \n",
    "    # 4. Log a text report\n",
    "    report = f\"\"\"\n",
    "Model Training Report\n",
    "===================\n",
    "Algorithm: Random Forest\n",
    "Training Samples: {len(X_train)}\n",
    "Test Samples: {len(X_test)}\n",
    "Features: {X_train.shape[1]}\n",
    "\n",
    "Hyperparameters:\n",
    "- n_estimators: 50\n",
    "- random_state: 42\n",
    "\n",
    "Results:\n",
    "- Accuracy: {accuracy:.4f}\n",
    "- Top 3 Important Features: {', '.join([f'feature_{i}' for i in np.argsort(importance)[-3:]])}\n",
    "\n",
    "Notes:\n",
    "- Model shows good performance on test set\n",
    "- Feature importance analysis completed\n",
    "- Ready for further evaluation\n",
    "\"\"\"\n",
    "    \n",
    "    with open(\"training_report.txt\", \"w\") as f:\n",
    "        f.write(report)\n",
    "    \n",
    "    mlflow.log_artifact(\"training_report.txt\")\n",
    "    print(\"‚úÖ Training report logged as text artifact\")\n",
    "    \n",
    "    print(f\"\\nüìä Summary of logged artifacts:\")\n",
    "    print(f\"   - Trained model (random_forest_model/)\")\n",
    "    print(f\"   - Visualization plot (model_analysis.png)\")\n",
    "    print(f\"   - Configuration file (model_config.json)\")\n",
    "    print(f\"   - Training report (training_report.txt)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**üîç Verification Step:**\n",
    "1. Go to your MLflow UI and open the \"artifacts-and-models-demo\" run\n",
    "2. Check the \"Artifacts\" section - you should see all logged files\n",
    "3. Download and view the artifacts to verify they contain the expected content\n",
    "4. Note how the model is stored with its metadata and dependencies\n",
    "\n",
    "---\n",
    "\n",
    "# 5. Model Registry\n",
    "\n",
    "**What is the Model Registry?**\n",
    "The Model Registry is like a sophisticated library system for your trained models. It provides version control and manages the journey from development to production.\n",
    "\n",
    "Let's register and manage models!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and register a model\n",
    "with mlflow.start_run(run_name=\"model-for-registry\") as run:\n",
    "    # Train a model\n",
    "    model = RandomForestClassifier(n_estimators=100, max_depth=8, random_state=42)\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # Log parameters and metrics\n",
    "    mlflow.log_params({\n",
    "        \"algorithm\": \"RandomForest\",\n",
    "        \"n_estimators\": 100,\n",
    "        \"max_depth\": 8\n",
    "    })\n",
    "    \n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    mlflow.log_metric(\"accuracy\", accuracy)\n",
    "    \n",
    "    # Log and register the model in one step\n",
    "    model_name = \"workshop-classifier\"\n",
    "    mlflow.sklearn.log_model(\n",
    "        model, \n",
    "        \"model\",\n",
    "        registered_model_name=model_name\n",
    "    )\n",
    "    \n",
    "    print(f\"‚úÖ Model trained and registered!\")\n",
    "    print(f\"   Model Name: {model_name}\")\n",
    "    print(f\"   Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"   Run ID: {run.info.run_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model lifecycle management\n",
    "client = MlflowClient()\n",
    "model_name = \"workshop-classifier\"\n",
    "\n",
    "try:\n",
    "    # Get the latest version\n",
    "    latest_versions = client.get_latest_versions(model_name)\n",
    "    if latest_versions:\n",
    "        latest_version = latest_versions[0]\n",
    "        print(f\"‚úÖ Found registered model:\")\n",
    "        print(f\"   Name: {latest_version.name}\")\n",
    "        print(f\"   Version: {latest_version.version}\")\n",
    "        print(f\"   Current Stage: {latest_version.current_stage}\")\n",
    "        \n",
    "        # Promote to Staging\n",
    "        client.transition_model_version_stage(\n",
    "            name=model_name,\n",
    "            version=latest_version.version,\n",
    "            stage=\"Staging\"\n",
    "        )\n",
    "        print(f\"\\n‚úÖ Model promoted to Staging stage!\")\n",
    "        \n",
    "        # Add description\n",
    "        client.update_model_version(\n",
    "            name=model_name,\n",
    "            version=latest_version.version,\n",
    "            description=\"Random Forest classifier trained in MLflow workshop. Shows good performance on test data.\"\n",
    "        )\n",
    "        print(f\"‚úÖ Model description added!\")\n",
    "        \n",
    "    else:\n",
    "        print(\"‚ùå No model versions found. Make sure the previous cell ran successfully.\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error managing model: {e}\")\n",
    "    print(\"This might happen if the model registry is not fully set up yet.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**üîç Verification Step:**\n",
    "1. Go to your MLflow UI and click on \"Models\" in the navigation\n",
    "2. You should see \"workshop-classifier\" in the model registry\n",
    "3. Click on it to see version history and stage transitions\n",
    "4. Verify the model is in \"Staging\" stage with the description\n",
    "\n",
    "---\n",
    "\n",
    "# 6. Organization with Tags\n",
    "\n",
    "**What are Tags?**\n",
    "Tags are like sticky notes for your ML runs - simple labels that help you organize and find experiments later.\n",
    "\n",
    "Let's practice using tags effectively!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create multiple runs with different tags for organization\n",
    "algorithms = [\"RandomForest\", \"LogisticRegression\"]\n",
    "data_versions = [\"v1.0\", \"v1.1\"]\n",
    "\n",
    "for algorithm in algorithms:\n",
    "    for data_version in data_versions:\n",
    "        with mlflow.start_run(run_name=f\"{algorithm.lower()}-{data_version}\") as run:\n",
    "            # Basic tags\n",
    "            mlflow.set_tag(\"team\", \"data-science\")\n",
    "            mlflow.set_tag(\"environment\", \"development\")\n",
    "            mlflow.set_tag(\"algorithm\", algorithm)\n",
    "            mlflow.set_tag(\"data_version\", data_version)\n",
    "            \n",
    "            # Multiple tags at once\n",
    "            mlflow.set_tags({\n",
    "                \"experiment_type\": \"baseline_comparison\",\n",
    "                \"priority\": \"high\" if algorithm == \"RandomForest\" else \"medium\",\n",
    "                \"reviewer\": \"workshop-participant\",\n",
    "                \"status\": \"completed\"\n",
    "            })\n",
    "            \n",
    "            # Train appropriate model\n",
    "            if algorithm == \"RandomForest\":\n",
    "                model = RandomForestClassifier(n_estimators=50, random_state=42)\n",
    "            else:\n",
    "                model = LogisticRegression(random_state=42, max_iter=1000)\n",
    "            \n",
    "            model.fit(X_train, y_train)\n",
    "            y_pred = model.predict(X_test)\n",
    "            \n",
    "            # Log basic info\n",
    "            mlflow.log_param(\"algorithm\", algorithm)\n",
    "            mlflow.log_param(\"data_version\", data_version)\n",
    "            mlflow.log_metric(\"accuracy\", accuracy_score(y_test, y_pred))\n",
    "            \n",
    "            print(f\"‚úÖ {algorithm} with {data_version} - Tagged and logged\")\n",
    "\n",
    "print(\"\\nüè∑Ô∏è Tags added for easy filtering:\")\n",
    "print(\"   - team: data-science\")\n",
    "print(\"   - environment: development\")\n",
    "print(\"   - algorithm: RandomForest/LogisticRegression\")\n",
    "print(\"   - data_version: v1.0/v1.1\")\n",
    "print(\"   - experiment_type: baseline_comparison\")\n",
    "print(\"   - priority: high/medium\")\n",
    "print(\"   - reviewer: workshop-participant\")\n",
    "print(\"   - status: completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**üîç Verification Step:**\n",
    "1. Go to your MLflow UI and view all runs in your experiment\n",
    "2. Use the filter functionality to filter by tags (e.g., `tags.algorithm = \"RandomForest\"`)\n",
    "3. Try different tag combinations to see how they help organize experiments\n",
    "4. Notice how tags make it easy to find specific types of runs\n",
    "\n",
    "---\n",
    "\n",
    "# 7. Automatic Logging\n",
    "\n",
    "**What is Automatic Logging?**\n",
    "Automatic logging is MLflow's smart assistant that automatically captures parameters, metrics, models, and artifacts when you use popular ML libraries.\n",
    "\n",
    "Let's see the magic of autolog!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable automatic logging for scikit-learn\n",
    "mlflow.sklearn.autolog()\n",
    "\n",
    "print(\"‚úÖ Automatic logging enabled for scikit-learn!\")\n",
    "print(\"   MLflow will now automatically log:\")\n",
    "print(\"   - All hyperparameters\")\n",
    "print(\"   - Training metrics\")\n",
    "print(\"   - Model artifacts\")\n",
    "print(\"   - Feature importance plots (when available)\")\n",
    "print(\"   - And much more!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train models with automatic logging - no manual logging needed!\n",
    "with mlflow.start_run(run_name=\"autolog-random-forest\") as run:\n",
    "    # Just your regular ML code - MLflow automatically logs everything!\n",
    "    model = RandomForestClassifier(\n",
    "        n_estimators=100, \n",
    "        max_depth=10, \n",
    "        min_samples_split=5,\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    # This single line triggers automatic logging of everything!\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Make predictions (also automatically logged)\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    print(f\"‚úÖ Random Forest trained with automatic logging!\")\n",
    "    print(f\"   Check the MLflow UI to see all the automatically logged information!\")\n",
    "    print(f\"   Run ID: {run.info.run_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare with Logistic Regression using autolog\n",
    "with mlflow.start_run(run_name=\"autolog-logistic-regression\") as run:\n",
    "    # Different algorithm, same automatic logging magic!\n",
    "    model = LogisticRegression(\n",
    "        C=1.0,\n",
    "        solver='liblinear',\n",
    "        random_state=42,\n",
    "        max_iter=1000\n",
    "    )\n",
    "    \n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    print(f\"‚úÖ Logistic Regression trained with automatic logging!\")\n",
    "    print(f\"   Run ID: {run.info.run_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can still add manual logging on top of automatic logging\n",
    "with mlflow.start_run(run_name=\"autolog-plus-manual\") as run:\n",
    "    # Automatic logging handles the basics\n",
    "    model = RandomForestClassifier(n_estimators=150, random_state=42)\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Add your custom tags and metrics\n",
    "    mlflow.set_tags({\n",
    "        \"custom_experiment\": \"autolog_demo\",\n",
    "        \"notes\": \"Combining autolog with manual logging\"\n",
    "    })\n",
    "    \n",
    "    # Add custom metrics\n",
    "    y_pred = model.predict(X_test)\n",
    "    custom_score = accuracy_score(y_test, y_pred) * 100  # Percentage\n",
    "    mlflow.log_metric(\"accuracy_percentage\", custom_score)\n",
    "    \n",
    "    print(f\"‚úÖ Combined automatic + manual logging completed!\")\n",
    "    print(f\"   Automatic: All sklearn parameters, metrics, and model\")\n",
    "    print(f\"   Manual: Custom tags and percentage accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Disable automatic logging when you want full control\n",
    "mlflow.sklearn.autolog(disable=True)\n",
    "print(\"‚úÖ Automatic logging disabled\")\n",
    "print(\"   Future sklearn models will require manual logging\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**üîç Verification Step:**\n",
    "1. Go to your MLflow UI and compare the autolog runs with manual logging runs\n",
    "2. Notice how autolog captured many more parameters automatically\n",
    "3. Check if feature importance plots were automatically generated\n",
    "4. Compare the amount of information captured with minimal code\n",
    "\n",
    "---\n",
    "\n",
    "# üéØ Workshop Summary and Best Practices\n",
    "\n",
    "Congratulations! You've successfully practiced all core MLflow constructs. Let's summarize what you've learned:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get summary of your experiment\n",
    "experiment = mlflow.get_experiment_by_name(\"mlflow-constructs-workshop\")\n",
    "client = MlflowClient()\n",
    "\n",
    "# Get all runs in the experiment\n",
    "runs = client.search_runs(\n",
    "    experiment_ids=[experiment.experiment_id],\n",
    "    order_by=[\"start_time DESC\"]\n",
    ")\n",
    "\n",
    "print(f\"üéâ Workshop Summary\")\n",
    "print(f\"==================\")\n",
    "print(f\"Experiment: {experiment.name}\")\n",
    "print(f\"Total Runs: {len(runs)}\")\n",
    "print(f\"Experiment ID: {experiment.experiment_id}\")\n",
    "\n",
    "print(f\"\\nüìä Runs Created:\")\n",
    "for i, run in enumerate(runs[:10], 1):  # Show first 10 runs\n",
    "    run_name = run.data.tags.get('mlflow.runName', 'Unnamed')\n",
    "    status = run.info.status\n",
    "    print(f\"   {i}. {run_name} - {status}\")\n",
    "\n",
    "if len(runs) > 10:\n",
    "    print(f\"   ... and {len(runs) - 10} more runs\")\n",
    "\n",
    "print(f\"\\n‚úÖ MLflow Constructs Mastered:\")\n",
    "print(f\"   1. ‚úÖ Experiments - Project organization\")\n",
    "print(f\"   2. ‚úÖ Runs - Individual training sessions\")\n",
    "print(f\"   3. ‚úÖ Parameters - Input configurations\")\n",
    "print(f\"   4. ‚úÖ Metrics - Performance measurements\")\n",
    "print(f\"   5. ‚úÖ Artifacts - Files and models\")\n",
    "print(f\"   6. ‚úÖ Model Registry - Version control\")\n",
    "print(f\"   7. ‚úÖ Tags - Organization and filtering\")\n",
    "print(f\"   8. ‚úÖ Automatic Logging - Effortless tracking\")\n",
    "\n",
    "print(f\"\\nüéØ Key Best Practices Learned:\")\n",
    "print(f\"   ‚Ä¢ Use descriptive experiment and run names\")\n",
    "print(f\"   ‚Ä¢ Always use 'with mlflow.start_run():' context\")\n",
    "print(f\"   ‚Ä¢ Log parameters before training, metrics after\")\n",
    "print(f\"   ‚Ä¢ Include artifacts for complete experiment story\")\n",
    "print(f\"   ‚Ä¢ Use tags for organization and filtering\")\n",
    "print(f\"   ‚Ä¢ Leverage autolog for rapid experimentation\")\n",
    "print(f\"   ‚Ä¢ Combine autolog with manual logging when needed\")\n",
    "\n",
    "print(f\"\\nüöÄ Next Steps:\")\n",
    "print(f\"   ‚Ä¢ Explore the MLflow UI to review all your experiments\")\n",
    "print(f\"   ‚Ä¢ Try the model registry features for deployment\")\n",
    "print(f\"   ‚Ä¢ Experiment with different ML libraries and autolog\")\n",
    "print(f\"   ‚Ä¢ Apply these concepts to your real ML projects\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîó Useful Resources\n",
    "\n",
    "- **MLflow Documentation**: https://mlflow.org/docs/latest/\n",
    "- **MLflow Tracking**: https://mlflow.org/docs/latest/tracking.html\n",
    "- **MLflow Models**: https://mlflow.org/docs/latest/models.html\n",
    "- **MLflow Model Registry**: https://mlflow.org/docs/latest/model-registry.html\n",
    "- **Automatic Logging**: https://mlflow.org/docs/latest/tracking.html#automatic-logging\n",
    "\n",
    "## üéì Workshop Complete!\n",
    "\n",
    "You have successfully completed the MLflow Constructs hands-on workshop! You now have practical experience with all core MLflow concepts and are ready to apply them to your machine learning projects.\n",
    "\n",
    "**Remember**: The key to mastering MLflow is consistent practice. Start incorporating these constructs into your daily ML workflow, and you'll soon see the benefits of organized, reproducible, and collaborative machine learning experiments."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}