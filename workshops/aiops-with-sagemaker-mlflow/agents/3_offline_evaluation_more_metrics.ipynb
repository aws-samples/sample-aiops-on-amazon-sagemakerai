{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agent offline Evalution with MLflow \n",
    "\n",
    "Offline evaluation is a critical methodology used to assess the performance of AI agents using historical groundtruth data without the need for live deployment. This approach serves multiple essential purposes in the development and deployment lifecycle of AI systems. By testing agent behavior in a controlled environment, users can mitigate risks, avoid costly mistakes, and protect against potential negative impacts in production environments.\n",
    "\n",
    "MLflow provides comprehensive tools and functionalities for experiment tracking, allowing developers to log metrics, parameters, and artifacts while comparing different agent versions. This tutorial we will introduce you how to do offline evaluation on MLflow. We will use the same example we showed in the previous sections\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reuse the LangGraph Agents for ETL error resolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1.0\n",
      "3.0.0\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import os\n",
    "\n",
    "from data.data import log_data_set, log_data\n",
    "from data.solution_book import knowledge_base\n",
    "\n",
    "import sagemaker_mlflow\n",
    "import mlflow\n",
    "\n",
    "print(sagemaker_mlflow.__version__)\n",
    "print(mlflow.__version__)\n",
    "\n",
    "tracking_server_arn = \"\" \n",
    "experiment_name = \"agent-mlflow-demo\"\n",
    "mlflow.set_tracking_uri(tracking_server_arn) \n",
    "mlflow.set_experiment(experiment_name)\n",
    "\n",
    "mlflow.langchain.autolog()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LangGraph Agent implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.prebuilt import create_react_agent\n",
    "from langchain.chat_models import init_chat_model\n",
    "from langchain_core.tools import tool\n",
    "\n",
    "@tool \n",
    "def log_identifier(ticket_id: str) -> str:\n",
    "    \"\"\"Get error type from ticket number\n",
    "\n",
    "    Args:\n",
    "        ticket_id: ticket id\n",
    "\n",
    "    Returns:\n",
    "        an error type\n",
    "\n",
    "    \"\"\"\n",
    "    if ticket_id not in log_data_set:\n",
    "        return \"ticket id not found in the database\"\n",
    "    \n",
    "    for item in log_data:\n",
    "        if item[\"id\"] == ticket_id:\n",
    "            return item['error_name']\n",
    "\n",
    "@tool(return_direct=True)\n",
    "def information_retriever(error_type: str) -> str:\n",
    "    \"\"\"Retriever error solution based on error type\n",
    "\n",
    "    Args:\n",
    "        error_type: user input error type\n",
    "    \n",
    "    Returns:\n",
    "        a str of steps \n",
    "    \"\"\"\n",
    "\n",
    "    if error_type not in knowledge_base.keys():\n",
    "        return \"error type not found in the knowledge base, please use your own knowledge\"\n",
    "    \n",
    "    return knowledge_base[error_type]\n",
    "\n",
    "llm = init_chat_model(\n",
    "    model= \"us.anthropic.claude-3-5-haiku-20241022-v1:0\",\n",
    "    model_provider=\"bedrock_converse\",\n",
    ")\n",
    "\n",
    "system_prompt = \"\"\"\n",
    "You are an expert a resolving ETL errors. You are equiped with two tools: \n",
    "1. log_identifier: Get error type from ticket number\n",
    "2. information_retriever: Retriever error solution based on error type\n",
    "\n",
    "You will use the ticket ID to gather information about the error using the log_identifier tool. \n",
    "Then you should search the database for information on how to resolve the error using the information_retriever tool\n",
    "\n",
    "Return ONLY the numbered steps without any introduction or conclusion. Format as:\n",
    "1. step 1 text\n",
    "2. step 2 text\n",
    "...\n",
    "\"\"\"\n",
    "\n",
    "agent = create_react_agent(\n",
    "    model=llm,\n",
    "    tools= [log_identifier, information_retriever], \n",
    "    prompt=system_prompt\n",
    ")\n",
    "\n",
    "def get_langGraph_agent_response(ticket_id = 'TICKET-001'):\n",
    "    # Prepare input for the agent\n",
    "    agent_input = {\"messages\": [{\"role\": \"user\", \"content\": ticket_id}]}\n",
    "    response = agent.invoke(agent_input)\n",
    "    return response \n",
    "\n",
    "\n",
    "#  Here we add one more function to retrieve all the tool calling information (tool names and their corresponding arguments)\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "\n",
    "def extract_tool_calls(response):\n",
    "    '''\n",
    "    extract tool calling information (tool names and their corresponding arguments)\n",
    "    '''\n",
    "    tool_calls_info = []\n",
    "    \n",
    "    for message in response['messages']:\n",
    "        if isinstance(message, AIMessage):\n",
    "            if isinstance(message.content, list):\n",
    "                for content_item in message.content:\n",
    "                    if content_item.get('type') == 'tool_use':\n",
    "                        function_name = content_item['name']\n",
    "                        arguments = content_item['input']\n",
    "                        tool_id = content_item['id']\n",
    "                        tool_calls_info.append({\n",
    "                            'name': function_name,\n",
    "                            'arguments': arguments                        })\n",
    "    \n",
    "    return tool_calls_info "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now Lets introduce the test cases we have created, we have created four test cases, test_case_1, test_case_2, test_case_3 and test_case_4. Below we showed two examples for test_case_1 and test_case_2\n",
    "\n",
    "Each test case is a dictionary contains separates feilds: \n",
    "- ticket_id: ticket id (this is the input to the agent)\n",
    "- error_name: this is expected error name based on the ticekt id \n",
    "- solution: this is expected solution steps \n",
    "- expected_tools: expected tools to use, store in python list and order matters\n",
    "- expected_arguments: expected arguements for the above tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data.test_cases_mlflow import test_case_1, test_case_2, test_case_3, test_case_4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ticket_id': 'TICKET-001',\n",
       " 'error_name': 'Connection Timeout',\n",
       " 'solution': ['1. Checked network connectivity between client and server',\n",
       "  '2. Verified server status and accessibility',\n",
       "  '3. Increased connection timeout settings',\n",
       "  '4. Checked firewall rules',\n",
       "  '5. Monitored network latency'],\n",
       " 'expected_tools': ['log_identifier', 'information_retriever'],\n",
       " 'expected_arguments': [{'ticket_id': 'TICKET-001'},\n",
       "  {'error_type': 'Connection Timeout'}]}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_case_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will invoke agent for each of the test cases, we will store them into a dataframe, this dataframe will be used in evaluation. The dataframe will contains: \n",
    "1. user_input: this is the input ticket number \n",
    "2. actual_output: Agent generated output \n",
    "3. expected_output: ground truth output \n",
    "4. tool_uses: Actual tool calling information contains both tools name and tools arguments\n",
    "5. expected_tools: ground truth tools to call \n",
    "6. expected_arguments: ground truth tools arguments\n",
    "\n",
    "### Create Evaluation Dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/09/25 02:02:13 WARNING mlflow.utils.autologging_utils: Encountered unexpected error during autologging: 2 validation errors for ChatMessage\n",
      "content.str\n",
      "  Input should be a valid string [type=string_type, input_value=[{'type': 'text', 'text':...P50vCq5TA2z9cY2xXTnXg'}], input_type=list]\n",
      "    For further information visit https://errors.pydantic.dev/2.11/v/string_type\n",
      "content.list[tagged-union[TextContentPart,ImageContentPart,AudioContentPart]].1\n",
      "  Input tag 'tool_use' found using 'type' does not match any of the expected tags: 'text', 'image_url', 'input_audio' [type=union_tag_invalid, input_value={'type': 'tool_use', 'nam...KP50vCq5TA2z9cY2xXTnXg'}, input_type=dict]\n",
      "    For further information visit https://errors.pydantic.dev/2.11/v/union_tag_invalid\n",
      "2025/09/25 02:02:16 WARNING mlflow.utils.autologging_utils: Encountered unexpected error during autologging: 2 validation errors for ChatMessage\n",
      "content.str\n",
      "  Input should be a valid string [type=string_type, input_value=[{'type': 'tool_use', 'na...t_tTpOmTvWT3qBSOKWJYQ'}], input_type=list]\n",
      "    For further information visit https://errors.pydantic.dev/2.11/v/string_type\n",
      "content.list[tagged-union[TextContentPart,ImageContentPart,AudioContentPart]].0\n",
      "  Input tag 'tool_use' found using 'type' does not match any of the expected tags: 'text', 'image_url', 'input_audio' [type=union_tag_invalid, input_value={'type': 'tool_use', 'nam...et_tTpOmTvWT3qBSOKWJYQ'}, input_type=dict]\n",
      "    For further information visit https://errors.pydantic.dev/2.11/v/union_tag_invalid\n",
      "2025/09/25 02:02:19 WARNING mlflow.utils.autologging_utils: Encountered unexpected error during autologging: 2 validation errors for ChatMessage\n",
      "content.str\n",
      "  Input should be a valid string [type=string_type, input_value=[{'type': 'text', 'text':...EoAKqAmTvCZpSxZ9Xlbig'}], input_type=list]\n",
      "    For further information visit https://errors.pydantic.dev/2.11/v/string_type\n",
      "content.list[tagged-union[TextContentPart,ImageContentPart,AudioContentPart]].1\n",
      "  Input tag 'tool_use' found using 'type' does not match any of the expected tags: 'text', 'image_url', 'input_audio' [type=union_tag_invalid, input_value={'type': 'tool_use', 'nam...5EoAKqAmTvCZpSxZ9Xlbig'}, input_type=dict]\n",
      "    For further information visit https://errors.pydantic.dev/2.11/v/union_tag_invalid\n",
      "2025/09/25 02:02:23 WARNING mlflow.utils.autologging_utils: Encountered unexpected error during autologging: 2 validation errors for ChatMessage\n",
      "content.str\n",
      "  Input should be a valid string [type=string_type, input_value=[{'type': 'text', 'text':...Dkr4XPkRZylURttEn4UEQ'}], input_type=list]\n",
      "    For further information visit https://errors.pydantic.dev/2.11/v/string_type\n",
      "content.list[tagged-union[TextContentPart,ImageContentPart,AudioContentPart]].1\n",
      "  Input tag 'tool_use' found using 'type' does not match any of the expected tags: 'text', 'image_url', 'input_audio' [type=union_tag_invalid, input_value={'type': 'tool_use', 'nam...cDkr4XPkRZylURttEn4UEQ'}, input_type=dict]\n",
      "    For further information visit https://errors.pydantic.dev/2.11/v/union_tag_invalid\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_input</th>\n",
       "      <th>actual_output</th>\n",
       "      <th>expected_output</th>\n",
       "      <th>tool_uses</th>\n",
       "      <th>expected_tools</th>\n",
       "      <th>expected_arguments</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TICKET-001</td>\n",
       "      <td>1. Check network connectivity between client a...</td>\n",
       "      <td>1. Checked network connectivity between client...</td>\n",
       "      <td>[{'name': 'log_identifier', 'arguments': {'tic...</td>\n",
       "      <td>[log_identifier, information_retriever]</td>\n",
       "      <td>[{'ticket_id': 'TICKET-001'}, {'error_type': '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TICKET-002</td>\n",
       "      <td>1. Verify database credentials are correct\\n2....</td>\n",
       "      <td>1. Verified database credentials\\n2. Checked u...</td>\n",
       "      <td>[{'name': 'log_identifier', 'arguments': {'tic...</td>\n",
       "      <td>[log_identifier, information_retriever]</td>\n",
       "      <td>[{'ticket_id': 'TICKET-002'}, {'error_type': '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TICKET-006</td>\n",
       "      <td>1. Remove temporary and unnecessary files\\n2. ...</td>\n",
       "      <td>1. Removed temporary and unnecessary files\\n2....</td>\n",
       "      <td>[{'name': 'log_identifier', 'arguments': {'tic...</td>\n",
       "      <td>[log_identifier, information_retriever]</td>\n",
       "      <td>[{'ticket_id': 'TICKET-006'}, {'error_type': '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TICKET-008</td>\n",
       "      <td>1. Review user access rights\\n2. Check file an...</td>\n",
       "      <td>1. Reviewed user access rights\\n2. Checked fil...</td>\n",
       "      <td>[{'name': 'log_identifier', 'arguments': {'tic...</td>\n",
       "      <td>[log_identifier, information_retriever, notify...</td>\n",
       "      <td>[{'ticket_id': 'TICKET-008'}, {'error_type': '...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_input                                      actual_output  \\\n",
       "0  TICKET-001  1. Check network connectivity between client a...   \n",
       "1  TICKET-002  1. Verify database credentials are correct\\n2....   \n",
       "2  TICKET-006  1. Remove temporary and unnecessary files\\n2. ...   \n",
       "3  TICKET-008  1. Review user access rights\\n2. Check file an...   \n",
       "\n",
       "                                     expected_output  \\\n",
       "0  1. Checked network connectivity between client...   \n",
       "1  1. Verified database credentials\\n2. Checked u...   \n",
       "2  1. Removed temporary and unnecessary files\\n2....   \n",
       "3  1. Reviewed user access rights\\n2. Checked fil...   \n",
       "\n",
       "                                           tool_uses  \\\n",
       "0  [{'name': 'log_identifier', 'arguments': {'tic...   \n",
       "1  [{'name': 'log_identifier', 'arguments': {'tic...   \n",
       "2  [{'name': 'log_identifier', 'arguments': {'tic...   \n",
       "3  [{'name': 'log_identifier', 'arguments': {'tic...   \n",
       "\n",
       "                                      expected_tools  \\\n",
       "0            [log_identifier, information_retriever]   \n",
       "1            [log_identifier, information_retriever]   \n",
       "2            [log_identifier, information_retriever]   \n",
       "3  [log_identifier, information_retriever, notify...   \n",
       "\n",
       "                                  expected_arguments  \n",
       "0  [{'ticket_id': 'TICKET-001'}, {'error_type': '...  \n",
       "1  [{'ticket_id': 'TICKET-002'}, {'error_type': '...  \n",
       "2  [{'ticket_id': 'TICKET-006'}, {'error_type': '...  \n",
       "3  [{'ticket_id': 'TICKET-008'}, {'error_type': '...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "test_cases = [test_case_1, test_case_2, test_case_3, test_case_4]\n",
    "\n",
    "result = [] \n",
    "\n",
    "for test_case in test_cases:\n",
    "\n",
    "    user_input = test_case['ticket_id']\n",
    "    error_name = test_case['error_name']\n",
    "    error_solution = \"\\n\".join(test_case['solution'])\n",
    "    expected_tools = test_case['expected_tools']\n",
    "    expected_arguments = test_case['expected_arguments']\n",
    "\n",
    "    agent_response = get_langGraph_agent_response(user_input)\n",
    "    actual_output = agent_response['messages'][-1].content\n",
    "    tool_uses = extract_tool_calls(agent_response)\n",
    "\n",
    "    result.append({\n",
    "        'user_input': user_input, \n",
    "        'actual_output': actual_output,\n",
    "        'expected_output': error_solution,\n",
    "        'tool_uses': tool_uses, \n",
    "        'expected_tools': expected_tools,\n",
    "        'expected_arguments': expected_arguments\n",
    "    })\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "eval_df = pd.DataFrame(result)\n",
    "eval_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_input</th>\n",
       "      <th>actual_output</th>\n",
       "      <th>expected_output</th>\n",
       "      <th>tool_uses</th>\n",
       "      <th>expected_tools</th>\n",
       "      <th>expected_arguments</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TICKET-001</td>\n",
       "      <td>1. Check network connectivity between client a...</td>\n",
       "      <td>1. Checked network connectivity between client...</td>\n",
       "      <td>[{'name': 'log_identifier', 'arguments': {'tic...</td>\n",
       "      <td>[log_identifier, information_retriever]</td>\n",
       "      <td>[{'ticket_id': 'TICKET-001'}, {'error_type': '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TICKET-002</td>\n",
       "      <td>1. Verify database credentials are correct\\n2....</td>\n",
       "      <td>1. Verified database credentials\\n2. Checked u...</td>\n",
       "      <td>[{'name': 'log_identifier', 'arguments': {'tic...</td>\n",
       "      <td>[log_identifier, information_retriever]</td>\n",
       "      <td>[{'ticket_id': 'TICKET-002'}, {'error_type': '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TICKET-006</td>\n",
       "      <td>1. Remove temporary and unnecessary files\\n2. ...</td>\n",
       "      <td>1. Removed temporary and unnecessary files\\n2....</td>\n",
       "      <td>[{'name': 'log_identifier', 'arguments': {'tic...</td>\n",
       "      <td>[log_identifier, information_retriever]</td>\n",
       "      <td>[{'ticket_id': 'TICKET-006'}, {'error_type': '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TICKET-008</td>\n",
       "      <td>1. Review user access rights\\n2. Check file an...</td>\n",
       "      <td>1. Reviewed user access rights\\n2. Checked fil...</td>\n",
       "      <td>[{'name': 'log_identifier', 'arguments': {'tic...</td>\n",
       "      <td>[log_identifier, information_retriever, notify...</td>\n",
       "      <td>[{'ticket_id': 'TICKET-008'}, {'error_type': '...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_input                                      actual_output  \\\n",
       "0  TICKET-001  1. Check network connectivity between client a...   \n",
       "1  TICKET-002  1. Verify database credentials are correct\\n2....   \n",
       "2  TICKET-006  1. Remove temporary and unnecessary files\\n2. ...   \n",
       "3  TICKET-008  1. Review user access rights\\n2. Check file an...   \n",
       "\n",
       "                                     expected_output  \\\n",
       "0  1. Checked network connectivity between client...   \n",
       "1  1. Verified database credentials\\n2. Checked u...   \n",
       "2  1. Removed temporary and unnecessary files\\n2....   \n",
       "3  1. Reviewed user access rights\\n2. Checked fil...   \n",
       "\n",
       "                                           tool_uses  \\\n",
       "0  [{'name': 'log_identifier', 'arguments': {'tic...   \n",
       "1  [{'name': 'log_identifier', 'arguments': {'tic...   \n",
       "2  [{'name': 'log_identifier', 'arguments': {'tic...   \n",
       "3  [{'name': 'log_identifier', 'arguments': {'tic...   \n",
       "\n",
       "                                      expected_tools  \\\n",
       "0            [log_identifier, information_retriever]   \n",
       "1            [log_identifier, information_retriever]   \n",
       "2            [log_identifier, information_retriever]   \n",
       "3  [log_identifier, information_retriever, notify...   \n",
       "\n",
       "                                  expected_arguments  \n",
       "0  [{'ticket_id': 'TICKET-001'}, {'error_type': '...  \n",
       "1  [{'ticket_id': 'TICKET-002'}, {'error_type': '...  \n",
       "2  [{'ticket_id': 'TICKET-006'}, {'error_type': '...  \n",
       "3  [{'ticket_id': 'TICKET-008'}, {'error_type': '...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we will combine **user_input**, **actual_output** and **tool_uses** into predictions, and combine **expected_output**, **expected_tools** and **expected_arguments** into targets. The reason for combination is when you create evaluation dataset for MLflow, you will need to specify the `prediction` and `targts`. The MLlfow doesn't accept multiple columns other than those. We will cover more on the later sections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_input</th>\n",
       "      <th>actual_output</th>\n",
       "      <th>expected_output</th>\n",
       "      <th>tool_uses</th>\n",
       "      <th>expected_tools</th>\n",
       "      <th>expected_arguments</th>\n",
       "      <th>predictions</th>\n",
       "      <th>targets</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TICKET-001</td>\n",
       "      <td>1. Check network connectivity between client a...</td>\n",
       "      <td>1. Checked network connectivity between client...</td>\n",
       "      <td>[{'name': 'log_identifier', 'arguments': {'tic...</td>\n",
       "      <td>[log_identifier, information_retriever]</td>\n",
       "      <td>[{'ticket_id': 'TICKET-001'}, {'error_type': '...</td>\n",
       "      <td>{'user_input': 'TICKET-001', 'actual_output': ...</td>\n",
       "      <td>{'expected_output': '1. Checked network connec...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TICKET-002</td>\n",
       "      <td>1. Verify database credentials are correct\\n2....</td>\n",
       "      <td>1. Verified database credentials\\n2. Checked u...</td>\n",
       "      <td>[{'name': 'log_identifier', 'arguments': {'tic...</td>\n",
       "      <td>[log_identifier, information_retriever]</td>\n",
       "      <td>[{'ticket_id': 'TICKET-002'}, {'error_type': '...</td>\n",
       "      <td>{'user_input': 'TICKET-002', 'actual_output': ...</td>\n",
       "      <td>{'expected_output': '1. Verified database cred...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TICKET-006</td>\n",
       "      <td>1. Remove temporary and unnecessary files\\n2. ...</td>\n",
       "      <td>1. Removed temporary and unnecessary files\\n2....</td>\n",
       "      <td>[{'name': 'log_identifier', 'arguments': {'tic...</td>\n",
       "      <td>[log_identifier, information_retriever]</td>\n",
       "      <td>[{'ticket_id': 'TICKET-006'}, {'error_type': '...</td>\n",
       "      <td>{'user_input': 'TICKET-006', 'actual_output': ...</td>\n",
       "      <td>{'expected_output': '1. Removed temporary and ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TICKET-008</td>\n",
       "      <td>1. Review user access rights\\n2. Check file an...</td>\n",
       "      <td>1. Reviewed user access rights\\n2. Checked fil...</td>\n",
       "      <td>[{'name': 'log_identifier', 'arguments': {'tic...</td>\n",
       "      <td>[log_identifier, information_retriever, notify...</td>\n",
       "      <td>[{'ticket_id': 'TICKET-008'}, {'error_type': '...</td>\n",
       "      <td>{'user_input': 'TICKET-008', 'actual_output': ...</td>\n",
       "      <td>{'expected_output': '1. Reviewed user access r...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_input                                      actual_output  \\\n",
       "0  TICKET-001  1. Check network connectivity between client a...   \n",
       "1  TICKET-002  1. Verify database credentials are correct\\n2....   \n",
       "2  TICKET-006  1. Remove temporary and unnecessary files\\n2. ...   \n",
       "3  TICKET-008  1. Review user access rights\\n2. Check file an...   \n",
       "\n",
       "                                     expected_output  \\\n",
       "0  1. Checked network connectivity between client...   \n",
       "1  1. Verified database credentials\\n2. Checked u...   \n",
       "2  1. Removed temporary and unnecessary files\\n2....   \n",
       "3  1. Reviewed user access rights\\n2. Checked fil...   \n",
       "\n",
       "                                           tool_uses  \\\n",
       "0  [{'name': 'log_identifier', 'arguments': {'tic...   \n",
       "1  [{'name': 'log_identifier', 'arguments': {'tic...   \n",
       "2  [{'name': 'log_identifier', 'arguments': {'tic...   \n",
       "3  [{'name': 'log_identifier', 'arguments': {'tic...   \n",
       "\n",
       "                                      expected_tools  \\\n",
       "0            [log_identifier, information_retriever]   \n",
       "1            [log_identifier, information_retriever]   \n",
       "2            [log_identifier, information_retriever]   \n",
       "3  [log_identifier, information_retriever, notify...   \n",
       "\n",
       "                                  expected_arguments  \\\n",
       "0  [{'ticket_id': 'TICKET-001'}, {'error_type': '...   \n",
       "1  [{'ticket_id': 'TICKET-002'}, {'error_type': '...   \n",
       "2  [{'ticket_id': 'TICKET-006'}, {'error_type': '...   \n",
       "3  [{'ticket_id': 'TICKET-008'}, {'error_type': '...   \n",
       "\n",
       "                                         predictions  \\\n",
       "0  {'user_input': 'TICKET-001', 'actual_output': ...   \n",
       "1  {'user_input': 'TICKET-002', 'actual_output': ...   \n",
       "2  {'user_input': 'TICKET-006', 'actual_output': ...   \n",
       "3  {'user_input': 'TICKET-008', 'actual_output': ...   \n",
       "\n",
       "                                             targets  \n",
       "0  {'expected_output': '1. Checked network connec...  \n",
       "1  {'expected_output': '1. Verified database cred...  \n",
       "2  {'expected_output': '1. Removed temporary and ...  \n",
       "3  {'expected_output': '1. Reviewed user access r...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_df['predictions'] = eval_df.apply(lambda row: {\n",
    "    'user_input': row['user_input'],\n",
    "    'actual_output': row['actual_output'],\n",
    "    'tool_uses': row['tool_uses']\n",
    "}, axis=1)\n",
    "\n",
    "eval_df['targets'] = eval_df.apply(lambda row: {\n",
    "    'expected_output': row['expected_output'],\n",
    "    'expected_tools': row['expected_tools'],\n",
    "    'expected_arguments': row['expected_arguments'],\n",
    "}, axis=1)\n",
    "\n",
    "eval_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    {'user_input': 'TICKET-001', 'actual_output': ...\n",
       "1    {'user_input': 'TICKET-002', 'actual_output': ...\n",
       "2    {'user_input': 'TICKET-006', 'actual_output': ...\n",
       "3    {'user_input': 'TICKET-008', 'actual_output': ...\n",
       "Name: predictions, dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_df['predictions']\n",
    "#eval_df['targets']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create customize evaluation metrics \n",
    "\n",
    "We use DeepEval as our evaluation framework as DeepEval contains usefull LLM as a Judge metrics and Heuristic metrics. Lets first define the LLM as a Judge model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepeval.models.base_model import DeepEvalBaseLLM\n",
    "\n",
    "class AWSBedrock(DeepEvalBaseLLM):\n",
    "    def __init__(\n",
    "        self,\n",
    "        model\n",
    "    ):\n",
    "        self.model = model\n",
    "\n",
    "    def load_model(self):\n",
    "        return self.model\n",
    "\n",
    "    def generate(self, prompt: str) -> str:\n",
    "        chat_model = self.load_model()\n",
    "        return chat_model.invoke(prompt).content\n",
    "\n",
    "    async def a_generate(self, prompt: str) -> str:\n",
    "        chat_model = self.load_model()\n",
    "        res = await chat_model.ainvoke(prompt)\n",
    "        return res.content\n",
    "\n",
    "    def get_model_name(self):\n",
    "        return \"Custom Bedrock Model\"\n",
    "\n",
    "\n",
    "aws_bedrock = AWSBedrock(model=llm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then lets load packages for DeepEval. Note here we will use two metrics `GEval` and `ToolCorrectnessMetrics`. \n",
    "- `GEval` use LLM as a Judge, you can specify the criteria and evaluation steps. We define one for Correctness which check whether teh facts in `actual output` contradicts any facts in `expected output`\n",
    "- `ToolCorrectnessMetrics` this is to check whether your agent uses correct tools "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlflow.metrics import MetricValue \n",
    "from deepeval.metrics import GEval\n",
    "from deepeval.test_case import LLMTestCaseParams\n",
    "from deepeval.test_case import LLMTestCase\n",
    "from deepeval import evaluate\n",
    "from deepeval.metrics import ToolCorrectnessMetric\n",
    "from deepeval.test_case import ToolCall\n",
    "\n",
    "correctness_metric = GEval(\n",
    "    name=\"Correctness\",\n",
    "    criteria=\"Determine whether the actual output is factually correct based on the expected output.\",\n",
    "    evaluation_steps=[\n",
    "        \"Check whether the facts in 'actual output' contradicts any facts in 'expected output'\",\n",
    "        \"You should also heavily penalize omission of detail\",\n",
    "        \"Vague language, or contradicting OPINIONS, are OK\"\n",
    "    ],\n",
    "    evaluation_params=[LLMTestCaseParams.INPUT, LLMTestCaseParams.ACTUAL_OUTPUT, LLMTestCaseParams.EXPECTED_OUTPUT],\n",
    "    model = aws_bedrock\n",
    ")\n",
    "\n",
    "## Tool Correctness \n",
    "tool_correctness_metric = ToolCorrectnessMetric()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_tool_correctness(predictions, targets):\n",
    "    scores = [] \n",
    "\n",
    "    for prediction, target in zip(predictions, targets):\n",
    "        test_case = LLMTestCase(\n",
    "            input= prediction['user_input'],\n",
    "            actual_output=prediction['actual_output'],\n",
    "            expected_output= target['expected_output'],\n",
    "            tools_called = [ToolCall(name=i['name']) for i in prediction['tool_uses']], \n",
    "            expected_tools = [ToolCall(name=i) for i in target['expected_tools']]\n",
    "        )\n",
    "\n",
    "        result = evaluate(test_cases=[test_case], metrics=[tool_correctness_metric])\n",
    "\n",
    "        scores.append(result.test_results[0].metrics_data[0].score)\n",
    "\n",
    "    return MetricValue(scores= scores)\n",
    "\n",
    "def evaluate_correctness(predictions, targets):\n",
    "    scores = [] \n",
    "\n",
    "    for prediction, target in zip(predictions, targets):\n",
    "        test_case = LLMTestCase(\n",
    "            input= prediction['user_input'],\n",
    "            actual_output=prediction['actual_output'],\n",
    "            expected_output= target['expected_output'],\n",
    "            tools_called = [ToolCall(name=i['name']) for i in prediction['tool_uses']], \n",
    "            expected_tools = [ToolCall(name=i) for i in target['expected_tools']]\n",
    "        )\n",
    "\n",
    "        result = evaluate(test_cases=[test_case], metrics=[correctness_metric])\n",
    "\n",
    "        scores.append(result.test_results[0].metrics_data[0].score)\n",
    "\n",
    "    return MetricValue(scores= scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets integrate those tool with MLflow. As we discussed above, MLflow will only accept precitions and targets as the arguments. We create two methods for tool correctness and final answer correctness. Both methods takes predictions and targets as the arguments and returns scores for these metrics. \n",
    "\n",
    "We use `make_metrics` method to make our customer metrics to be a MLflow metrics. Notices that here we use `greater_is_better = True` This indicate the score is higher the better. In future if you have similar metrics like incorrectness, you can set `greater_is_better = False`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlflow.models import make_metric\n",
    "answer_correctness = make_metric(\n",
    "    eval_fn=evaluate_correctness, greater_is_better= True , name=\"answer_correctness\"\n",
    ")\n",
    "\n",
    "tool_answer_correctness = make_metric(\n",
    "    eval_fn=evaluate_tool_correctness, greater_is_better= True, name=\"tool_answer_correctness\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can use `mlflow.evaluate()` to see the performance of the chain on an evaluation dataset we created. This evaluation will take our two customized metrics (answer_correctness and tool_answer_correctness) into consideration. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with mlflow.start_run() as evaluation_run:\n",
    "    eval_dataset = mlflow.data.from_pandas(\n",
    "        df=eval_df,\n",
    "        name=\"eval_dataset_v1\",\n",
    "        targets=\"targets\",\n",
    "        predictions=\"predictions\",\n",
    "    )\n",
    "    mlflow.log_input(dataset=eval_dataset)\n",
    "    # Run the evaluation based on extra metrics\n",
    "    # Current active model will be automatically used\n",
    "    result = mlflow.evaluate(\n",
    "        data=eval_dataset,\n",
    "        extra_metrics=[\n",
    "            answer_correctness, \n",
    "            tool_answer_correctness\n",
    "        ]\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Offline Evaluation Results\n",
    "\n",
    "The results will be attached to your evaluation dataset frameworks with additional columns for scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"See aggregated evaluation results below: \\n{result.metrics}\")\n",
    "result.tables[\"eval_results_table\"]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
