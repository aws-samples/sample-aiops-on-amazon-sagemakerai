{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agent offline Evalution with MLflow \n",
    "\n",
    "Offline evaluation is a critical methodology used to assess the performance of AI agents using historical groundtruth data without the need for live deployment. This approach serves multiple essential purposes in the development and deployment lifecycle of AI systems. By testing agent behavior in a controlled environment, users can mitigate risks, avoid costly mistakes, and protect against potential negative impacts in production environments.\n",
    "\n",
    "MLflow provides comprehensive tools and functionalities for experiment tracking, allowing developers to log metrics, parameters, and artifacts while comparing different agent versions. This tutorial we will introduce you how to do offline evaluation on MLflow. We will use the same example we showed in the previous sections\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reuse the LangGraph Agents for ETL error resolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1.0\n",
      "3.0.0\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import os\n",
    "\n",
    "from data.data import log_data_set, log_data\n",
    "from data.solution_book import knowledge_base\n",
    "\n",
    "import sagemaker_mlflow\n",
    "import mlflow\n",
    "\n",
    "print(sagemaker_mlflow.__version__)\n",
    "print(mlflow.__version__)\n",
    "\n",
    "tracking_server_arn = \"\" \n",
    "experiment_name = \"agent-mlflow-demo\"\n",
    "mlflow.set_tracking_uri(tracking_server_arn) \n",
    "mlflow.set_experiment(experiment_name)\n",
    "\n",
    "mlflow.langchain.autolog()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LangGraph Agent implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.prebuilt import create_react_agent\n",
    "from langchain.chat_models import init_chat_model\n",
    "from langchain_core.tools import tool\n",
    "\n",
    "@tool \n",
    "def log_identifier(ticket_id: str) -> str:\n",
    "    \"\"\"Get error type from ticket number\n",
    "\n",
    "    Args:\n",
    "        ticket_id: ticket id\n",
    "\n",
    "    Returns:\n",
    "        an error type\n",
    "\n",
    "    \"\"\"\n",
    "    if ticket_id not in log_data_set:\n",
    "        return \"ticket id not found in the database\"\n",
    "    \n",
    "    for item in log_data:\n",
    "        if item[\"id\"] == ticket_id:\n",
    "            return item['error_name']\n",
    "\n",
    "@tool(return_direct=True)\n",
    "def information_retriever(error_type: str) -> str:\n",
    "    \"\"\"Retriever error solution based on error type\n",
    "\n",
    "    Args:\n",
    "        error_type: user input error type\n",
    "    \n",
    "    Returns:\n",
    "        a str of steps \n",
    "    \"\"\"\n",
    "\n",
    "    if error_type not in knowledge_base.keys():\n",
    "        return \"error type not found in the knowledge base, please use your own knowledge\"\n",
    "    \n",
    "    return knowledge_base[error_type]\n",
    "\n",
    "llm = init_chat_model(\n",
    "    model= \"us.anthropic.claude-3-5-haiku-20241022-v1:0\",\n",
    "    model_provider=\"bedrock_converse\",\n",
    ")\n",
    "\n",
    "system_prompt = \"\"\"\n",
    "You are an expert a resolving ETL errors. You are equiped with two tools: \n",
    "1. log_identifier: Get error type from ticket number\n",
    "2. information_retriever: Retriever error solution based on error type\n",
    "\n",
    "You will use the ticket ID to gather information about the error using the log_identifier tool. \n",
    "Then you should search the database for information on how to resolve the error using the information_retriever tool\n",
    "\n",
    "Return ONLY the numbered steps without any introduction or conclusion. Format as:\n",
    "1. step 1 text\n",
    "2. step 2 text\n",
    "...\n",
    "\"\"\"\n",
    "\n",
    "agent = create_react_agent(\n",
    "    model=llm,\n",
    "    tools= [log_identifier, information_retriever], \n",
    "    prompt=system_prompt\n",
    ")\n",
    "\n",
    "def get_langGraph_agent_response(ticket_id = 'TICKET-001'):\n",
    "    # Prepare input for the agent\n",
    "    agent_input = {\"messages\": [{\"role\": \"user\", \"content\": ticket_id}]}\n",
    "    response = agent.invoke(agent_input)\n",
    "    return response "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now Lets introduce the test cases we have created, we have created four test cases, test_case_1, test_case_2, test_case_3 and test_case_4. Below we showed two examples for test_case_1 and test_case_2\n",
    "\n",
    "Each test case is a dictionary contains separates feilds: \n",
    "- ticket_id: ticket id (this is the input to the agent)\n",
    "- error_name: this is expected error name based on the ticekt id \n",
    "- solution: this is expected solution steps \n",
    "- expected_tools: expected tools to use, store in python list and order matters\n",
    "- expected_arguments: expected arguements for the above tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data.test_cases_mlflow import test_case_1, test_case_2, test_case_3, test_case_4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ticket_id': 'TICKET-001',\n",
       " 'error_name': 'Connection Timeout',\n",
       " 'solution': ['1. Check network connectivity between client and server',\n",
       "  '2. Verify if the server is running and accessible',\n",
       "  '3. Increase the connection timeout settings',\n",
       "  '4. Check for firewall rules blocking the connection',\n",
       "  '5. Monitor network latency and bandwidth'],\n",
       " 'expected_tools': ['log_identifier', 'information_retriever'],\n",
       " 'expected_arguments': [{'ticket_id': 'TICKET-001'},\n",
       "  {'error_type': 'Connection Timeout'}]}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_case_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will invoke agent for each of the test cases, we will store them into a dataframe, this dataframe will be used in evaluation. The dataframe will contains: \n",
    "1. user_input: this is the input ticket number \n",
    "2. actual_output: Agent generated output \n",
    "3. expected_output: ground truth output \n",
    "\n",
    "### Create Evaluation Dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/09/25 04:59:21 WARNING mlflow.utils.autologging_utils: Encountered unexpected error during autologging: 2 validation errors for ChatMessage\n",
      "content.str\n",
      "  Input should be a valid string [type=string_type, input_value=[{'type': 'text', 'text':...9yVbKlrT9S-OXS68JRD0A'}], input_type=list]\n",
      "    For further information visit https://errors.pydantic.dev/2.11/v/string_type\n",
      "content.list[tagged-union[TextContentPart,ImageContentPart,AudioContentPart]].1\n",
      "  Input tag 'tool_use' found using 'type' does not match any of the expected tags: 'text', 'image_url', 'input_audio' [type=union_tag_invalid, input_value={'type': 'tool_use', 'nam...09yVbKlrT9S-OXS68JRD0A'}, input_type=dict]\n",
      "    For further information visit https://errors.pydantic.dev/2.11/v/union_tag_invalid\n",
      "2025/09/25 04:59:24 WARNING mlflow.utils.autologging_utils: Encountered unexpected error during autologging: 2 validation errors for ChatMessage\n",
      "content.str\n",
      "  Input should be a valid string [type=string_type, input_value=[{'type': 'tool_use', 'na...1pDfITmSnWJ88PvrM_rMw'}], input_type=list]\n",
      "    For further information visit https://errors.pydantic.dev/2.11/v/string_type\n",
      "content.list[tagged-union[TextContentPart,ImageContentPart,AudioContentPart]].0\n",
      "  Input tag 'tool_use' found using 'type' does not match any of the expected tags: 'text', 'image_url', 'input_audio' [type=union_tag_invalid, input_value={'type': 'tool_use', 'nam...P1pDfITmSnWJ88PvrM_rMw'}, input_type=dict]\n",
      "    For further information visit https://errors.pydantic.dev/2.11/v/union_tag_invalid\n",
      "2025/09/25 04:59:27 WARNING mlflow.utils.autologging_utils: Encountered unexpected error during autologging: 2 validation errors for ChatMessage\n",
      "content.str\n",
      "  Input should be a valid string [type=string_type, input_value=[{'type': 'text', 'text':...J4aBfmkQvOs-LYI-ctH4Q'}], input_type=list]\n",
      "    For further information visit https://errors.pydantic.dev/2.11/v/string_type\n",
      "content.list[tagged-union[TextContentPart,ImageContentPart,AudioContentPart]].1\n",
      "  Input tag 'tool_use' found using 'type' does not match any of the expected tags: 'text', 'image_url', 'input_audio' [type=union_tag_invalid, input_value={'type': 'tool_use', 'nam...5J4aBfmkQvOs-LYI-ctH4Q'}, input_type=dict]\n",
      "    For further information visit https://errors.pydantic.dev/2.11/v/union_tag_invalid\n",
      "2025/09/25 04:59:30 WARNING mlflow.utils.autologging_utils: Encountered unexpected error during autologging: 2 validation errors for ChatMessage\n",
      "content.str\n",
      "  Input should be a valid string [type=string_type, input_value=[{'type': 'tool_use', 'na..._aen0jgQxqfAhKEZIrwYg'}], input_type=list]\n",
      "    For further information visit https://errors.pydantic.dev/2.11/v/string_type\n",
      "content.list[tagged-union[TextContentPart,ImageContentPart,AudioContentPart]].0\n",
      "  Input tag 'tool_use' found using 'type' does not match any of the expected tags: 'text', 'image_url', 'input_audio' [type=union_tag_invalid, input_value={'type': 'tool_use', 'nam...I_aen0jgQxqfAhKEZIrwYg'}, input_type=dict]\n",
      "    For further information visit https://errors.pydantic.dev/2.11/v/union_tag_invalid\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "test_cases = [test_case_1, test_case_2, test_case_3, test_case_4]\n",
    "\n",
    "result = [] \n",
    "\n",
    "for test_case in test_cases:\n",
    "\n",
    "    user_input = test_case['ticket_id']\n",
    "    error_solution = \"\\n\".join(test_case['solution'])\n",
    "    agent_response = get_langGraph_agent_response(user_input)\n",
    "    actual_output = agent_response['messages'][-1].content\n",
    "\n",
    "    result.append({\n",
    "        'user_input': user_input, \n",
    "        'actual_output': actual_output,\n",
    "        'expected_output': error_solution\n",
    "    })\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "eval_df = pd.DataFrame(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_input</th>\n",
       "      <th>actual_output</th>\n",
       "      <th>expected_output</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TICKET-001</td>\n",
       "      <td>1. Check network connectivity between client a...</td>\n",
       "      <td>1. Check network connectivity between client a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TICKET-002</td>\n",
       "      <td>1. Verify database credentials are correct\\n2....</td>\n",
       "      <td>1. Verify database credentials are correct\\n2....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TICKET-006</td>\n",
       "      <td>1. Remove temporary and unnecessary files\\n2. ...</td>\n",
       "      <td>1. Remove temporary and unnecessary files\\n2. ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TICKET-008</td>\n",
       "      <td>1. Review user access rights\\n2. Check file an...</td>\n",
       "      <td>1. Review user access rights\\n2. Check file an...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_input                                      actual_output  \\\n",
       "0  TICKET-001  1. Check network connectivity between client a...   \n",
       "1  TICKET-002  1. Verify database credentials are correct\\n2....   \n",
       "2  TICKET-006  1. Remove temporary and unnecessary files\\n2. ...   \n",
       "3  TICKET-008  1. Review user access rights\\n2. Check file an...   \n",
       "\n",
       "                                     expected_output  \n",
       "0  1. Check network connectivity between client a...  \n",
       "1  1. Verify database credentials are correct\\n2....  \n",
       "2  1. Remove temporary and unnecessary files\\n2. ...  \n",
       "3  1. Review user access rights\\n2. Check file an...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create evaluation metrics for MLflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While MLflow has some built-in [Heuristic](https://mlflow.org/docs/2.21.3/llms/llm-evaluate/#heuristic-based-metrics) and [LLM as a Judge metrics](https://mlflow.org/docs/2.21.3/llms/llm-evaluate/#llm-as-a-judge-metrics), those metrics are limited and may not be able to fit all situations. This section we will show you how to build your own custom metrics.\n",
    "\n",
    "1. You need to define your custom metric method with two arguments `predictions` and `targets`. \"predictions\" is a list of predicted results from you agent and \"targets\" is a list of ground truth. You can define any custom certeria for judgement. Here we design it to be exact match \n",
    "2. Use `make_metrics` method to make our customer metrics to be a MLflow metrics. Notices that here we use `greater_is_better = True` This indicate the score is higher the better. In future if you have similar metrics like incorrectness, you can set `greater_is_better = False`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlflow.metrics import MetricValue \n",
    "from mlflow.models import make_metric\n",
    "\n",
    "def custom_correctness(predictions, targets):\n",
    "    '''\n",
    "    custom correctness based on exact match \n",
    "\n",
    "    Args:\n",
    "        precitions: list of agent predicted values \n",
    "        targets: list of ground truth values \n",
    "\n",
    "    Returns: \n",
    "        A list of MetricValue scores\n",
    "    '''\n",
    "    scores = [] \n",
    "\n",
    "    for prediction, target in zip(predictions, targets):\n",
    "        if prediction.strip() == target.strip():\n",
    "            scores.append(1)\n",
    "        else:\n",
    "            scores.append(0)\n",
    "\n",
    "    return MetricValue(scores= scores)\n",
    "\n",
    "custom_correctness_metric = make_metric(\n",
    "    eval_fn=custom_correctness, greater_is_better= True , name=\"custom_answer_correctness\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run offline evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can use `mlflow.evaluate()` to see the performance of the chain on an evaluation dataset we created. we will use the `custom_correctness_metric` we created\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/09/25 05:00:08 INFO mlflow.models.evaluation.default_evaluator: Testing metrics on first row...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🏃 View run marvelous-cat-105 at: https://us-east-1.experiments.sagemaker.aws/#/experiments/1/runs/ab98516b896a42bea5949c86ddd49323\n",
      "🧪 View experiment at: https://us-east-1.experiments.sagemaker.aws/#/experiments/1\n"
     ]
    }
   ],
   "source": [
    "with mlflow.start_run() as evaluation_run:\n",
    "    eval_dataset = mlflow.data.from_pandas(\n",
    "        df=eval_df,\n",
    "        name=\"eval_dataset\",\n",
    "        targets=\"expected_output\",\n",
    "        predictions=\"actual_output\",\n",
    "    )\n",
    "    mlflow.log_input(dataset=eval_dataset)\n",
    "\n",
    "\n",
    "    result = mlflow.evaluate(\n",
    "        data=eval_dataset,\n",
    "        extra_metrics=[\n",
    "            custom_correctness_metric\n",
    "        ]\n",
    "    )   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Offline Evaluation Results\n",
    "\n",
    "The results will be attached to your evaluation dataset frameworks with additional columns for scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "See aggregated evaluation results below: \n",
      "{'custom_answer_correctness/mean': 1.0, 'custom_answer_correctness/variance': 0.0, 'custom_answer_correctness/p90': 1.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7679f5acb9f9465bade908f6462d11cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading artifacts:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_input</th>\n",
       "      <th>expected_output</th>\n",
       "      <th>actual_output</th>\n",
       "      <th>custom_answer_correctness/score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TICKET-001</td>\n",
       "      <td>1. Check network connectivity between client a...</td>\n",
       "      <td>1. Check network connectivity between client a...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TICKET-002</td>\n",
       "      <td>1. Verify database credentials are correct\\n2....</td>\n",
       "      <td>1. Verify database credentials are correct\\n2....</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TICKET-006</td>\n",
       "      <td>1. Remove temporary and unnecessary files\\n2. ...</td>\n",
       "      <td>1. Remove temporary and unnecessary files\\n2. ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TICKET-008</td>\n",
       "      <td>1. Review user access rights\\n2. Check file an...</td>\n",
       "      <td>1. Review user access rights\\n2. Check file an...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_input                                    expected_output  \\\n",
       "0  TICKET-001  1. Check network connectivity between client a...   \n",
       "1  TICKET-002  1. Verify database credentials are correct\\n2....   \n",
       "2  TICKET-006  1. Remove temporary and unnecessary files\\n2. ...   \n",
       "3  TICKET-008  1. Review user access rights\\n2. Check file an...   \n",
       "\n",
       "                                       actual_output  \\\n",
       "0  1. Check network connectivity between client a...   \n",
       "1  1. Verify database credentials are correct\\n2....   \n",
       "2  1. Remove temporary and unnecessary files\\n2. ...   \n",
       "3  1. Review user access rights\\n2. Check file an...   \n",
       "\n",
       "   custom_answer_correctness/score  \n",
       "0                                1  \n",
       "1                                1  \n",
       "2                                1  \n",
       "3                                1  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f\"See aggregated evaluation results below: \\n{result.metrics}\")\n",
    "result.tables[\"eval_results_table\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLflow GenAI metrics with Bedrock"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MLflow also have some built-in [Heuristic](https://mlflow.org/docs/2.21.3/llms/llm-evaluate/#heuristic-based-metrics) and [LLM as a Judge metrics](https://mlflow.org/docs/2.21.3/llms/llm-evaluate/#llm-as-a-judge-metrics). Here we show one example you can use for answer_correctness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"AWS_REGION\"] = \"<your-aws-region>\"\n",
    "\n",
    "# Option 1. Role-based authentication\n",
    "os.environ[\"AWS_ROLE_ARN\"] = \"<your-aws-role-arn>\"\n",
    "\n",
    "# Option 2. API key-based authentication\n",
    "os.environ[\"AWS_ACCESS_KEY_ID\"] = \"<your-aws-access-key-id>\"\n",
    "os.environ[\"AWS_SECRET_ACCESS_KEY\"] = \"<your-aws-secret-access-key>\"\n",
    "\n",
    "# You can also use session token for temporary credentials.\n",
    "# os.environ[\"AWS_SESSION_TOKEN\"] = \"<your-aws-session-token>\"\n",
    "\n",
    "answer_correctness = mlflow.metrics.genai.answer_correctness(\n",
    "    model=\"bedrock:/anthropic.claude-3-5-sonnet-20241022-v2:0\",\n",
    "    parameters={\n",
    "        \"temperature\": 0,\n",
    "        \"max_tokens\": 256,\n",
    "        \"anthropic_version\": \"bedrock-2023-05-31\",\n",
    "    },\n",
    ")\n",
    "\n",
    "with mlflow.start_run() as evaluation_run:\n",
    "    eval_dataset = mlflow.data.from_pandas(\n",
    "        df=eval_df,\n",
    "        name=\"eval_dataset_v3\",\n",
    "        targets=\"expected_output\",\n",
    "        predictions=\"actual_output\",\n",
    "    )\n",
    "    mlflow.log_input(dataset=eval_dataset)\n",
    "\n",
    "\n",
    "    result = mlflow.evaluate(\n",
    "        data=eval_dataset,\n",
    "        extra_metrics=[\n",
    "            custom_correctness_metric\n",
    "        ],\n",
    "        evaluator_config={\"col_mapping\": {\"inputs\": \"user_input\"}}\n",
    "    )\n",
    "\n",
    "print(f\"See aggregated evaluation results below: \\n{result.metrics}\")\n",
    "result.tables[\"eval_results_table\"]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
