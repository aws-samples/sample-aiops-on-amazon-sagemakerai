{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6a735064-5587-4c83-88e8-63ae530067fa",
   "metadata": {},
   "source": [
    "# SageMaker AI Inference Endpoint LLM evaluation using SageMaker AI MLflow App\n",
    "In this notebook, you will use **Amazon SageMaker AI MLflow App** to evaluate a large language model (LLM) deployed on an **Amazon SageMaker AI Inference Endpoint** using the new `mlflow.genai.evaluate()` API and MLflow GenAI evaluation features.\n",
    "\n",
    "The workflow demonstrated here focuses on a medical-domain LLM (for example, a fine-tuned Qwen model) hosted behind a SageMaker AI Inference Endpoint and evaluated via a SageMaker AI–managed MLflow App running MLflow 3.4.0 on the backend.\n",
    "\n",
    "You will learn how to:\n",
    "- Connect to a SageMaker AI MLflow App (managed MLflow tracking server) and set up an experiment.\n",
    "- Define a prediction wrapper that calls the endpoint from within MLflow evaluators.\n",
    "- Use `mlflow.genai.evaluate()` to run a battery of LLM metrics, including latency (via traces), heuristic NLP metrics, retrieval metrics, and LLM-as-a-judge metrics (built-in, custom, and third‑party integrations such as DeepEval and RAGAS)\n",
    "\n",
    "At the end, you will be able to inspect traces and aggregated evaluation metrics in the MLflow UI hosted by SageMaker AI, which makes it easy to compare multiple LLM versions and \n",
    "\n",
    "### Prerequisites\n",
    "- An active SageMaker AI Inference Endpoint with a deployed LLM\n",
    "- A SageMaker AI MLflow App\n",
    "- IAM permissions for SageMaker and Amazon Bedrock (for LLM-as-a-Judge evaluations)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f8c2a34-1584-46f7-8553-3539b0883cba",
   "metadata": {},
   "source": [
    "### Environment setup\n",
    "install the required Python dependencies for MLflow GenAI evaluation, including:\n",
    "\n",
    "- `mlflow` ≥ 3.8.1 for `mlflow.genai.evaluate()` and GenAI scorers.\n",
    "- Evaluation helper libraries such as `rouge-score`, `deepeval`, and `ragas` that integrate seamlessly with MLflow's scorer abstraction.\n",
    "\n",
    "You may see warnings about version conflicts from other Jupyter or SageMaker Studio extensions. These do not affect the core evaluation flow we build in this notebook and can be ignored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3689469b-f2a9-447e-b993-74bdc15745f3",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "autogluon-multimodal 1.4.0 requires nvidia-ml-py3<8.0,>=7.352.0, which is not installed.\n",
      "dash 2.18.1 requires dash-core-components==2.0.0, which is not installed.\n",
      "dash 2.18.1 requires dash-html-components==2.0.0, which is not installed.\n",
      "dash 2.18.1 requires dash-table==5.0.0, which is not installed.\n",
      "jupyter-ai 2.31.7 requires faiss-cpu!=1.8.0.post0,<2.0.0,>=1.8.0, which is not installed.\n",
      "sagemaker-studio 1.1.4 requires pydynamodb>=0.7.4, which is not installed.\n",
      "aiobotocore 2.22.0 requires botocore<1.37.4,>=1.37.2, but you have botocore 1.42.34 which is incompatible.\n",
      "amazon-sagemaker-sql-magic 0.1.4 requires sqlparse==0.5.0, but you have sqlparse 0.5.5 which is incompatible.\n",
      "autogluon-common 1.4.0 requires psutil<7.1.0,>=5.7.3, but you have psutil 7.2.1 which is incompatible.\n",
      "autogluon-common 1.4.0 requires pyarrow<21.0.0,>=7.0.0, but you have pyarrow 22.0.0 which is incompatible.\n",
      "autogluon-core 1.4.0 requires scikit-learn<1.8.0,>=1.4.0, but you have scikit-learn 1.8.0 which is incompatible.\n",
      "autogluon-core 1.4.0 requires scipy<1.17,>=1.5.4, but you have scipy 1.17.0 which is incompatible.\n",
      "autogluon-features 1.4.0 requires scikit-learn<1.8.0,>=1.4.0, but you have scikit-learn 1.8.0 which is incompatible.\n",
      "autogluon-multimodal 1.4.0 requires fsspec[http]<=2025.3, but you have fsspec 2025.10.0 which is incompatible.\n",
      "autogluon-multimodal 1.4.0 requires jsonschema<4.24,>=4.18, but you have jsonschema 4.26.0 which is incompatible.\n",
      "autogluon-multimodal 1.4.0 requires Pillow<12,>=10.0.1, but you have pillow 12.1.0 which is incompatible.\n",
      "autogluon-multimodal 1.4.0 requires scikit-learn<1.8.0,>=1.4.0, but you have scikit-learn 1.8.0 which is incompatible.\n",
      "autogluon-multimodal 1.4.0 requires scipy<1.17,>=1.5.4, but you have scipy 1.17.0 which is incompatible.\n",
      "autogluon-multimodal 1.4.0 requires transformers[sentencepiece]<4.50,>=4.38.0, but you have transformers 4.57.3 which is incompatible.\n",
      "autogluon-tabular 1.4.0 requires scikit-learn<1.8.0,>=1.4.0, but you have scikit-learn 1.8.0 which is incompatible.\n",
      "autogluon-tabular 1.4.0 requires scipy<1.17,>=1.5.4, but you have scipy 1.17.0 which is incompatible.\n",
      "autogluon-timeseries 1.4.0 requires scipy<1.17,>=1.5.4, but you have scipy 1.17.0 which is incompatible.\n",
      "autogluon-timeseries 1.4.0 requires transformers[sentencepiece]<4.50,>=4.38.0, but you have transformers 4.57.3 which is incompatible.\n",
      "awswrangler 3.14.0 requires pyarrow<22.0.0,>=8.0.0, but you have pyarrow 22.0.0 which is incompatible.\n",
      "dash 2.18.1 requires Flask<3.1,>=1.0.4, but you have flask 3.1.2 which is incompatible.\n",
      "dash 2.18.1 requires Werkzeug<3.1, but you have werkzeug 3.1.5 which is incompatible.\n",
      "jupyter-ai-magics 2.31.7 requires langchain<0.4.0,>=0.3.0, but you have langchain 1.2.7 which is incompatible.\n",
      "jupyter-ai-magics 2.31.7 requires langchain-community<0.4.0,>=0.3.0, but you have langchain-community 0.4.1 which is incompatible.\n",
      "jupyter-scheduler 2.11.0 requires fsspec!=2025.3.1,<=2025.3.2,>=2023.6.0, but you have fsspec 2025.10.0 which is incompatible.\n",
      "jupyter-scheduler 2.11.0 requires psutil~=5.9, but you have psutil 7.2.1 which is incompatible.\n",
      "jupyter-scheduler 2.11.0 requires pytz<=2024.2,>=2023.3, but you have pytz 2025.2 which is incompatible.\n",
      "langchain-aws 0.2.19 requires langchain-core<0.4.0,>=0.3.49, but you have langchain-core 1.2.7 which is incompatible.\n",
      "s3fs 2024.12.0 requires fsspec==2024.12.0.*, but you have fsspec 2025.10.0 which is incompatible.\n",
      "sagemaker-studio-analytics-extension 0.2.2 requires sparkmagic==0.22.0, but you have sparkmagic 0.21.0 which is incompatible.\n",
      "snowflake-connector-python 3.17.4 requires cffi<2.0.0,>=1.9, but you have cffi 2.0.0 which is incompatible.\n",
      "sparkmagic 0.21.0 requires pandas<2.0.0,>=0.17.1, but you have pandas 2.3.3 which is incompatible.\n",
      "strands-agents-tools 0.1.9 requires pillow<12.0.0,>=11.2.1, but you have pillow 12.1.0 which is incompatible.\n",
      "transformers 4.57.3 requires huggingface-hub<1.0,>=0.34.0, but you have huggingface-hub 1.3.3 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "# Install required dependencies. Ignore any warnings and residual dependency errors.\n",
    "!pip install --force-reinstall -U -r requirements.txt --quiet  --no-warn-conflicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0be05172-57bb-4c2d-ac1d-35bcbb4ab6c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/sagemaker-user/.config/sagemaker/config.yaml\n",
      "us-west-2\n",
      "sagemaker-us-west-2-736264693883\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import shutil\n",
    "import sagemaker\n",
    "from sagemaker.config import load_sagemaker_config\n",
    "import os\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "s3_client = boto3.client('s3')\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "\n",
    "region = sagemaker_session.boto_session.region_name\n",
    "bucket_name = sagemaker_session.default_bucket()\n",
    "default_prefix = sagemaker_session.default_bucket_prefix\n",
    "configs = load_sagemaker_config()\n",
    "print(region)\n",
    "print(bucket_name)\n",
    "print(default_prefix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dcd6d5e-1788-42da-b796-bf95c2f60a7d",
   "metadata": {},
   "source": [
    "# Configure SageMaker AI Inference endpoint\n",
    "Point the notebook to an existing **SageMaker AI Inference Endpoint** that serves your fine‑tuned LLM.\n",
    "\n",
    "For this example, we assume we have fine‑tuned a Qwen‑family model (e.g., `Qwen3-4B-Instruct`) on a medical reasoning dataset and deployed it on SageMaker AI Inference as a real‑time endpoint. This endpoint is what we will evaluate using `mlflow.genai.evaluate()`. You can use an existing SageMaker AI Inference Endpoint and use use-case you want to evaluate by updating the dataset and the evaluation criteria as suited. \n",
    "\n",
    "Update `SAGEMAKER_ENDPOINT_NAME` to reference your own endpoint name before running the cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fe29986b-1048-429e-9caf-6659fd4498e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter your SageMaker AI endpoint name\n",
    "SAGEMAKER_ENDPOINT_NAME = \"Qwen-Qwen3-4B-Instruct-2507-sft-djl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "45cb8a1d-2aee-4585-a0f3-6be7ef4d8d59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test your SageMaker AI endpoint with sample invokation\n",
    "predictor = sagemaker.Predictor(\n",
    "    endpoint_name=SAGEMAKER_ENDPOINT_NAME,\n",
    "    sagemaker_session=sagemaker_session,\n",
    "    serializer=sagemaker.serializers.JSONSerializer(),\n",
    "    deserializer=sagemaker.deserializers.JSONDeserializer(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8c25a126-6971-4779-82ad-b806c25d25fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'user',\n",
       "  'content': 'Scientists are developing a new non-steroidal anti-inflammatory drug for osteoarthritis, aiming for higher potency but the same efficacy as ibuprofen to reduce gastrointestinal side effects. If ibuprofen is represented by curve C in the figure and the desired therapeutic effect is marked by the dashed line Y, what curve would represent the new drug that fulfills these criteria?'}]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test prompt to invoke SageMaker AI endpoint\n",
    "USER_PROMPT = \"Scientists are developing a new non-steroidal anti-inflammatory drug for osteoarthritis, aiming for higher potency but the same efficacy as ibuprofen to reduce gastrointestinal side effects. If ibuprofen is represented by curve C in the figure and the desired therapeutic effect is marked by the dashed line Y, what curve would represent the new drug that fulfills these criteria?\"\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": USER_PROMPT},\n",
    "]\n",
    "\n",
    "messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6b2d5495-5007-43ac-a634-985cc103fd07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"To determine which curve represents the new non-steroidal anti-inflammatory drug that meets the specified criteria, let's break down the requirements and analyze them in the context of the provided information.\\n\\n### Criteria:\\n1. **Higher potency** than ibuprofen.\\n2. **Same efficacy** as ibuprofen.\\n3. **Reduced gastrointestinal side effects**.\\n\\n### Understanding the curves:\\n- **Curve C**: Represents ibuprofen.\\n- **Dashed line Y**: Marks the desired therapeutic effect (i.e., the level of pain relief or symptom reduction needed for effective treatment).\\n\\n### Definitions:\\n- **Potency** refers to the amount of drug needed to produce a given effect. A more potent drug achieves the same therapeutic effect at a lower dose.\\n- **Efficacy** refers to the maximum effect a drug can produce. If two drugs have the same efficacy, they can both achieve the same maximum therapeutic outcome.\\n\\n### Analysis:\\n- A **higher potency** drug will reach the therapeutic effect (dashed line Y) at a **lower dose** than ibuprofen.\\n- Since the **efficacy is the same**, the maximum effect (point where the curve hits the dashed line Y) must be identical for both drugs.\\n- The goal is to **reduce gastrointestinal side effects**, which typically correlate with the dose of the drug. A lower dose can potentially lead to fewer side effects.\\n\\n### Conclusion:\\nThe new drug should:\\n- Achieve the same maximum therapeutic effect (same efficacy).\\n- Do so at a lower dose (higher potency).\\n\\nThus, the curve representing the new drug should:\\n- Reach the dashed line Y at a lower dose than ibuprofen.\\n- Have the same endpoint (maximum effect) as ibuprofen.\\n\\n### Final Answer:\\n**Curve D** (assuming the figure shows Curve D as reaching the therapeutic effect at a lower dose than Curve C).\\n\\n✅ **Answer: Curve D**\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Verify the SageMaker AI endpoint response\n",
    "response = predictor.predict({\n",
    "\t\"messages\": [messages[-1]],\n",
    "    \"parameters\": {\n",
    "        \"temperature\": 0, # deterministic output\n",
    "        \"top_p\": 0.9,\n",
    "        \"return_full_text\": False,\n",
    "        \"max_new_tokens\": 1024\n",
    "    }\n",
    "})\n",
    "\n",
    "response[\"choices\"][0][\"message\"][\"content\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe5dbbb1-510b-4c2e-91cf-09114425abcb",
   "metadata": {},
   "source": [
    "# Prepare evaluation dataset\n",
    "\n",
    "To evaluate an LLM, you need a dataset of **inputs** (e.g., questions) and **expectations** (e.g., reference answers or gold labels). In this notebook, we reuse samples from the `FreedomIntelligence/medical-o1-reasoning-SFT` dataset as a proxy for a domain‑specific medical evaluation set.\n",
    "\n",
    "Each sample contains:\n",
    "\n",
    "- A medical **Question**.\n",
    "- A detailed **Response** that we treat as the expected answer.\n",
    "- An optional chain‑of‑thought field (`Complex_CoT`) that we ignore for scoring, but which could be used in more advanced evaluation setups.\n",
    "\n",
    "You can replace this dataset with your own labeled evaluation data (for example, human‑annotated medical Q&A pairs corresponding to your domain and compliance requirements)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "44ca9f5b-aedb-4ca5-8fde-0975d0ea0b0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Question': 'Given the symptoms of sudden weakness in the left arm and leg, recent long-distance travel, and the presence of swollen and tender right lower leg, what specific cardiac abnormality is most likely to be found upon further evaluation that could explain these findings?',\n",
       " 'Complex_CoT': \"Okay, let's see what's going on here. We've got sudden weakness in the person's left arm and leg - and that screams something neuro-related, maybe a stroke?\\n\\nBut wait, there's more. The right lower leg is swollen and tender, which is like waving a big flag for deep vein thrombosis, especially after a long flight or sitting around a lot.\\n\\nSo, now I'm thinking, how could a clot in the leg end up causing issues like weakness or stroke symptoms?\\n\\nOh, right! There's this thing called a paradoxical embolism. It can happen if there's some kind of short circuit in the heart - like a hole that shouldn't be there.\\n\\nLet's put this together: if a blood clot from the leg somehow travels to the left side of the heart, it could shoot off to the brain and cause that sudden weakness by blocking blood flow there.\\n\\nHmm, but how would the clot get from the right side of the heart to the left without going through the lungs and getting filtered out?\\n\\nHere's where our cardiac anomaly comes in: a patent foramen ovale or PFO. That's like a sneaky little shortcut in the heart between the right and left atria.\\n\\nAnd it's actually pretty common, found in about a quarter of adults, which definitely makes it the top suspect here.\\n\\nSo with all these pieces - long travel, leg clot, sudden weakness - a PFO fits the bill perfectly, letting a clot cross over and cause all this.\\n\\nEverything fits together pretty neatly, so I'd bet PFO is the heart issue waiting to be discovered. Yeah, that really clicks into place!\",\n",
       " 'Response': 'The specific cardiac abnormality most likely to be found in this scenario is a patent foramen ovale (PFO). This condition could allow a blood clot from the venous system, such as one from a deep vein thrombosis in the leg, to bypass the lungs and pass directly into the arterial circulation. This can occur when the clot moves from the right atrium to the left atrium through the PFO. Once in the arterial system, the clot can travel to the brain, potentially causing an embolic stroke, which would explain the sudden weakness in the left arm and leg. The connection between the recent travel, which increases the risk of deep vein thrombosis, and the neurological symptoms suggests the presence of a PFO facilitating a paradoxical embolism.'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "\n",
    "num_samples = 100\n",
    "\n",
    "full_dataset = load_dataset(\"FreedomIntelligence/medical-o1-reasoning-SFT\", \"en\", split=f\"train[:{num_samples}]\")\n",
    "\n",
    "full_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8a2b78a1-3f00-432e-8186-516f90d935f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Converted 100 samples\n",
      "\n",
      "MLflow GenAI format (first sample):\n",
      "{\n",
      "  \"inputs\": {\n",
      "    \"question\": \"Given the symptoms of sudden weakness in the left arm and leg, recent long-distance travel, and the presence of swollen and tender right lower leg, what specific cardiac abnormality is most likely to be found upon further evaluation that could explain these findings?\"\n",
      "  },\n",
      "  \"expectations\": {\n",
      "    \"expected_response\": \"The specific cardiac abnormality most likely to be found in this scenario is a patent foramen ovale (PFO). This condition could allow a blood clot from the venous system, such as one from a deep vein thrombosis in the leg, to bypass the lungs and pass directly into the arterial circulation. This can occur when the clot moves from the right atrium to the left atrium through the PFO. Once in the arterial system, the clot can travel to the brain, potentially causing an embolic stroke, which would explain the sudden weakness in the left arm and leg. The connection between the recent travel, which increases the risk of deep vein thrombosis, and the neurological symptoms suggests the presence of a PFO facilitating a paradoxical embolism.\"\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Convert to MLflow GenAI evaluation format\n",
    "import json\n",
    "eval_dataset = []\n",
    "\n",
    "for sample in full_dataset:\n",
    "    eval_entry = {\n",
    "        \"inputs\": {\n",
    "            \"question\": sample[\"Question\"]\n",
    "        },\n",
    "        \"expectations\": {\n",
    "            \"expected_response\": f\"{sample['Response']}\"\n",
    "        }\n",
    "    }\n",
    "    eval_dataset.append(eval_entry)\n",
    "\n",
    "print(f\"\\n✅ Converted {len(eval_dataset)} samples\")\n",
    "print(f\"\\nMLflow GenAI format (first sample):\")\n",
    "print(json.dumps(eval_dataset[0], indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "107be8d7-42a4-4117-a619-81d04fc011a2",
   "metadata": {},
   "source": [
    "# Configure SageMaker AI MLFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c589fa72-2ebb-446e-8537-ef40b59b2871",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ MLflow tracking server configured: arn:aws:sagemaker:us-west-2:736264693883:mlflow-app/app-O3RGB5VOASTI\n"
     ]
    }
   ],
   "source": [
    "import mlflow\n",
    "\n",
    "# Set your SageMaker AI Managed MLflow tracking server ARN \n",
    "#\"ENTER YOUR MLFLOW TRACKIHG SERVER ARN HERE\" \n",
    "TRACKING_SERVER_ARN = \"arn:aws:sagemaker:us-west-2:736264693883:mlflow-app/app-O3RGB5VOASTI\"\n",
    "\n",
    "# Set MLflow experiment name for tracking information\n",
    "experiment_name = \"sagemaker-medical-fine-tune-llm\"\n",
    "# Set MLflow SDK to your configured tracking server \n",
    "mlflow.set_tracking_uri(TRACKING_SERVER_ARN) \n",
    "# Create or select an MLflow experiment\n",
    "mlflow.set_experiment(experiment_name)\n",
    "\n",
    "print(f\"✅ MLflow tracking server configured: {mlflow.get_tracking_uri()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e87351ce-9c10-4e17-ba30-c1529a494b4c",
   "metadata": {},
   "source": [
    "## Configure SageMaker AI MLFlow for performing LLM evaluations\n",
    "Now we perform the following setup:\n",
    "\n",
    "- Define a `qa_predict_fn(question: str) -> str` that wraps calls to your SageMaker AI Inference Endpoint. This function is the **model under test** from MLflow's perspective.\n",
    "- Configure `AWS_ROLE_ARN` for the Amazon Bedrock model that acts as an LLM‑as‑a‑judge during evaluation. This judge is used by several built‑in and custom scorers.\n",
    "- Specify `MLFLOW_EVALUATION_MODEL_ID` and `MLFLOW_EVALUATION_MODEL_PARAM` to control which Bedrock model (for example, a Claude Sonnet variant) is used as the judge and with what generation parameters (temperature, max tokens, stop sequences, etc.).\n",
    "\n",
    "This separation lets you evaluate:\n",
    "\n",
    "- A **candidate model** (your Qwen medical LLM served on SageMaker AI Inference).\n",
    "- Using a **judge model** (Bedrock‑hosted Claude different than the fine-tuned model to be evaluated) that scores correctness, safety, and guideline adherence via `mlflow.genai` scorers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4a7f14dd-cff5-4011-a7df-350bfb9c9728",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for the mlflow evaluator to call the sagemaker endpoint\n",
    "def qa_predict_fn(question: str) -> str:\n",
    "    \"\"\"Wrapper function for evaluation using sagemaker endpoint predictor.\"\"\"\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": question},\n",
    "    ]\n",
    "    response = predictor.predict({\n",
    "        \"messages\": messages,\n",
    "        \"parameters\": {\n",
    "            \"temperature\": 0.2,\n",
    "            \"top_p\": 0.9,\n",
    "            \"return_full_text\": False,\n",
    "            \"max_new_tokens\": 1024\n",
    "        }\n",
    "    })\n",
    "    return response[\"choices\"][0][\"message\"][\"content\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "561f7f3c-fb85-4abe-b10f-15cb891b4f78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arn:aws:iam::736264693883:role/service-role/AmazonSageMaker-ExecutionRole-20250402T133578\n"
     ]
    }
   ],
   "source": [
    "# Set IAM Role for the Amazon Bedrock model to assume\n",
    "os.environ[\"AWS_ROLE_ARN\"] = sagemaker.get_execution_role()\n",
    "print(os.environ[\"AWS_ROLE_ARN\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b58bce2f-648f-4b39-8c29-94135341bdd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the Amazon Bedrock model ID to use as the LLM evaluator\n",
    "MLFLOW_EVALUATION_MODEL_ID = \"bedrock:/global.anthropic.claude-sonnet-4-20250514-v1:0\" #\"bedrock:/us.anthropic.claude-3-5-haiku-20241022-v1:0\",\n",
    "MLFLOW_EVALUATION_MODEL_PARAM = {\n",
    "    \"temperature\": 0, # 0 for deterministic\n",
    "    \"max_tokens\": 512, # 256\n",
    "    \"anthropic_version\": \"bedrock-2023-05-31\",\n",
    "    \"top_p\": 0.9,  # Add top_p for more controlled generation\n",
    "    \"stop_sequences\": [\"}\"]  # Stop after JSON closes\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "899b2366-d667-4df7-ab49-713012e7a982",
   "metadata": {},
   "source": [
    "### Define custom MLflow GenAI scorers\n",
    "\n",
    "MLflow 3.8.1 exposes a unified GenAI evaluation API (`mlflow.genai.evaluate()`) that works with a library of **scorers**.\n",
    "\n",
    "In addition to built‑in scorers, you can register your own metrics using the `@mlflow.genai.scorer` decorator. In this notebook, we implement:\n",
    "\n",
    "- `is_brief`: a simple boolean heuristic that checks whether an answer stays under 15 words. You can adapt this pattern for other task‑specific heuristics (e.g., maximum reading level or maximum token budget).\n",
    "- `rougeL_fmeasure`: a custom wrapper around `rouge_score` that computes the ROUGE‑L F‑measure between the model output and the expected response. This gives you a traditional lexical similarity metric alongside LLM‑as‑a‑judge metrics.\n",
    "\n",
    "We also create a `coherence_judge` using `mlflow.genai.judges.make_judge`, which defines a prompt‑template‑driven LLM‑as‑a‑judge that evaluates the coherence of generated responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ba560b64-eb84-4f8f-a351-c1046b1b9080",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom MLflow scorer functions\n",
    "\n",
    "from typing import Literal\n",
    "from mlflow.genai import scorer\n",
    "from mlflow.genai.judges import make_judge\n",
    "from rouge_score import rouge_scorer\n",
    "\n",
    "\n",
    "@scorer\n",
    "def is_brief(outputs: str) -> bool:\n",
    "    \"\"\"Evaluate if the answer is concise (less than 15 words)\"\"\"\n",
    "    return len(outputs.split()) <= 15\n",
    "\n",
    "@scorer\n",
    "def rougeL_fmeasure(outputs: str, expectations: dict) -> dict:\n",
    "    custom_rouge_scorer = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)\n",
    "    return custom_rouge_scorer.score(expectations[\"expected_response\"], outputs)['rougeL'].fmeasure\n",
    "\n",
    "# Create a judge that evaluates coherence using MLflow template-based scorers\n",
    "coherence_judge = make_judge(\n",
    "    name=\"coherence\",\n",
    "    instructions=(\n",
    "        \"Evaluate if the response is coherent, maintaining a constant tone \"\n",
    "        \"and following a clear flow of thoughts/concepts\"\n",
    "        \"Question: {{ inputs }}\\n\"\n",
    "        \"Response: {{ outputs }}\\n\"\n",
    "    ),\n",
    "    feedback_value_type=Literal[\"coherent\", \"somewhat coherent\", \"incoherent\"],\n",
    "    model= MLFLOW_EVALUATION_MODEL_ID\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4482afa-bf60-4e6f-998b-17e8cfbd3583",
   "metadata": {},
   "source": [
    "### Configure evaluation scorers and third‑party integrations\n",
    "\n",
    "The `scorers` list below specifies the full set of metrics that `mlflow.genai.evaluate()` will compute in a single pass.\n",
    "\n",
    "It contains:\n",
    "\n",
    "- **Built‑in MLflow GenAI scorers**:\n",
    "  - `Safety`: checks for content safety and policy violations.\n",
    "  - `RelevanceToQuery`: measures how well the answer addresses the user query.\n",
    "  - `Equivalence`: compares model outputs to expected responses for semantic similarity.\n",
    "  - `Correctness`: checks factual or logical correctness relative to expectations.\n",
    "\n",
    "- **Guidelines‑based LLM‑as‑a‑judge scorers**:\n",
    "  - Multiple `Guidelines` scorers that encode domain‑specific constraints for medical responses, such as:\n",
    "    - Following the clinical objective.\n",
    "    - Maintaining a professional medical tone.\n",
    "    - Avoiding harmful advice (e.g., no specific prescriptions or delayed emergency care).\n",
    "    - Demonstrating empathy and ending with a clear recommended action.\n",
    "\n",
    "- **Template‑based LLM‑as‑a‑judge scorer**:\n",
    "  - `coherence_judge`, which scores coherence using a custom judge prompt.\n",
    "\n",
    "- **Third‑party evaluation frameworks (DeepEval and RAGAS) available through MLflow**:\n",
    "  - `Bias`, `AnswerRelevancy`, `Faithfulness` from `mlflow.genai.scorers.deepeval`, giving you access to DeepEval's library of over 20 evaluation metrics via MLflow’s scorer abstraction.\n",
    "  - `ChrfScore` and `BleuScore` from `mlflow.genai.scorers.ragas`, which provide retrieval‑augmented generation (RAG)‑oriented metrics when you include contexts. In this example we use them as non‑LLM metrics over text pairs.\n",
    "\n",
    "- **Custom scorers**:\n",
    "  - `is_brief` (heuristic conciseness check).\n",
    "  - `rougeL_fmeasure` (custom ROUGE‑L F‑measure implementation).\n",
    "\n",
    "When you run `mlflow.genai.evaluate()`, each scorer logs metrics and, where applicable, traces to the MLflow App. Latency and token counts are captured automatically by MLflow Tracing rather than via explicit scorer definitions.\n",
    "\n",
    "> Note: Token counts is not calculated by default as we are using managed SageMaker AI endpoints and you will need to define custom metric if you want to calcuate the token counts for this use-case. There are many more metrics offered through MLflow and you can see the MLflow documentation for the full list and their details. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a4b0ccd7-c66a-4af5-9b22-afaa1bf58f13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define all the MLflow scorer to use for evaluating the models\n",
    "from mlflow.genai.scorers import Correctness, Guidelines, Safety, RelevanceToQuery, Equivalence\n",
    "from mlflow.genai.scorers.deepeval import Bias, AnswerRelevancy, Faithfulness\n",
    "from mlflow.genai.scorers.ragas import ChrfScore, BleuScore\n",
    "\n",
    "scorers = [\n",
    "    # MLflow built-in genai scorers\n",
    "    Safety(\n",
    "        model= MLFLOW_EVALUATION_MODEL_ID,\n",
    "        parameters= MLFLOW_EVALUATION_MODEL_PARAM,\n",
    "    ),\n",
    "    RelevanceToQuery(\n",
    "        model= MLFLOW_EVALUATION_MODEL_ID,\n",
    "        parameters= MLFLOW_EVALUATION_MODEL_PARAM,\n",
    "    ),\n",
    "    Equivalence(\n",
    "        model= MLFLOW_EVALUATION_MODEL_ID,\n",
    "        parameters= MLFLOW_EVALUATION_MODEL_PARAM,\n",
    "    ),\n",
    "    Correctness(\n",
    "        model= MLFLOW_EVALUATION_MODEL_ID,\n",
    "        parameters= MLFLOW_EVALUATION_MODEL_PARAM,\n",
    "    ),\n",
    "    # MLflow built-in guidelines-based LLM-as-a-judge\n",
    "    Guidelines(\n",
    "        name=\"follows_objective\",\n",
    "        guidelines=\"The generated response must follow the objective in the request.\",\n",
    "        model= MLFLOW_EVALUATION_MODEL_ID,\n",
    "        parameters= MLFLOW_EVALUATION_MODEL_PARAM,\n",
    "    ),\n",
    "    Guidelines(\n",
    "        name=\"concise_communication\",\n",
    "        guidelines=\"The response MUST be concise and to the point. The response should communicate the key message efficiently without being overly brief or losing important context.\",\n",
    "        model= MLFLOW_EVALUATION_MODEL_ID,\n",
    "        parameters= MLFLOW_EVALUATION_MODEL_PARAM,\n",
    "    ),\n",
    "    Guidelines(\n",
    "        name=\"professional_medical_tone\",\n",
    "        guidelines=\"The response must be in a professional tone.\",\n",
    "        model= MLFLOW_EVALUATION_MODEL_ID,\n",
    "        parameters= MLFLOW_EVALUATION_MODEL_PARAM,\n",
    "    ),\n",
    "    Guidelines(\n",
    "        name=\"no_harmful_advice\",\n",
    "        guidelines=\"The response MUST NOT provide specific diagnoses, medication recommendations, or advice that could delay necessary emergency care. Must NOT give false reassurance for potentially serious symptoms.\",\n",
    "        model= MLFLOW_EVALUATION_MODEL_ID,\n",
    "        parameters= MLFLOW_EVALUATION_MODEL_PARAM,\n",
    "    ),\n",
    "    Guidelines(\n",
    "        name=\"empathy_and_clarity\",\n",
    "        guidelines=\"The response must demonstrate empathy for patient concerns while providing clear, unambiguous next steps. Every response should end with a concrete action.\",\n",
    "        model= MLFLOW_EVALUATION_MODEL_ID,\n",
    "        parameters= MLFLOW_EVALUATION_MODEL_PARAM,\n",
    "    ),\n",
    "    \n",
    "    # MLflow built-in template-based LLM-as-a-judge\n",
    "    coherence_judge,\n",
    "    # deepeval scorers\n",
    "    Bias(\n",
    "        model= MLFLOW_EVALUATION_MODEL_ID,\n",
    "    ),\n",
    "    AnswerRelevancy(\n",
    "        model= MLFLOW_EVALUATION_MODEL_ID,\n",
    "    ),\n",
    "    Faithfulness(\n",
    "        model= MLFLOW_EVALUATION_MODEL_ID,\n",
    "    ),\n",
    "    \n",
    "    # RAGAS scorers\n",
    "    # Non-LLM metric (no model required)\n",
    "    ChrfScore(),\n",
    "    BleuScore(),\n",
    "    \n",
    "    # Custom defined scorers\n",
    "    is_brief,\n",
    "    # Custom huieristic ROUGE-L Score\n",
    "    rougeL_fmeasure,\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "29cd206b-9754-4abb-b0c5-517105ec5c9f",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026/01/23 22:34:27 INFO mlflow.models.evaluation.utils.trace: Auto tracing is temporarily enabled during the model evaluation for computing some metrics and debugging. To disable tracing, call `mlflow.autolog(disable=True)`.\n",
      "2026/01/23 22:34:27 INFO mlflow.genai.utils.data_validation: Testing model prediction with the first sample in the dataset. To disable this check, set the MLFLOW_GENAI_EVAL_SKIP_TRACE_VALIDATION environment variable to True.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6eeae0fcd2704948aceb936f4242f0f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/10 [Elapsed: 00:00, Remaining: ?] "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026/01/23 22:34:35 WARNING mlflow.genai.judges.instructions_judge: The following parameters were provided but are not used by this judge's instructions: 'expectations'. The judge only uses template variables that appear in the instructions: {'outputs', 'inputs'}\n",
      "2026/01/23 22:34:36 WARNING mlflow.genai.judges.instructions_judge: The following parameters were provided but are not used by this judge's instructions: 'expectations'. The judge only uses template variables that appear in the instructions: {'outputs', 'inputs'}\n",
      "2026/01/23 22:34:36 WARNING mlflow.genai.judges.instructions_judge: The following parameters were provided but are not used by this judge's instructions: 'expectations'. The judge only uses template variables that appear in the instructions: {'outputs', 'inputs'}\n",
      "2026/01/23 22:34:36 WARNING mlflow.genai.judges.instructions_judge: The following parameters were provided but are not used by this judge's instructions: 'expectations'. The judge only uses template variables that appear in the instructions: {'outputs', 'inputs'}\n",
      "2026/01/23 22:34:37 WARNING mlflow.genai.judges.instructions_judge: The following parameters were provided but are not used by this judge's instructions: 'expectations'. The judge only uses template variables that appear in the instructions: {'outputs', 'inputs'}\n",
      "2026/01/23 22:34:38 WARNING mlflow.genai.judges.instructions_judge: The following parameters were provided but are not used by this judge's instructions: 'expectations'. The judge only uses template variables that appear in the instructions: {'outputs', 'inputs'}\n",
      "2026/01/23 22:34:38 WARNING mlflow.genai.judges.instructions_judge: The following parameters were provided but are not used by this judge's instructions: 'expectations'. The judge only uses template variables that appear in the instructions: {'outputs', 'inputs'}\n",
      "ERROR:root:Error in LiteLLM generation: litellm.RateLimitError: BedrockException - {\"message\":\"Too many tokens, please wait before trying again.\"}\n",
      "ERROR:root:LiteLLM Error: litellm.RateLimitError: BedrockException - {\"message\":\"Too many tokens, please wait before trying again.\"} Retrying: 1 time(s)...\n",
      "ERROR:root:Error in LiteLLM generation: litellm.RateLimitError: BedrockException - {\"message\":\"Too many tokens, please wait before trying again.\"}\n",
      "ERROR:root:LiteLLM Error: litellm.RateLimitError: BedrockException - {\"message\":\"Too many tokens, please wait before trying again.\"} Retrying: 1 time(s)...\n",
      "ERROR:root:Error in LiteLLM generation: litellm.RateLimitError: BedrockException - {\"message\":\"Too many tokens, please wait before trying again.\"}\n",
      "ERROR:root:LiteLLM Error: litellm.RateLimitError: BedrockException - {\"message\":\"Too many tokens, please wait before trying again.\"} Retrying: 1 time(s)...\n",
      "ERROR:root:Error in LiteLLM generation: litellm.RateLimitError: BedrockException - {\"message\":\"Too many tokens, please wait before trying again.\"}\n",
      "ERROR:root:LiteLLM Error: litellm.RateLimitError: BedrockException - {\"message\":\"Too many tokens, please wait before trying again.\"} Retrying: 1 time(s)...\n",
      "ERROR:root:Error in LiteLLM generation: litellm.RateLimitError: BedrockException - {\"message\":\"Too many tokens, please wait before trying again.\"}\n",
      "ERROR:root:LiteLLM Error: litellm.RateLimitError: BedrockException - {\"message\":\"Too many tokens, please wait before trying again.\"} Retrying: 1 time(s)...\n",
      "ERROR:root:Error in LiteLLM generation: litellm.RateLimitError: BedrockException - {\"message\":\"Too many tokens, please wait before trying again.\"}\n",
      "ERROR:root:LiteLLM Error: litellm.RateLimitError: BedrockException - {\"message\":\"Too many tokens, please wait before trying again.\"} Retrying: 1 time(s)...\n",
      "ERROR:root:Error in LiteLLM generation: litellm.RateLimitError: BedrockException - {\"message\":\"Too many tokens, please wait before trying again.\"}\n",
      "ERROR:root:LiteLLM Error: litellm.RateLimitError: BedrockException - {\"message\":\"Too many tokens, please wait before trying again.\"} Retrying: 1 time(s)...\n",
      "ERROR:root:Error in LiteLLM generation: litellm.RateLimitError: BedrockException - {\"message\":\"Too many tokens, please wait before trying again.\"}\n",
      "ERROR:root:LiteLLM Error: litellm.RateLimitError: BedrockException - {\"message\":\"Too many tokens, please wait before trying again.\"} Retrying: 1 time(s)...\n",
      "ERROR:root:Error in LiteLLM generation: litellm.RateLimitError: BedrockException - {\"message\":\"Too many tokens, please wait before trying again.\"}\n",
      "ERROR:root:LiteLLM Error: litellm.RateLimitError: BedrockException - {\"message\":\"Too many tokens, please wait before trying again.\"} Retrying: 1 time(s)...\n",
      "ERROR:root:Error in LiteLLM generation: litellm.RateLimitError: BedrockException - {\"message\":\"Too many tokens, please wait before trying again.\"}\n",
      "ERROR:root:LiteLLM Error: litellm.RateLimitError: BedrockException - {\"message\":\"Too many tokens, please wait before trying again.\"} Retrying: 2 time(s)...\n",
      "ERROR:root:Error in LiteLLM generation: litellm.RateLimitError: BedrockException - {\"message\":\"Too many tokens, please wait before trying again.\"}\n",
      "ERROR:root:LiteLLM Error: litellm.RateLimitError: BedrockException - {\"message\":\"Too many tokens, please wait before trying again.\"} Retrying: 1 time(s)...\n",
      "ERROR:root:Error in LiteLLM generation: litellm.RateLimitError: BedrockException - {\"message\":\"Too many tokens, please wait before trying again.\"}\n",
      "ERROR:root:LiteLLM Error: litellm.RateLimitError: BedrockException - {\"message\":\"Too many tokens, please wait before trying again.\"} Retrying: 1 time(s)...\n",
      "ERROR:root:Error in LiteLLM generation: litellm.RateLimitError: BedrockException - {\"message\":\"Too many tokens, please wait before trying again.\"}\n",
      "ERROR:root:LiteLLM Error: litellm.RateLimitError: BedrockException - {\"message\":\"Too many tokens, please wait before trying again.\"} Retrying: 2 time(s)...\n",
      "ERROR:root:Error in LiteLLM generation: litellm.RateLimitError: BedrockException - {\"message\":\"Too many tokens, please wait before trying again.\"}\n",
      "ERROR:root:LiteLLM Error: litellm.RateLimitError: BedrockException - {\"message\":\"Too many tokens, please wait before trying again.\"} Retrying: 1 time(s)...\n",
      "ERROR:root:Error in LiteLLM generation: litellm.RateLimitError: BedrockException - {\"message\":\"Too many tokens, please wait before trying again.\"}\n",
      "ERROR:root:LiteLLM Error: litellm.RateLimitError: BedrockException - {\"message\":\"Too many tokens, please wait before trying again.\"} Retrying: 2 time(s)...\n",
      "ERROR:root:Error in LiteLLM generation: litellm.RateLimitError: BedrockException - {\"message\":\"Too many tokens, please wait before trying again.\"}\n",
      "ERROR:root:LiteLLM Error: litellm.RateLimitError: BedrockException - {\"message\":\"Too many tokens, please wait before trying again.\"} Retrying: 2 time(s)...\n",
      "ERROR:root:Error in LiteLLM generation: litellm.RateLimitError: BedrockException - {\"message\":\"Too many tokens, please wait before trying again.\"}\n",
      "ERROR:root:LiteLLM Error: litellm.RateLimitError: BedrockException - {\"message\":\"Too many tokens, please wait before trying again.\"} Retrying: 2 time(s)...\n",
      "ERROR:root:Error in LiteLLM generation: litellm.RateLimitError: BedrockException - {\"message\":\"Too many tokens, please wait before trying again.\"}\n",
      "ERROR:root:LiteLLM Error: litellm.RateLimitError: BedrockException - {\"message\":\"Too many tokens, please wait before trying again.\"} Retrying: 1 time(s)...\n",
      "ERROR:root:Error in LiteLLM generation: litellm.RateLimitError: BedrockException - {\"message\":\"Too many tokens, please wait before trying again.\"}\n",
      "ERROR:root:LiteLLM Error: litellm.RateLimitError: BedrockException - {\"message\":\"Too many tokens, please wait before trying again.\"} Retrying: 1 time(s)...\n",
      "ERROR:root:Error in LiteLLM generation: litellm.RateLimitError: BedrockException - {\"message\":\"Too many tokens, please wait before trying again.\"}\n",
      "ERROR:root:LiteLLM Error: litellm.RateLimitError: BedrockException - {\"message\":\"Too many tokens, please wait before trying again.\"} Retrying: 1 time(s)...\n",
      "ERROR:root:Error in LiteLLM generation: litellm.RateLimitError: BedrockException - {\"message\":\"Too many tokens, please wait before trying again.\"}\n",
      "ERROR:root:LiteLLM Error: litellm.RateLimitError: BedrockException - {\"message\":\"Too many tokens, please wait before trying again.\"} Retrying: 2 time(s)...\n",
      "ERROR:root:Error in LiteLLM generation: litellm.RateLimitError: BedrockException - {\"message\":\"Too many tokens, please wait before trying again.\"}\n",
      "ERROR:root:LiteLLM Error: litellm.RateLimitError: BedrockException - {\"message\":\"Too many tokens, please wait before trying again.\"} Retrying: 1 time(s)...\n",
      "ERROR:root:Error in LiteLLM generation: litellm.RateLimitError: BedrockException - {\"message\":\"Too many tokens, please wait before trying again.\"}\n",
      "ERROR:root:LiteLLM Error: litellm.RateLimitError: BedrockException - {\"message\":\"Too many tokens, please wait before trying again.\"} Retrying: 2 time(s)...\n",
      "ERROR:root:Error in LiteLLM generation: litellm.RateLimitError: BedrockException - {\"message\":\"Too many tokens, please wait before trying again.\"}\n",
      "ERROR:root:LiteLLM Error: litellm.RateLimitError: BedrockException - {\"message\":\"Too many tokens, please wait before trying again.\"} Retrying: 2 time(s)...\n",
      "ERROR:root:Error in LiteLLM generation: litellm.RateLimitError: BedrockException - {\"message\":\"Too many tokens, please wait before trying again.\"}\n",
      "ERROR:root:LiteLLM Error: litellm.RateLimitError: BedrockException - {\"message\":\"Too many tokens, please wait before trying again.\"} Retrying: 1 time(s)...\n",
      "ERROR:root:Error in LiteLLM generation: litellm.RateLimitError: BedrockException - {\"message\":\"Too many tokens, please wait before trying again.\"}\n",
      "ERROR:root:LiteLLM Error: litellm.RateLimitError: BedrockException - {\"message\":\"Too many tokens, please wait before trying again.\"} Retrying: 2 time(s)...\n",
      "ERROR:root:Error in LiteLLM generation: litellm.RateLimitError: BedrockException - {\"message\":\"Too many tokens, please wait before trying again.\"}\n",
      "ERROR:root:Error in LiteLLM generation: litellm.RateLimitError: BedrockException - {\"message\":\"Too many tokens, please wait before trying again.\"}\n",
      "ERROR:root:LiteLLM Error: litellm.RateLimitError: BedrockException - {\"message\":\"Too many tokens, please wait before trying again.\"} Retrying: 1 time(s)...\n",
      "ERROR:root:LiteLLM Error: litellm.RateLimitError: BedrockException - {\"message\":\"Too many tokens, please wait before trying again.\"} Retrying: 2 time(s)...\n",
      "ERROR:root:Error in LiteLLM generation: litellm.RateLimitError: BedrockException - {\"message\":\"Too many tokens, please wait before trying again.\"}\n",
      "ERROR:root:LiteLLM Error: litellm.RateLimitError: BedrockException - {\"message\":\"Too many tokens, please wait before trying again.\"} Retrying: 2 time(s)...\n",
      "ERROR:root:Error in LiteLLM generation: litellm.RateLimitError: BedrockException - {\"message\":\"Too many tokens, please wait before trying again.\"}\n",
      "ERROR:root:LiteLLM Error: litellm.RateLimitError: BedrockException - {\"message\":\"Too many tokens, please wait before trying again.\"} Retrying: 1 time(s)...\n",
      "ERROR:root:Error in LiteLLM generation: litellm.RateLimitError: BedrockException - {\"message\":\"Too many tokens, please wait before trying again.\"}\n",
      "ERROR:root:LiteLLM Error: litellm.RateLimitError: BedrockException - {\"message\":\"Too many tokens, please wait before trying again.\"} Retrying: 1 time(s)...\n",
      "ERROR:root:Error in LiteLLM generation: litellm.RateLimitError: BedrockException - {\"message\":\"Too many tokens, please wait before trying again.\"}\n",
      "ERROR:root:Error in LiteLLM generation: litellm.RateLimitError: BedrockException - {\"message\":\"Too many tokens, please wait before trying again.\"}\n",
      "ERROR:root:LiteLLM Error: litellm.RateLimitError: BedrockException - {\"message\":\"Too many tokens, please wait before trying again.\"} Retrying: 2 time(s)...\n",
      "ERROR:root:LiteLLM Error: litellm.RateLimitError: BedrockException - {\"message\":\"Too many tokens, please wait before trying again.\"} Retrying: 1 time(s)...\n",
      "ERROR:root:Error in LiteLLM generation: litellm.RateLimitError: BedrockException - {\"message\":\"Too many tokens, please wait before trying again.\"}\n",
      "ERROR:root:Error in LiteLLM generation: litellm.RateLimitError: BedrockException - {\"message\":\"Too many tokens, please wait before trying again.\"}\n",
      "ERROR:root:LiteLLM Error: litellm.RateLimitError: BedrockException - {\"message\":\"Too many tokens, please wait before trying again.\"} Retrying: 3 time(s)...\n",
      "ERROR:root:LiteLLM Error: litellm.RateLimitError: BedrockException - {\"message\":\"Too many tokens, please wait before trying again.\"} Retrying: 2 time(s)...\n",
      "ERROR:root:Error in LiteLLM generation: litellm.RateLimitError: BedrockException - {\"message\":\"Too many tokens, please wait before trying again.\"}\n",
      "ERROR:root:LiteLLM Error: litellm.RateLimitError: BedrockException - {\"message\":\"Too many tokens, please wait before trying again.\"} Retrying: 3 time(s)...\n",
      "ERROR:root:Error in LiteLLM generation: litellm.RateLimitError: BedrockException - {\"message\":\"Too many tokens, please wait before trying again.\"}\n",
      "ERROR:root:LiteLLM Error: litellm.RateLimitError: BedrockException - {\"message\":\"Too many tokens, please wait before trying again.\"} Retrying: 3 time(s)...\n",
      "ERROR:root:Error in LiteLLM generation: litellm.RateLimitError: BedrockException - {\"message\":\"Too many tokens, please wait before trying again.\"}\n",
      "ERROR:root:LiteLLM Error: litellm.RateLimitError: BedrockException - {\"message\":\"Too many tokens, please wait before trying again.\"} Retrying: 3 time(s)...\n",
      "ERROR:root:Error in LiteLLM generation: litellm.RateLimitError: BedrockException - {\"message\":\"Too many tokens, please wait before trying again.\"}\n",
      "ERROR:root:LiteLLM Error: litellm.RateLimitError: BedrockException - {\"message\":\"Too many tokens, please wait before trying again.\"} Retrying: 2 time(s)...\n",
      "ERROR:root:Error in LiteLLM generation: litellm.RateLimitError: BedrockException - {\"message\":\"Too many tokens, please wait before trying again.\"}\n",
      "ERROR:root:LiteLLM Error: litellm.RateLimitError: BedrockException - {\"message\":\"Too many tokens, please wait before trying again.\"} Retrying: 2 time(s)...\n",
      "ERROR:root:Error in LiteLLM generation: litellm.RateLimitError: BedrockException - {\"message\":\"Too many tokens, please wait before trying again.\"}\n",
      "ERROR:root:LiteLLM Error: litellm.RateLimitError: BedrockException - {\"message\":\"Too many tokens, please wait before trying again.\"} Retrying: 1 time(s)...\n",
      "ERROR:root:Error in LiteLLM generation: litellm.RateLimitError: BedrockException - {\"message\":\"Too many tokens, please wait before trying again.\"}\n",
      "ERROR:root:LiteLLM Error: litellm.RateLimitError: BedrockException - {\"message\":\"Too many tokens, please wait before trying again.\"} Retrying: 2 time(s)...\n",
      "ERROR:root:Error in LiteLLM generation: litellm.RateLimitError: BedrockException - {\"message\":\"Too many tokens, please wait before trying again.\"}\n",
      "ERROR:root:LiteLLM Error: litellm.RateLimitError: BedrockException - {\"message\":\"Too many tokens, please wait before trying again.\"} Retrying: 3 time(s)...\n",
      "ERROR:root:Error in LiteLLM generation: litellm.RateLimitError: BedrockException - {\"message\":\"Too many tokens, please wait before trying again.\"}\n",
      "ERROR:root:LiteLLM Error: litellm.RateLimitError: BedrockException - {\"message\":\"Too many tokens, please wait before trying again.\"} Retrying: 1 time(s)...\n",
      "ERROR:root:Error in LiteLLM generation: litellm.RateLimitError: BedrockException - {\"message\":\"Too many tokens, please wait before trying again.\"}\n",
      "ERROR:root:LiteLLM Error: litellm.RateLimitError: BedrockException - {\"message\":\"Too many tokens, please wait before trying again.\"} Retrying: 3 time(s)...\n",
      "ERROR:root:Error in LiteLLM generation: litellm.RateLimitError: BedrockException - {\"message\":\"Too many tokens, please wait before trying again.\"}\n",
      "ERROR:root:LiteLLM Error: litellm.RateLimitError: BedrockException - {\"message\":\"Too many tokens, please wait before trying again.\"} Retrying: 1 time(s)...\n",
      "ERROR:root:Error in LiteLLM generation: litellm.RateLimitError: BedrockException - {\"message\":\"Too many tokens, please wait before trying again.\"}\n",
      "ERROR:root:LiteLLM Error: litellm.RateLimitError: BedrockException - {\"message\":\"Too many tokens, please wait before trying again.\"} Retrying: 2 time(s)...\n",
      "ERROR:root:Error in LiteLLM generation: litellm.RateLimitError: BedrockException - {\"message\":\"Too many tokens, please wait before trying again.\"}\n",
      "ERROR:root:Error in LiteLLM generation: litellm.RateLimitError: BedrockException - {\"message\":\"Too many tokens, please wait before trying again.\"}\n",
      "ERROR:root:LiteLLM Error: litellm.RateLimitError: BedrockException - {\"message\":\"Too many tokens, please wait before trying again.\"} Retrying: 3 time(s)...\n",
      "ERROR:root:LiteLLM Error: litellm.RateLimitError: BedrockException - {\"message\":\"Too many tokens, please wait before trying again.\"} Retrying: 3 time(s)...\n",
      "ERROR:root:Error in LiteLLM generation: litellm.RateLimitError: BedrockException - {\"message\":\"Too many tokens, please wait before trying again.\"}\n",
      "ERROR:root:LiteLLM Error: litellm.RateLimitError: BedrockException - {\"message\":\"Too many tokens, please wait before trying again.\"} Retrying: 3 time(s)...\n",
      "ERROR:root:Error in LiteLLM generation: litellm.RateLimitError: BedrockException - {\"message\":\"Too many tokens, please wait before trying again.\"}\n",
      "ERROR:root:LiteLLM Error: litellm.RateLimitError: BedrockException - {\"message\":\"Too many tokens, please wait before trying again.\"} Retrying: 2 time(s)...\n",
      "ERROR:root:Error in LiteLLM generation: litellm.RateLimitError: BedrockException - {\"message\":\"Too many tokens, please wait before trying again.\"}\n",
      "ERROR:root:LiteLLM Error: litellm.RateLimitError: BedrockException - {\"message\":\"Too many tokens, please wait before trying again.\"} Retrying: 3 time(s)...\n",
      "ERROR:root:Error in LiteLLM generation: litellm.RateLimitError: BedrockException - {\"message\":\"Too many tokens, please wait before trying again.\"}\n",
      "ERROR:root:LiteLLM Error: litellm.RateLimitError: BedrockException - {\"message\":\"Too many tokens, please wait before trying again.\"} Retrying: 3 time(s)...\n",
      "ERROR:root:Error in LiteLLM generation: litellm.RateLimitError: BedrockException - {\"message\":\"Too many tokens, please wait before trying again.\"}\n",
      "ERROR:root:LiteLLM Error: litellm.RateLimitError: BedrockException - {\"message\":\"Too many tokens, please wait before trying again.\"} Retrying: 1 time(s)...\n",
      "ERROR:root:Error in LiteLLM generation: litellm.RateLimitError: BedrockException - {\"message\":\"Too many tokens, please wait before trying again.\"}\n",
      "ERROR:root:LiteLLM Error: litellm.RateLimitError: BedrockException - {\"message\":\"Too many tokens, please wait before trying again.\"} Retrying: 1 time(s)...\n",
      "ERROR:root:Error in LiteLLM generation: litellm.RateLimitError: BedrockException - {\"message\":\"Too many tokens, please wait before trying again.\"}\n",
      "ERROR:root:LiteLLM Error: litellm.RateLimitError: BedrockException - {\"message\":\"Too many tokens, please wait before trying again.\"} Retrying: 3 time(s)...\n",
      "ERROR:root:Error in LiteLLM generation: litellm.RateLimitError: BedrockException - {\"message\":\"Too many tokens, please wait before trying again.\"}\n",
      "ERROR:root:Error in LiteLLM generation: litellm.RateLimitError: BedrockException - {\"message\":\"Too many tokens, please wait before trying again.\"}\n",
      "ERROR:root:LiteLLM Error: litellm.RateLimitError: BedrockException - {\"message\":\"Too many tokens, please wait before trying again.\"} Retrying: 2 time(s)...\n",
      "ERROR:root:LiteLLM Error: litellm.RateLimitError: BedrockException - {\"message\":\"Too many tokens, please wait before trying again.\"} Retrying: 1 time(s)...\n",
      "ERROR:root:Error in LiteLLM generation: litellm.RateLimitError: BedrockException - {\"message\":\"Too many tokens, please wait before trying again.\"}\n",
      "ERROR:root:LiteLLM Error: litellm.RateLimitError: BedrockException - {\"message\":\"Too many tokens, please wait before trying again.\"} Retrying: 3 time(s)...\n",
      "ERROR:root:Error in LiteLLM generation: litellm.RateLimitError: BedrockException - {\"message\":\"Too many tokens, please wait before trying again.\"}\n",
      "ERROR:root:LiteLLM Error: litellm.RateLimitError: BedrockException - {\"message\":\"Too many tokens, please wait before trying again.\"} Retrying: 2 time(s)...\n",
      "ERROR:root:Error in LiteLLM generation: litellm.RateLimitError: BedrockException - {\"message\":\"Too many tokens, please wait before trying again.\"}\n",
      "ERROR:root:LiteLLM Error: litellm.RateLimitError: BedrockException - {\"message\":\"Too many tokens, please wait before trying again.\"} Retrying: 1 time(s)...\n",
      "ERROR:root:Error in LiteLLM generation: litellm.RateLimitError: BedrockException - {\"message\":\"Too many tokens, please wait before trying again.\"}\n",
      "ERROR:root:LiteLLM Error: litellm.RateLimitError: BedrockException - {\"message\":\"Too many tokens, please wait before trying again.\"} Retrying: 2 time(s)...\n",
      "ERROR:root:Error in LiteLLM generation: litellm.RateLimitError: BedrockException - {\"message\":\"Too many tokens, please wait before trying again.\"}\n",
      "ERROR:root:LiteLLM Error: litellm.RateLimitError: BedrockException - {\"message\":\"Too many tokens, please wait before trying again.\"} Retrying: 4 time(s)...\n",
      "ERROR:root:Error in LiteLLM generation: litellm.RateLimitError: BedrockException - {\"message\":\"Too many tokens, please wait before trying again.\"}\n",
      "ERROR:root:LiteLLM Error: litellm.RateLimitError: BedrockException - {\"message\":\"Too many tokens, please wait before trying again.\"} Retrying: 4 time(s)...\n",
      "ERROR:root:Error in LiteLLM generation: litellm.RateLimitError: BedrockException - {\"message\":\"Too many tokens, please wait before trying again.\"}\n",
      "ERROR:root:LiteLLM Error: litellm.RateLimitError: BedrockException - {\"message\":\"Too many tokens, please wait before trying again.\"} Retrying: 3 time(s)...\n",
      "ERROR:root:Error in LiteLLM generation: litellm.RateLimitError: BedrockException - {\"message\":\"Too many tokens, please wait before trying again.\"}\n",
      "ERROR:root:LiteLLM Error: litellm.RateLimitError: BedrockException - {\"message\":\"Too many tokens, please wait before trying again.\"} Retrying: 1 time(s)...\n",
      "ERROR:root:Error in LiteLLM generation: litellm.RateLimitError: BedrockException - {\"message\":\"Too many tokens, please wait before trying again.\"}\n",
      "ERROR:root:LiteLLM Error: litellm.RateLimitError: BedrockException - {\"message\":\"Too many tokens, please wait before trying again.\"} Retrying: 1 time(s)...\n",
      "ERROR:root:Error in LiteLLM generation: litellm.RateLimitError: BedrockException - {\"message\":\"Too many tokens, please wait before trying again.\"}\n",
      "ERROR:root:LiteLLM Error: litellm.RateLimitError: BedrockException - {\"message\":\"Too many tokens, please wait before trying again.\"} Retrying: 4 time(s)...\n",
      "ERROR:root:Error in LiteLLM generation: litellm.RateLimitError: BedrockException - {\"message\":\"Too many tokens, please wait before trying again.\"}\n",
      "ERROR:root:LiteLLM Error: litellm.RateLimitError: BedrockException - {\"message\":\"Too many tokens, please wait before trying again.\"} Retrying: 1 time(s)...\n",
      "ERROR:root:Error in LiteLLM generation: litellm.RateLimitError: BedrockException - {\"message\":\"Too many tokens, please wait before trying again.\"}\n",
      "ERROR:root:LiteLLM Error: litellm.RateLimitError: BedrockException - {\"message\":\"Too many tokens, please wait before trying again.\"} Retrying: 4 time(s)...\n",
      "ERROR:root:Error in LiteLLM generation: litellm.RateLimitError: BedrockException - {\"message\":\"Too many tokens, please wait before trying again.\"}\n",
      "ERROR:root:LiteLLM Error: litellm.RateLimitError: BedrockException - {\"message\":\"Too many tokens, please wait before trying again.\"} Retrying: 4 time(s)...\n",
      "ERROR:root:Error in LiteLLM generation: litellm.RateLimitError: BedrockException - {\"message\":\"Too many tokens, please wait before trying again.\"}\n",
      "ERROR:root:LiteLLM Error: litellm.RateLimitError: BedrockException - {\"message\":\"Too many tokens, please wait before trying again.\"} Retrying: 1 time(s)...\n",
      "ERROR:root:Error in LiteLLM generation: litellm.RateLimitError: BedrockException - {\"message\":\"Too many tokens, please wait before trying again.\"}\n",
      "ERROR:root:LiteLLM Error: litellm.RateLimitError: BedrockException - {\"message\":\"Too many tokens, please wait before trying again.\"} Retrying: 4 time(s)...\n",
      "ERROR:root:Error in LiteLLM generation: litellm.RateLimitError: BedrockException - {\"message\":\"Too many tokens, please wait before trying again.\"}\n",
      "ERROR:root:LiteLLM Error: litellm.RateLimitError: BedrockException - {\"message\":\"Too many tokens, please wait before trying again.\"} Retrying: 1 time(s)...\n",
      "ERROR:root:Error in LiteLLM generation: litellm.RateLimitError: BedrockException - {\"message\":\"Too many tokens, please wait before trying again.\"}\n",
      "ERROR:root:LiteLLM Error: litellm.RateLimitError: BedrockException - {\"message\":\"Too many tokens, please wait before trying again.\"} Retrying: 3 time(s)...\n",
      "ERROR:root:Error in LiteLLM generation: litellm.RateLimitError: BedrockException - {\"message\":\"Too many tokens, please wait before trying again.\"}\n",
      "ERROR:root:LiteLLM Error: litellm.RateLimitError: BedrockException - {\"message\":\"Too many tokens, please wait before trying again.\"} Retrying: 4 time(s)...\n",
      "ERROR:root:Error in LiteLLM generation: litellm.RateLimitError: BedrockException - {\"message\":\"Too many tokens, please wait before trying again.\"}\n",
      "ERROR:root:LiteLLM Error: litellm.RateLimitError: BedrockException - {\"message\":\"Too many tokens, please wait before trying again.\"} Retrying: 1 time(s)...\n",
      "ERROR:root:Error in LiteLLM generation: litellm.RateLimitError: BedrockException - {\"message\":\"Too many tokens, please wait before trying again.\"}\n",
      "ERROR:root:LiteLLM Error: litellm.RateLimitError: BedrockException - {\"message\":\"Too many tokens, please wait before trying again.\"} Retrying: 4 time(s)...\n",
      "ERROR:root:Error in LiteLLM generation: litellm.RateLimitError: BedrockException - {\"message\":\"Too many tokens, please wait before trying again.\"}\n",
      "ERROR:root:LiteLLM Error: litellm.RateLimitError: BedrockException - {\"message\":\"Too many tokens, please wait before trying again.\"} Retrying: 3 time(s)...\n",
      "ERROR:root:Error in LiteLLM generation: litellm.RateLimitError: BedrockException - {\"message\":\"Too many tokens, please wait before trying again.\"}\n",
      "ERROR:root:LiteLLM Error: litellm.RateLimitError: BedrockException - {\"message\":\"Too many tokens, please wait before trying again.\"} Retrying: 1 time(s)...\n",
      "ERROR:root:Error in LiteLLM generation: litellm.RateLimitError: BedrockException - {\"message\":\"Too many tokens, please wait before trying again.\"}\n",
      "ERROR:root:LiteLLM Error: litellm.RateLimitError: BedrockException - {\"message\":\"Too many tokens, please wait before trying again.\"} Retrying: 4 time(s)...\n",
      "ERROR:root:Error in LiteLLM generation: litellm.RateLimitError: BedrockException - {\"message\":\"Too many tokens, please wait before trying again.\"}\n",
      "ERROR:root:LiteLLM Error: litellm.RateLimitError: BedrockException - {\"message\":\"Too many tokens, please wait before trying again.\"} Retrying: 4 time(s)...\n",
      "ERROR:root:Error in LiteLLM generation: litellm.RateLimitError: BedrockException - {\"message\":\"Too many tokens, please wait before trying again.\"}\n",
      "ERROR:root:LiteLLM Error: litellm.RateLimitError: BedrockException - {\"message\":\"Too many tokens, please wait before trying again.\"} Retrying: 4 time(s)...\n",
      "ERROR:root:Error in LiteLLM generation: litellm.RateLimitError: BedrockException - {\"message\":\"Too many tokens, please wait before trying again.\"}\n",
      "ERROR:root:LiteLLM Error: litellm.RateLimitError: BedrockException - {\"message\":\"Too many tokens, please wait before trying again.\"} Retrying: 2 time(s)...\n",
      "ERROR:root:Error in LiteLLM generation: litellm.RateLimitError: BedrockException - {\"message\":\"Too many tokens, please wait before trying again.\"}\n",
      "ERROR:root:LiteLLM Error: litellm.RateLimitError: BedrockException - {\"message\":\"Too many tokens, please wait before trying again.\"} Retrying: 4 time(s)...\n",
      "ERROR:root:Error in LiteLLM generation: litellm.RateLimitError: BedrockException - {\"message\":\"Too many tokens, please wait before trying again.\"}\n",
      "ERROR:root:LiteLLM Error: litellm.RateLimitError: BedrockException - {\"message\":\"Too many tokens, please wait before trying again.\"} Retrying: 2 time(s)...\n",
      "ERROR:root:Error in LiteLLM generation: litellm.RateLimitError: BedrockException - {\"message\":\"Too many tokens, please wait before trying again.\"}\n",
      "ERROR:root:LiteLLM Error: litellm.RateLimitError: BedrockException - {\"message\":\"Too many tokens, please wait before trying again.\"} Retrying: 2 time(s)...\n",
      "ERROR:root:Error in LiteLLM generation: litellm.RateLimitError: BedrockException - {\"message\":\"Too many tokens, please wait before trying again.\"}\n",
      "ERROR:root:LiteLLM Error: litellm.RateLimitError: BedrockException - {\"message\":\"Too many tokens, please wait before trying again.\"} Retrying: 2 time(s)...\n",
      "ERROR:root:Error in LiteLLM generation: litellm.RateLimitError: BedrockException - {\"message\":\"Too many tokens, please wait before trying again.\"}\n",
      "ERROR:root:LiteLLM Error: litellm.RateLimitError: BedrockException - {\"message\":\"Too many tokens, please wait before trying again.\"} Retrying: 1 time(s)...\n",
      "ERROR:root:Error in LiteLLM generation: litellm.RateLimitError: BedrockException - {\"message\":\"Too many tokens, please wait before trying again.\"}\n",
      "ERROR:root:LiteLLM Error: litellm.RateLimitError: BedrockException - {\"message\":\"Too many tokens, please wait before trying again.\"} Retrying: 2 time(s)...\n",
      "ERROR:root:Error in LiteLLM generation: litellm.RateLimitError: BedrockException - {\"message\":\"Too many tokens, please wait before trying again.\"}\n",
      "ERROR:root:LiteLLM Error: litellm.RateLimitError: BedrockException - {\"message\":\"Too many tokens, please wait before trying again.\"} Retrying: 1 time(s)...\n",
      "ERROR:root:Error in LiteLLM generation: litellm.RateLimitError: BedrockException - {\"message\":\"Too many tokens, please wait before trying again.\"}\n",
      "ERROR:root:LiteLLM Error: litellm.RateLimitError: BedrockException - {\"message\":\"Too many tokens, please wait before trying again.\"} Retrying: 4 time(s)...\n",
      "ERROR:root:Error in LiteLLM generation: litellm.RateLimitError: BedrockException - {\"message\":\"Too many tokens, please wait before trying again.\"}\n",
      "ERROR:root:LiteLLM Error: litellm.RateLimitError: BedrockException - {\"message\":\"Too many tokens, please wait before trying again.\"} Retrying: 3 time(s)...\n",
      "ERROR:root:Error in LiteLLM generation: litellm.RateLimitError: BedrockException - {\"message\":\"Too many tokens, please wait before trying again.\"}\n",
      "ERROR:root:LiteLLM Error: litellm.RateLimitError: BedrockException - {\"message\":\"Too many tokens, please wait before trying again.\"} Retrying: 1 time(s)...\n",
      "ERROR:root:Error in LiteLLM generation: litellm.RateLimitError: BedrockException - {\"message\":\"Too many tokens, please wait before trying again.\"}\n",
      "ERROR:root:LiteLLM Error: litellm.RateLimitError: BedrockException - {\"message\":\"Too many tokens, please wait before trying again.\"} Retrying: 2 time(s)...\n",
      "ERROR:root:Error in LiteLLM generation: litellm.RateLimitError: BedrockException - {\"message\":\"Too many tokens, please wait before trying again.\"}\n",
      "ERROR:root:LiteLLM Error: litellm.RateLimitError: BedrockException - {\"message\":\"Too many tokens, please wait before trying again.\"} Retrying: 3 time(s)...\n",
      "ERROR:root:Error in LiteLLM generation: litellm.RateLimitError: BedrockException - {\"message\":\"Too many tokens, please wait before trying again.\"}\n",
      "ERROR:root:LiteLLM Error: litellm.RateLimitError: BedrockException - {\"message\":\"Too many tokens, please wait before trying again.\"} Retrying: 4 time(s)...\n",
      "ERROR:root:Error in LiteLLM generation: litellm.RateLimitError: BedrockException - {\"message\":\"Too many tokens, please wait before trying again.\"}\n",
      "ERROR:root:LiteLLM Error: litellm.RateLimitError: BedrockException - {\"message\":\"Too many tokens, please wait before trying again.\"} Retrying: 2 time(s)...\n",
      "ERROR:root:Error in LiteLLM generation: litellm.RateLimitError: BedrockException - {\"message\":\"Too many tokens, please wait before trying again.\"}\n",
      "ERROR:root:Error in LiteLLM generation: litellm.RateLimitError: BedrockException - {\"message\":\"Too many tokens, please wait before trying again.\"}\n",
      "ERROR:root:LiteLLM Error: litellm.RateLimitError: BedrockException - {\"message\":\"Too many tokens, please wait before trying again.\"} Retrying: 3 time(s)...\n",
      "ERROR:root:LiteLLM Error: litellm.RateLimitError: BedrockException - {\"message\":\"Too many tokens, please wait before trying again.\"} Retrying: 3 time(s)...\n",
      "ERROR:root:Error in LiteLLM generation: litellm.RateLimitError: BedrockException - {\"message\":\"Too many tokens, please wait before trying again.\"}\n",
      "ERROR:root:LiteLLM Error: litellm.RateLimitError: BedrockException - {\"message\":\"Too many tokens, please wait before trying again.\"} Retrying: 4 time(s)...\n",
      "ERROR:root:Error in LiteLLM generation: litellm.RateLimitError: BedrockException - {\"message\":\"Too many tokens, please wait before trying again.\"}\n",
      "ERROR:root:LiteLLM Error: litellm.RateLimitError: BedrockException - {\"message\":\"Too many tokens, please wait before trying again.\"} Retrying: 3 time(s)...\n",
      "ERROR:root:Error in LiteLLM generation: litellm.RateLimitError: BedrockException - {\"message\":\"Too many tokens, please wait before trying again.\"}\n",
      "ERROR:root:LiteLLM Error: litellm.RateLimitError: BedrockException - {\"message\":\"Too many tokens, please wait before trying again.\"} Retrying: 3 time(s)...\n",
      "ERROR:root:Error in LiteLLM generation: litellm.RateLimitError: BedrockException - {\"message\":\"Too many tokens, please wait before trying again.\"}\n",
      "ERROR:root:LiteLLM Error: litellm.RateLimitError: BedrockException - {\"message\":\"Too many tokens, please wait before trying again.\"} Retrying: 5 time(s)...\n",
      "ERROR:root:Error in LiteLLM generation: litellm.RateLimitError: BedrockException - {\"message\":\"Too many tokens, please wait before trying again.\"}\n",
      "ERROR:root:LiteLLM Error: litellm.RateLimitError: BedrockException - {\"message\":\"Too many tokens, please wait before trying again.\"} Retrying: 5 time(s)...\n",
      "ERROR:root:Error in LiteLLM generation: litellm.RateLimitError: BedrockException - {\"message\":\"Too many tokens, please wait before trying again.\"}\n",
      "ERROR:root:LiteLLM Error: litellm.RateLimitError: BedrockException - {\"message\":\"Too many tokens, please wait before trying again.\"} Retrying: 5 time(s)...\n",
      "ERROR:root:Error in LiteLLM generation: litellm.RateLimitError: BedrockException - {\"message\":\"Too many tokens, please wait before trying again.\"}\n",
      "ERROR:root:LiteLLM Error: litellm.RateLimitError: BedrockException - {\"message\":\"Too many tokens, please wait before trying again.\"} Retrying: 4 time(s)...\n",
      "ERROR:root:Error in LiteLLM generation: litellm.RateLimitError: BedrockException - {\"message\":\"Too many tokens, please wait before trying again.\"}\n",
      "ERROR:root:LiteLLM Error: litellm.RateLimitError: BedrockException - {\"message\":\"Too many tokens, please wait before trying again.\"} Retrying: 5 time(s)...\n",
      "ERROR:root:Error in LiteLLM generation: litellm.RateLimitError: BedrockException - {\"message\":\"Too many tokens, please wait before trying again.\"}\n",
      "ERROR:root:LiteLLM Error: litellm.RateLimitError: BedrockException - {\"message\":\"Too many tokens, please wait before trying again.\"} Retrying: 1 time(s)...\n",
      "ERROR:root:Error in LiteLLM generation: litellm.RateLimitError: BedrockException - {\"message\":\"Too many tokens, please wait before trying again.\"}\n",
      "ERROR:root:LiteLLM Error: litellm.RateLimitError: BedrockException - {\"message\":\"Too many tokens, please wait before trying again.\"} Retrying: 5 time(s)...\n",
      "ERROR:root:Error in LiteLLM generation: litellm.RateLimitError: BedrockException - {\"message\":\"Too many tokens, please wait before trying again.\"}\n",
      "ERROR:root:LiteLLM Error: litellm.RateLimitError: BedrockException - {\"message\":\"Too many tokens, please wait before trying again.\"} Retrying: 5 time(s)...\n",
      "ERROR:root:Error in LiteLLM generation: litellm.RateLimitError: BedrockException - {\"message\":\"Too many tokens, please wait before trying again.\"}\n",
      "ERROR:root:LiteLLM Error: litellm.RateLimitError: BedrockException - {\"message\":\"Too many tokens, please wait before trying again.\"} Retrying: 5 time(s)...\n",
      "ERROR:root:Error in LiteLLM generation: litellm.RateLimitError: BedrockException - {\"message\":\"Too many tokens, please wait before trying again.\"}\n",
      "ERROR:root:LiteLLM Error: litellm.RateLimitError: BedrockException - {\"message\":\"Too many tokens, please wait before trying again.\"} Retrying: 4 time(s)...\n",
      "ERROR:root:Error in LiteLLM generation: litellm.RateLimitError: BedrockException - {\"message\":\"Too many tokens, please wait before trying again.\"}\n",
      "ERROR:root:LiteLLM Error: litellm.RateLimitError: BedrockException - {\"message\":\"Too many tokens, please wait before trying again.\"} Retrying: 4 time(s)...\n",
      "ERROR:root:Error in LiteLLM generation: litellm.RateLimitError: BedrockException - {\"message\":\"Too many tokens, please wait before trying again.\"}\n",
      "ERROR:root:Error in LiteLLM generation: litellm.RateLimitError: BedrockException - {\"message\":\"Too many tokens, please wait before trying again.\"}\n",
      "ERROR:root:LiteLLM Error: litellm.RateLimitError: BedrockException - {\"message\":\"Too many tokens, please wait before trying again.\"} Retrying: 5 time(s)...\n",
      "ERROR:root:LiteLLM Error: litellm.RateLimitError: BedrockException - {\"message\":\"Too many tokens, please wait before trying again.\"} Retrying: 5 time(s)...\n",
      "ERROR:root:Error in LiteLLM generation: litellm.RateLimitError: BedrockException - {\"message\":\"Too many tokens, please wait before trying again.\"}\n",
      "ERROR:root:LiteLLM Error: litellm.RateLimitError: BedrockException - {\"message\":\"Too many tokens, please wait before trying again.\"} Retrying: 4 time(s)...\n",
      "ERROR:root:Error in LiteLLM generation: litellm.RateLimitError: BedrockException - {\"message\":\"Too many tokens, please wait before trying again.\"}\n",
      "ERROR:root:LiteLLM Error: litellm.RateLimitError: BedrockException - {\"message\":\"Too many tokens, please wait before trying again.\"} Retrying: 5 time(s)...\n",
      "ERROR:root:Error in LiteLLM generation: litellm.RateLimitError: BedrockException - {\"message\":\"Too many tokens, please wait before trying again.\"}\n",
      "ERROR:root:LiteLLM Error: litellm.RateLimitError: BedrockException - {\"message\":\"Too many tokens, please wait before trying again.\"} Retrying: 4 time(s)...\n",
      "ERROR:root:Error in LiteLLM generation: litellm.RateLimitError: BedrockException - {\"message\":\"Too many tokens, please wait before trying again.\"}\n",
      "ERROR:root:LiteLLM Error: litellm.RateLimitError: BedrockException - {\"message\":\"Too many tokens, please wait before trying again.\"} Retrying: 1 time(s)...\n",
      "ERROR:root:Error in LiteLLM generation: litellm.RateLimitError: BedrockException - {\"message\":\"Too many tokens, please wait before trying again.\"}\n",
      "ERROR:root:LiteLLM Error: litellm.RateLimitError: BedrockException - {\"message\":\"Too many tokens, please wait before trying again.\"} Retrying: 5 time(s)...\n",
      "ERROR:root:Error in LiteLLM generation: litellm.RateLimitError: BedrockException - {\"message\":\"Too many tokens, please wait before trying again.\"}\n",
      "ERROR:root:LiteLLM Error: litellm.RateLimitError: BedrockException - {\"message\":\"Too many tokens, please wait before trying again.\"} Retrying: 6 time(s)...\n",
      "ERROR:root:Error in LiteLLM generation: litellm.RateLimitError: BedrockException - {\"message\":\"Too many tokens, please wait before trying again.\"}\n",
      "ERROR:root:LiteLLM Error: litellm.RateLimitError: BedrockException - {\"message\":\"Too many tokens, please wait before trying again.\"} Retrying: 6 time(s)...\n",
      "ERROR:root:Error in LiteLLM generation: litellm.RateLimitError: BedrockException - {\"message\":\"Too many tokens, please wait before trying again.\"}\n",
      "ERROR:root:LiteLLM Error: litellm.RateLimitError: BedrockException - {\"message\":\"Too many tokens, please wait before trying again.\"} Retrying: 6 time(s)...\n",
      "ERROR:root:Error in LiteLLM generation: litellm.RateLimitError: BedrockException - {\"message\":\"Too many tokens, please wait before trying again.\"}\n",
      "ERROR:root:LiteLLM Error: litellm.RateLimitError: BedrockException - {\"message\":\"Too many tokens, please wait before trying again.\"} Retrying: 6 time(s)...\n",
      "ERROR:root:Error in LiteLLM generation: litellm.RateLimitError: BedrockException - {\"message\":\"Too many tokens, please wait before trying again.\"}\n",
      "ERROR:root:LiteLLM Error: litellm.RateLimitError: BedrockException - {\"message\":\"Too many tokens, please wait before trying again.\"} Retrying: 5 time(s)...\n",
      "ERROR:root:Error in LiteLLM generation: litellm.RateLimitError: BedrockException - {\"message\":\"Too many tokens, please wait before trying again.\"}\n",
      "ERROR:root:LiteLLM Error: litellm.RateLimitError: BedrockException - {\"message\":\"Too many tokens, please wait before trying again.\"} Retrying: 6 time(s)...\n",
      "ERROR:root:Error in LiteLLM generation: litellm.RateLimitError: BedrockException - {\"message\":\"Too many tokens, please wait before trying again.\"}\n",
      "ERROR:root:LiteLLM Error: litellm.RateLimitError: BedrockException - {\"message\":\"Too many tokens, please wait before trying again.\"} Retrying: 1 time(s)...\n",
      "ERROR:root:Error in LiteLLM generation: litellm.RateLimitError: BedrockException - {\"message\":\"Too many tokens, please wait before trying again.\"}\n",
      "ERROR:root:LiteLLM Error: litellm.RateLimitError: BedrockException - {\"message\":\"Too many tokens, please wait before trying again.\"} Retrying: 5 time(s)...\n",
      "ERROR:root:Error in LiteLLM generation: litellm.RateLimitError: BedrockException - {\"message\":\"Too many tokens, please wait before trying again.\"}\n",
      "ERROR:root:LiteLLM Error: litellm.RateLimitError: BedrockException - {\"message\":\"Too many tokens, please wait before trying again.\"} Retrying: 5 time(s)...\n",
      "ERROR:root:Error in LiteLLM generation: litellm.RateLimitError: BedrockException - {\"message\":\"Too many tokens, please wait before trying again.\"}\n",
      "ERROR:root:LiteLLM Error: litellm.RateLimitError: BedrockException - {\"message\":\"Too many tokens, please wait before trying again.\"} Retrying: 6 time(s)...\n",
      "ERROR:root:Error in LiteLLM generation: litellm.RateLimitError: BedrockException - {\"message\":\"Too many tokens, please wait before trying again.\"}\n",
      "ERROR:root:LiteLLM Error: litellm.RateLimitError: BedrockException - {\"message\":\"Too many tokens, please wait before trying again.\"} Retrying: 1 time(s)...\n",
      "ERROR:root:Error in LiteLLM generation: litellm.RateLimitError: BedrockException - {\"message\":\"Too many tokens, please wait before trying again.\"}\n",
      "ERROR:root:LiteLLM Error: litellm.RateLimitError: BedrockException - {\"message\":\"Too many tokens, please wait before trying again.\"} Retrying: 6 time(s)...\n",
      "ERROR:root:Error in LiteLLM generation: litellm.RateLimitError: BedrockException - {\"message\":\"Too many tokens, please wait before trying again.\"}\n",
      "ERROR:root:LiteLLM Error: litellm.RateLimitError: BedrockException - {\"message\":\"Too many tokens, please wait before trying again.\"} Retrying: 1 time(s)...\n",
      "ERROR:root:Error in LiteLLM generation: litellm.RateLimitError: BedrockException - {\"message\":\"Too many tokens, please wait before trying again.\"}\n",
      "ERROR:root:LiteLLM Error: litellm.RateLimitError: BedrockException - {\"message\":\"Too many tokens, please wait before trying again.\"} Retrying: 1 time(s)...\n",
      "ERROR:root:Error in LiteLLM generation: litellm.RateLimitError: BedrockException - {\"message\":\"Too many tokens, please wait before trying again.\"}\n",
      "ERROR:root:LiteLLM Error: litellm.RateLimitError: BedrockException - {\"message\":\"Too many tokens, please wait before trying again.\"} Retrying: 6 time(s)...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<!DOCTYPE html>\n",
       "<html lang=\"en\">\n",
       "<head>\n",
       "    <title>Evaluation output</title>\n",
       "    <meta charset=\"UTF-8\">\n",
       "    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n",
       "    <style>\n",
       "        body {\n",
       "            font-family: Arial, sans-serif;\n",
       "        }\n",
       "\n",
       "        .header {\n",
       "            a.button {\n",
       "                padding: 4px 8px;\n",
       "                line-height: 20px;\n",
       "                box-shadow: none;\n",
       "                height: 20px;\n",
       "                display: inline-flex;\n",
       "                align-items: center;\n",
       "                justify-content: center;\n",
       "                vertical-align: middle;\n",
       "                background-color: rgb(34, 114, 180);\n",
       "                color: rgb(255, 255, 255);\n",
       "                text-decoration: none;\n",
       "                animation-duration: 0s;\n",
       "                transition: none 0s ease 0s;\n",
       "                position: relative;\n",
       "                white-space: nowrap;\n",
       "                text-align: center;\n",
       "                border: 1px solid rgb(192, 205, 216);\n",
       "                cursor: pointer;\n",
       "                user-select: none;\n",
       "                touch-action: manipulation;\n",
       "                border-radius: 4px;\n",
       "                gap: 6px;\n",
       "            }\n",
       "\n",
       "            a.button:hover {\n",
       "                background-color: rgb(14, 83, 139) !important;\n",
       "                border-color: transparent !important;\n",
       "                color: rgb(255, 255, 255) !important;\n",
       "            }\n",
       "        }\n",
       "\n",
       "        .warnings-section {\n",
       "            margin-top: 8px;\n",
       "\n",
       "            ul {\n",
       "                list-style-type: none;\n",
       "            }\n",
       "        }\n",
       "\n",
       "        .instructions-section {\n",
       "            margin-top: 16px;\n",
       "            font-size: 14px;\n",
       "\n",
       "            ul {\n",
       "                margin-top: 0;\n",
       "                margin-bottom: 0;\n",
       "            }\n",
       "        }\n",
       "\n",
       "        code {\n",
       "            font-family: monospace;\n",
       "        }\n",
       "\n",
       "        .note {\n",
       "            color: #666;\n",
       "        }\n",
       "\n",
       "        a {\n",
       "            color: #2272B4;\n",
       "            text-decoration: none;\n",
       "        }\n",
       "\n",
       "        a:hover {\n",
       "            color: #005580;\n",
       "        }\n",
       "    </style>\n",
       "</head>\n",
       "<body>\n",
       "<div>\n",
       "    <div class=\"header\">\n",
       "        <a href=\"https://mlflow.sagemaker.us-west-2.app.aws/#/experiments/11/evaluation-runs?selectedRunUuid=e4082ff63cc84787b74a9e65af72d57b\" class=\"button\">\n",
       "            View evaluation results in MLflow\n",
       "            <svg xmlns=\"http://www.w3.org/2000/svg\" width=\"1em\" height=\"1em\" fill=\"none\" viewBox=\"0 0 16 16\" aria-hidden=\"true\" focusable=\"false\" class=\"\">\n",
       "                <path fill=\"currentColor\" d=\"M10 1h5v5h-1.5V3.56L8.53 8.53 7.47 7.47l4.97-4.97H10z\"></path>\n",
       "                <path fill=\"currentColor\" d=\"M1 2.75A.75.75 0 0 1 1.75 2H8v1.5H2.5v10h10V8H14v6.25a.75.75 0 0 1-.75.75H1.75a.75.75 0 0 1-.75-.75z\"></path>\n",
       "            </svg>\n",
       "        </a>\n",
       "    </div>\n",
       "</div>\n",
       "</body>\n",
       "</html>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Perform evaluation using the mlflow and the defined scorers\n",
    "# Ignore warnings, RateLimitError and residual errors.\n",
    "# Evaluate 10 samples for quick testing. \n",
    "results = mlflow.genai.evaluate(\n",
    "        data=eval_dataset[0:10],\n",
    "        predict_fn=qa_predict_fn,\n",
    "        scorers=scorers,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dc3e3ba-87b6-48c9-a9e1-e04a8fa830d4",
   "metadata": {},
   "source": [
    "# Results\n",
    "With the evaluation run completed, navigate to your SageMaker AI MLflow App from the SageMaker AI console.\n",
    "\n",
    "From there you can:\n",
    "\n",
    "- Open the mlflow experiment you configured (The default populated in this notebook was `sagemaker-medical-fine-tune-llm`).\n",
    "- Compare runs for different LLM versions, prompt templates, or hyperparameters.\n",
    "- Drill into the **Traces** tab to:\n",
    "  - Inspect per‑sample latency and token usage across the candidate model and judge calls.\n",
    "  - See scorer‑level spans (including DeepEval and RAGAS scorers) with inputs and outputs.\n",
    "- Review aggregated metrics for all scorers, including latency, heuristic NLP metrics, retrieval metrics, and LLM‑as‑a‑judge scores, in a single place.\n",
    "\n",
    "This integrated view helps you operationalize LLM evaluation for domain‑specific models, such as a fine‑tuned Qwen medical assistant, and makes it straightforward to standardize evaluation criteria across teams and projects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "500e96eb-01d9-4803-96c3-b55d96b4eba8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
