{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1c3389a7-345b-41be-b610-b345f59ee3c6",
   "metadata": {},
   "source": [
    "# Experimentation jupyter notebook for SageMaker AI Endpoint Inference Monitoring with SageMaker AI MLflow\n",
    "This is a notebook to experiment with the concepts used in SageMaker AI Endpoint Inference Monitoring custom solution.\n",
    "\n",
    "> IMPORTANT: This notebook is provided for education and experimentation. See the CDK code for production ready solution.\n",
    "> This jupyter notebook for tested in SageMakerAI Studio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63c683df-4137-4f6e-97e4-0236368b6236",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required dependencies. Ignore any warnings and residual dependency errors.\n",
    "!pip install --force-reinstall -U -r requirements.txt --quiet  --no-warn-conflicts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5facbc9-2e0d-45a6-922f-65bd1e9bd8ed",
   "metadata": {},
   "source": [
    "# Import dependencies and set required variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9ccf991-2979-4e02-8c5e-1bee785a5ef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker.jumpstart.model import JumpStartModel\n",
    "from sagemaker.model_monitor import DataCaptureConfig\n",
    "from sagemaker.serializers import JSONSerializer\n",
    "from sagemaker.deserializers import JSONDeserializer\n",
    "import mlflow\n",
    "from datetime import datetime\n",
    "\n",
    "# Initialize SageMaker session\n",
    "sagemaker_session = sagemaker.Session()\n",
    "role = sagemaker.get_execution_role()\n",
    "region = boto3.Session().region_name\n",
    "sagemaker_bucket = sagemaker_session.default_bucket()\n",
    "\n",
    "print(f\"SageMaker role: {role}\")\n",
    "print(f\"Region: {region}\")\n",
    "print(f\"Default bucket: {sagemaker_bucket}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60869984-c1ba-46d5-a17d-b04794d43a8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# S3 folder key to store data capture jsonl files\n",
    "data_capture_s3_key = f\"llama-3-1-8b-instruct-{datetime.now().strftime('%Y-%m-%d-%H-%M-%S')}\"\n",
    "\n",
    "# Deploy Llama 3.1 8B Instruct from JumpStart\n",
    "model_id = \"meta-textgeneration-llama-3-1-8b-instruct\"\n",
    "model_version = \"*\"  # Use latest version\n",
    "\n",
    "# Deploy with data capture enabled\n",
    "endpoint_name = f\"llama-3-1-8b-instruct-{datetime.now().strftime('%Y-%m-%d-%H-%M-%S')}\"\n",
    "\n",
    "# Enter your desired S3 bucket\n",
    "bucket = sagemaker_bucket # REPLACE if not default bucket\n",
    "\n",
    "print(f\"SageMaker data capture: {data_capture_s3_key}\")\n",
    "print(f\"SageMaker endpoint name: {endpoint_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a26f6c5-bd76-4611-adac-92634f3871e5",
   "metadata": {},
   "source": [
    "## Data capture configuration\n",
    "> Note: We will use the default bucket configured with sagemaker studio "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28c27d4a-a35e-4de5-b081-c0ad5006dc12",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_capture_config = DataCaptureConfig(\n",
    "    enable_capture=True,\n",
    "    sampling_percentage=100,  # Capture 100% of requests\n",
    "    destination_s3_uri=f's3://{bucket}/{data_capture_s3_key}',\n",
    "    capture_options=[\"REQUEST\", \"RESPONSE\"],\n",
    "    json_content_types=[\"application/json\"],\n",
    ")\n",
    "\n",
    "print(\"Data capture will be saved to:\", data_capture_config.destination_s3_uri)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a92793c-69b0-4576-a67f-aefbe3a405ea",
   "metadata": {},
   "source": [
    "## Deploy LLM to SageMaker AI Endpoint\n",
    "1. Load LLM from SageMaker Jumpstart\n",
    "2. Deploy the Jumpstart LLM to SageMakerAI Endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4787be1b-0bde-48d6-9926-2b8f79e59417",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Deploying {model_id}...\")\n",
    "\n",
    "# Create JumpStart model\n",
    "model = JumpStartModel(\n",
    "    model_id=model_id,\n",
    "    model_version=model_version,\n",
    "    role=role,\n",
    "    sagemaker_session=sagemaker_session\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b6edb41-5f08-4c34-9e59-57adc86c5c71",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = model.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type=\"ml.g5.2xlarge\",  # GPU instance for Llama\n",
    "    endpoint_name=endpoint_name,\n",
    "    data_capture_config=data_capture_config, # Enable the data capture config\n",
    "    serializer=JSONSerializer(),\n",
    "    deserializer=JSONDeserializer(),\n",
    "    accept_eula=True,\n",
    ")\n",
    "\n",
    "print(f\"✅ Endpoint deployed successfully: {endpoint_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b8864b8-4fde-44eb-a13d-ee4564658c9c",
   "metadata": {},
   "source": [
    "## Configure SageMakerAI MLflow app connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b1c8c98-9a96-4e9d-bf37-f38ae5e44cc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Update this if you want to use a specific MLflow app\n",
    "mlflow_app_name = 'DefaultMLFlowApp'\n",
    "\n",
    "# Get MLflow app ARN\n",
    "sm_client = boto3.client('sagemaker', region_name=region)\n",
    "mlflow_list = sm_client.list_mlflow_apps()\n",
    "\n",
    "print(f'Found {len(mlflow_list[\"Summaries\"])} MLflow app(s) in your account:')\n",
    "for app in mlflow_list['Summaries']:\n",
    "    print(f'  - {app[\"Name\"]}')\n",
    "\n",
    "# Find the specified MLflow app\n",
    "mlflow_app_arn = None\n",
    "for mlflow_app in mlflow_list['Summaries']:\n",
    "    if mlflow_app['Name'] == mlflow_app_name:\n",
    "        mlflow_app_arn = mlflow_app['Arn']\n",
    "        break\n",
    "\n",
    "if mlflow_app_arn:\n",
    "    print(f'\\n Using MLflow app: {mlflow_app_name}')\n",
    "    print(f'  ARN: {mlflow_app_arn}')\n",
    "else:\n",
    "    raise ValueError(f'MLflow app \"{mlflow_app_name}\" not found. Please check the name or create one in SageMaker Studio.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efab8acf-31b8-405f-ae8f-c4fc8c02ee02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set MLflow tracking URI and create/select experiment\n",
    "mlflow.set_tracking_uri(mlflow_app_arn)\n",
    "mlflow_experiment_name = endpoint_name # We will keep it same as the endpoint name. You can replace.\n",
    "\n",
    "try:\n",
    "    # Try to create a new experiment\n",
    "    experiment_id = mlflow.create_experiment(mlflow_experiment_name)\n",
    "    print(f' Created new MLflow experiment: {mlflow_experiment_name}')\n",
    "    print(f'  Experiment ID: {experiment_id}')\n",
    "except:\n",
    "    # Experiment already exists, set it as active\n",
    "    mlflow.set_experiment(mlflow_experiment_name)\n",
    "    experiment = mlflow.get_experiment_by_name(mlflow_experiment_name)\n",
    "    print(f' Using existing MLflow experiment: {mlflow_experiment_name}')\n",
    "    print(f'  Experiment ID: {experiment.experiment_id}')\n",
    "\n",
    "print('\\n You can view your experiments in the SageMaker Studio MLflow App UI')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3f7e9ca-a211-4da6-a9d2-6d53a48cea2d",
   "metadata": {},
   "source": [
    "## Test your sagemaker endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdabacfc-052f-4884-bb9d-45472474181c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function to invoke your sagemaker endpoint\n",
    "import json\n",
    "import boto3\n",
    "from datetime import datetime\n",
    "\n",
    "sagemaker_runtime = boto3.client('sagemaker-runtime')\n",
    "\n",
    "def invoke_sagemaker_endpoint(prompt, endpoint_name, max_tokens=512, temperature=0.7):\n",
    "    \"\"\"\n",
    "    Invoke  endpoint\n",
    "    \"\"\"\n",
    "    payload = {\n",
    "        \"inputs\": prompt,\n",
    "        \"parameters\": {\n",
    "            \"max_new_tokens\": max_tokens,\n",
    "            \"temperature\": temperature,\n",
    "            \"top_p\": 0.9,\n",
    "            \"do_sample\": True\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    start_time = datetime.now()\n",
    "    \n",
    "    response = sagemaker_runtime.invoke_endpoint(\n",
    "        EndpointName=endpoint_name,\n",
    "        ContentType='application/json',\n",
    "        Body=json.dumps(payload)\n",
    "    )\n",
    "    \n",
    "    end_time = datetime.now()\n",
    "    latency = (end_time - start_time).total_seconds() * 1000\n",
    "    \n",
    "    result = json.loads(response['Body'].read().decode())\n",
    "    \n",
    "    print(f\"Latency: {latency:.2f}ms\")\n",
    "    print(f\"Response: {result}\")\n",
    "    \n",
    "    return result, latency\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2add297f-d8df-4557-92b1-e97592f9b966",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with sample prompts\n",
    "test_prompts = [\n",
    "    \"What is machine learning?\",\n",
    "    \"Explain the difference between supervised and unsupervised learning.\",\n",
    "    \"Write a Python function to calculate fibonacci numbers.\"\n",
    "]\n",
    "\n",
    "for prompt in test_prompts:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Prompt: {prompt}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    response, latency = invoke_sagemaker_endpoint(prompt, endpoint_name)\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44894536-1c86-4dc1-a7ed-785edce7e5f1",
   "metadata": {},
   "source": [
    "## Verify data capture jsonl file creation\n",
    "> IMPORTANT: It will take a few seconds from the data capture jsonl file to land in the S3 bucket.\n",
    "> \n",
    "> You will need to retry after few seconds if you don't see jsonl file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7582e84-aa4a-4863-9315-4d224e7117b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_client = boto3.client('s3')\n",
    "response = s3_client.list_objects_v2(\n",
    "    Bucket=bucket,\n",
    "    Prefix=data_capture_s3_key,\n",
    "    MaxKeys=50\n",
    ")\n",
    "\n",
    "if 'Contents' in response:\n",
    "    print(f\"Found {len(response['Contents'])} capture files\")\n",
    "    for obj in response['Contents']:\n",
    "        print(f\"   {obj['Key']}\")\n",
    "        s3_file_key = obj['Key']\n",
    "else:\n",
    "    print(\"No capture files yet. Retry after few seconds\")\n",
    "\n",
    "print(\"Final data capture json file: {s3_file_key}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0638a3f-2097-4700-9b7b-401069ca4d58",
   "metadata": {},
   "source": [
    "## Log MLflow traces from SageMaker DataCapture "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7584cd93-0950-4056-879c-02cdc902dbac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "import awswrangler as wr\n",
    "import pandas as pd\n",
    "\n",
    "def log_single_inference_event(event_data, additional_trace_attr):\n",
    "    \"\"\"\n",
    "    Log a single inference event with MLflow trace decorator\n",
    "    \"\"\"\n",
    "    print(f'Recieved {event_data}')\n",
    "    # Extract data\n",
    "    event_id = event_data['eventMetadata']['eventId']\n",
    "    inference_time = event_data['eventMetadata']['inferenceTime']\n",
    "    \n",
    "    # Parse input/output\n",
    "    input_data = json.loads(event_data['captureData']['endpointInput']['data'])\n",
    "\n",
    "    # Parse output\n",
    "    endpoint_output = event_data['captureData']['endpointOutput']\n",
    "    data = endpoint_output['data']\n",
    "    output_encoding = endpoint_output.get('encoding', 'JSON')\n",
    "    \n",
    "    # Decode and parse\n",
    "    output_data = json.loads(base64.b64decode(data).decode('utf-8')) if output_encoding == 'BASE64' else json.loads(data)\n",
    "    output_type = event_data['captureData']['endpointOutput']['observedContentType']\n",
    "    print(f\"Trace input data: {input_data} /n\")\n",
    "    print(f\"Trace output data: {output_data} /n\")\n",
    "    \n",
    "    # Create trace data\n",
    "    with mlflow.start_span(name=additional_trace_attr['s3_file_key']) as span:\n",
    "        span.set_inputs(\n",
    "            {\n",
    "                \"prompt\": input_data.get('inputs', ''),\n",
    "                \"parameters\": input_data.get('parameters', {})\n",
    "            }\n",
    "        )\n",
    "        span.set_attributes(\n",
    "            event_data['eventMetadata']\n",
    "        )\n",
    "        span.set_attributes(\n",
    "            additional_trace_attr\n",
    "        )\n",
    "        if output_type == None:\n",
    "            span.set_status(\"ERROR\")\n",
    "        # Update\n",
    "        span.set_outputs(\n",
    "            output_data\n",
    "        )\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaeeba88-12db-40e0-ac98-cb2e36a6bcf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper processing function\n",
    "def process_capture_file_events(bucket, s3_file_key):\n",
    "    path = f's3://{bucket}/{s3_file_key}'\n",
    "    # define custom metadata as needed\n",
    "    additional_trace_attr = {\n",
    "        \"s3_bucket.name\":bucket,\n",
    "        \"s3_file_key\":s3_file_key,\n",
    "        \"sagemaker.endpoint_name\":endpoint_name,\n",
    "    }\n",
    "    mlflow.set_experiment(mlflow_experiment_name)\n",
    "    with mlflow.start_run(run_name=\"sagemaker_inference_logging\"):\n",
    "        df = wr.s3.read_json(path=path, lines=True)\n",
    "        \n",
    "        for idx, row in df.iterrows():\n",
    "            result = log_single_inference_event(row.to_dict(), additional_trace_attr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48162f55-27e3-40e3-99b2-bbbf403e575c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run\n",
    "process_capture_file_events(bucket, s3_file_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f419703f-44f0-4968-aca2-6ad16be2c952",
   "metadata": {},
   "source": [
    "# MLflow LLM Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05a9b2c5-651c-4c2f-9499-8ada9db53694",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# Set IAM Role for the Amazon Bedrock model to assume\n",
    "os.environ[\"AWS_ROLE_ARN\"] = sagemaker.get_execution_role()\n",
    "print(os.environ[\"AWS_ROLE_ARN\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93b65ebf-e836-4b8d-b366-6205d85e64d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the Amazon Bedrock model ID to use as the LLM evaluator\n",
    "MLFLOW_EVALUATION_MODEL_ID = \"bedrock:/global.anthropic.claude-sonnet-4-20250514-v1:0\"\n",
    "MLFLOW_EVALUATION_MODEL_PARAM = {\n",
    "    \"temperature\": 0, # 0 for deterministic\n",
    "    \"max_tokens\": 512, # 256\n",
    "    \"anthropic_version\": \"bedrock-2023-05-31\",\n",
    "    \"top_p\": 0.9,  # Add top_p for more controlled generation\n",
    "    \"stop_sequences\": [\"}\"]  # Stop after JSON closes\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9490c2db-3bc6-4958-836e-3f636e959694",
   "metadata": {},
   "source": [
    "## Define MLflow score metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb1db4ae-3b4e-4773-8623-3fe952fe2753",
   "metadata": {},
   "source": [
    "#### Create custom MLflow scorer function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "672cf273-71b6-4994-9fae-8e04a7129ec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Literal\n",
    "from mlflow.genai import scorer\n",
    "from mlflow.genai.judges import make_judge\n",
    "\n",
    "@scorer\n",
    "def tokens_words(outputs) -> int:\n",
    "    \"\"\"Approximate words in the response\"\"\"\n",
    "    try:\n",
    "        words = len(outputs['generated_text'])\n",
    "    except:\n",
    "        return 0\n",
    "    return words\n",
    "\n",
    "# Create a judge that evaluates coherence using MLflow template-based scorers\n",
    "coherence_judge = make_judge(\n",
    "    name=\"coherence\",\n",
    "    instructions=(\n",
    "        \"Evaluate if the response is coherent, maintaining a constant tone \"\n",
    "        \"and following a clear flow of thoughts/concepts\"\n",
    "        \"Question: {{ inputs }}\\n\"\n",
    "        \"Response: {{ outputs }}\\n\"\n",
    "    ),\n",
    "    feedback_value_type=Literal[\"coherent\", \"somewhat coherent\", \"incoherent\"],\n",
    "    model= MLFLOW_EVALUATION_MODEL_ID\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c57caab9-dd60-4669-9883-9895e26a3b24",
   "metadata": {},
   "source": [
    "#### Define all the MLflow scorer\n",
    "We will include predefined MLflow scorer. And then Load all the scorers toghether."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bba25fe-13b1-486d-b1f0-218b724a62f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlflow.genai.scorers import Guidelines, Safety, RelevanceToQuery, Fluency\n",
    "\n",
    "scorers = [\n",
    "    # MLflow built-in genai scorers\n",
    "    Safety(\n",
    "        model= MLFLOW_EVALUATION_MODEL_ID,\n",
    "        parameters= MLFLOW_EVALUATION_MODEL_PARAM,\n",
    "    ),\n",
    "    RelevanceToQuery(\n",
    "        model= MLFLOW_EVALUATION_MODEL_ID,\n",
    "        parameters= MLFLOW_EVALUATION_MODEL_PARAM,\n",
    "    ),\n",
    "    Fluency(\n",
    "        model= MLFLOW_EVALUATION_MODEL_ID,\n",
    "        parameters= MLFLOW_EVALUATION_MODEL_PARAM,\n",
    "    ),\n",
    "    # MLflow built-in guidelines-based LLM-as-a-judge\n",
    "    Guidelines(\n",
    "        name=\"follows_objective\",\n",
    "        guidelines=\"The generated response must follow the objective in the request.\",\n",
    "        model= MLFLOW_EVALUATION_MODEL_ID,\n",
    "        parameters= MLFLOW_EVALUATION_MODEL_PARAM,\n",
    "    ),\n",
    "    Guidelines(\n",
    "        name=\"professional_tone\",\n",
    "        guidelines=\"The response must be in a professional tone.\",\n",
    "        model= MLFLOW_EVALUATION_MODEL_ID,\n",
    "        parameters= MLFLOW_EVALUATION_MODEL_PARAM,\n",
    "    ),\n",
    "    Guidelines(\n",
    "        name=\"no_harmful_advice\",\n",
    "        guidelines=\"The response MUST NOT cause harm to any human.\",\n",
    "        model= MLFLOW_EVALUATION_MODEL_ID,\n",
    "        parameters= MLFLOW_EVALUATION_MODEL_PARAM,\n",
    "    ),\n",
    "    # MLflow built-in template-based LLM-as-a-judge\n",
    "    coherence_judge,\n",
    "    # Custom defined scorers\n",
    "    tokens_words,\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9415541-32e5-472b-a26c-23c603eb9726",
   "metadata": {},
   "source": [
    "### Perform MLflow GenAI Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78e12076-1b55-4222-a33b-0d3f19eb88f5",
   "metadata": {},
   "source": [
    "#### Load MLflow traces for GenAI Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9b319b3-c124-41aa-b936-69453789c0c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "traces = mlflow.search_traces(\n",
    "    filter_string=f\"name = '{s3_file_key}'\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "223a2a8d-bf42-4d21-ba6b-278ef7c45a39",
   "metadata": {},
   "outputs": [],
   "source": [
    "traces"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "041e8a2f-7167-4b89-aca5-17ff2e87fdf8",
   "metadata": {},
   "source": [
    "#### Perform the evaluation job\n",
    "> Note: This is the first pass at Traces evaluation, skip this if you see error to the second pass\n",
    ">\n",
    "> Ignore All WARNINGS\n",
    ">\n",
    "> During the first pass you might encounter MLflow OSS Error `RestException: BAD_REQUEST: (psycopg2.errors.UniqueViolation) duplicate key value violates unique constraint \"entity_associations_pk\"`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f22620a9-56a5-4868-b925-2e235e8fd825",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "results = mlflow.genai.evaluate(\n",
    "    data=traces,\n",
    "    scorers=scorers,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8e793e2-3829-40db-a086-48437c0f2be7",
   "metadata": {},
   "source": [
    "### Second pass: run evaluation in batches\n",
    "> To handle the MLflow issue https://github.com/mlflow/mlflow/issues/21002\n",
    ">\n",
    "> Ignore All WARNINGS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3f9ce73-be52-42a9-b0f3-2ed3e307b4e6",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "results = mlflow.genai.evaluate(\n",
    "    data=traces,\n",
    "    scorers=scorers,\n",
    ")\n",
    "print(f\"Evaluations completed successfully ✅\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3632e28f-2412-483f-b8ad-b0159414b55b",
   "metadata": {},
   "source": [
    "### See results in SageMaker AI MLflow App\n",
    "1. Go the the MLflow experiment and view the results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a6762d1-1880-4ac1-a8f4-03f44381ddba",
   "metadata": {},
   "source": [
    "# Load testing CDK deployment(Optional)\n",
    "We reuse samples from the `FreedomIntelligence/medical-o1-reasoning-SFT` dataset as a proxy for a load testing dataset.\n",
    "\n",
    "Each sample contains:\n",
    "\n",
    "- A medical **Question**.\n",
    "- A detailed **Response** that we treat as the expected answer.\n",
    "- An optional chain‑of‑thought field (`Complex_CoT`) that we ignore for scoring, but which could be used in more advanced evaluation setups.\n",
    "\n",
    "You can replace this dataset with your own dataset (for example, human‑annotated medical Q&A pairs corresponding to your domain and compliance requirements).\n",
    "\n",
    "> Note: Load testing with large sample size of dataset will take significant time. Start with a small dataset sample size. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7437338d-d800-48d6-a114-db00961ad4cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "\n",
    "# Set the desired sample size to test. \n",
    "num_samples = 100\n",
    "\n",
    "full_dataset = load_dataset(\"FreedomIntelligence/medical-o1-reasoning-SFT\", \"en\", split=f\"train[:{num_samples}]\")\n",
    "\n",
    "full_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "466cc61d-d9f6-4bf4-b754-c775f5a9206c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import boto3\n",
    "from datetime import datetime\n",
    "\n",
    "sagemaker_runtime = boto3.client('sagemaker-runtime')\n",
    "\n",
    "def invoke_smai_endpoint(prompt, endpoint_name, max_tokens=512, temperature=0.7):\n",
    "    \"\"\"\n",
    "    Invoke sagemaker endpoint\n",
    "    \"\"\"\n",
    "    payload = {\n",
    "        \"inputs\": prompt,\n",
    "        \"parameters\": {\n",
    "            \"max_new_tokens\": max_tokens,\n",
    "            \"temperature\": temperature,\n",
    "            \"top_p\": 0.9,\n",
    "            \"do_sample\": True\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    response = sagemaker_runtime.invoke_endpoint(\n",
    "        EndpointName=endpoint_name,\n",
    "        ContentType='application/json',\n",
    "        Body=json.dumps(payload)\n",
    "    )\n",
    "    \n",
    "    result = json.loads(response['Body'].read().decode())\n",
    "    \n",
    "    # print(f\"Response: {result}\")\n",
    "    \n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b9f6f52-0747-47a9-a405-134b9304775b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Test with sample prompts\n",
    "count = 0\n",
    "for prompt in full_dataset:\n",
    "    response = invoke_smai_endpoint(prompt[\"Question\"], endpoint_name)\n",
    "    count+=1\n",
    "print(f\"✨ Load testing complete: {count}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
